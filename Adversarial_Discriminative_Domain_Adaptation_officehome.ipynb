{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this script we will implement the adversarial discriminative domain adaptation algorithm\n",
    "#the algorithm is described in the paper \"Adversarial Discriminative Domain Adaptation\" by Ganin et al.\n",
    "#we will use MNIST as the source domain and USPS as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the whole training procedure has 3 steps:\n",
    "#1. train the a (feature extractor CNN + Classifier ) on the source domain\n",
    "#2. train a GAN on the features extracted from the source domain and the target domain to minimize the discrepancy between the two domains\n",
    "#3. test the classifier on the target domain\n",
    "\n",
    "#1. in step 1 both the feature extractor CNN and the classifier are trainable\n",
    "#2. in step 2 only the GAN Discriminator is trainable\n",
    "#3. in step 3 none are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use wasserstein loss\n",
    "#we will use gradient penalty for the discriminator\n",
    "#we eill use Resnet50 as base model for the feature extractor CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'officehome_adda'\n",
    "version = 'v0'\n",
    "\n",
    "#concat experiment name and version to get experiment id\n",
    "experiment_id = experiment_name + '_' + version\n",
    "\n",
    "model_path = 'saved_models/ADDA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/officehome/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU name\n",
    "#\n",
    "GPU_NAME = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neceassary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, Function\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#import utils\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for visualizations\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 16:35:05.431352: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "#import tenserboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#initialize tensorboard writer\n",
    "#create writer for tensorboard\n",
    "writer = SummaryWriter(f'runs/'+experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable cudnn\n",
    "cudnn.benchmark = True\n",
    "#cuda cache clear\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device\n",
    "device = torch.device(GPU_NAME if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the hyperparameters\n",
    "BATCH = 50\n",
    "batch_size = BATCH\n",
    "EPOCHS = 1\n",
    "NUM_EPOCHS_PRETRAINING = 1\n",
    "\n",
    "#WHGAN parameters\n",
    "NUM_EPOCHS_GAN = 1\n",
    "CRITIC_ITERATIONS = 5\n",
    "LEARNING_RATE_GAN = 1e-4\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "CHANNELS_IMG = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "#parameters of ADAM optimizer\n",
    "LEARNING_RATE = 0.001\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "\n",
    "#parameters of SGD optimizer with momentum\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function for making all parameters of a model non trainable or trainable based on require_grad\n",
    "def freeze_unfreeze_model(model, require_grad = True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = require_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "def save_model(model):\n",
    "    #check if model path exists\n",
    "    if not os.path.exists(model_path, name_to_save):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    #we will save the model by the name of the experiment id \n",
    "    torch.save(model.state_dict(),  f'{model_path}/{experiment_id}+{name_to_save}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will have 3 different models : \n",
    "# 1. Feature Extractor CNN or Encoder, Source Feature Extractor CNN or Source Encoder AND Target Feature Extractor CNN or Target Encoder\n",
    "# 2. Classifier\n",
    "# 3. Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us first build the feature extractor\n",
    "#we input a resent50 model\n",
    "\n",
    "#creating the model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "#send weight sto gpu\n",
    "# weights = weights.to(device)\n",
    "#sending the model to GPU\n",
    "\n",
    "base_resnet = resnet50(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print model\n",
    "base_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will change the first convolution layer to accept single channel image\n",
    "#conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# #if CHANNELS_IMG == 1:\n",
    "# if CHANNELS_IMG == 1:\n",
    "#     base_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# #change the last fully connected layer to output classes in NUM_CLASSES\n",
    "# base_resnet.fc = nn.Linear(2048, NUM_CLASSES,  bias=True)\n",
    "\n",
    "#write a function to changethe model based on number of channels and number of classes\n",
    "def change_model(model, num_classes = NUM_CLASSES):\n",
    "    \n",
    "    model.fc = nn.Linear(2048, num_classes,  bias=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "base_resnet = change_model(base_resnet, num_classes = NUM_CLASSES)\n",
    "base_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will write class for the feature extractor network\n",
    "#we will pass the resnet50 model as the input to the class, and will use : nn.Sequential(*list(original_model.children())[:-2]) to get the feature extractor part of the model\n",
    "\n",
    "class ENCODER_CNN(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ENCODER_CNN, self).__init__()\n",
    "        # self.base_model = base_model\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENCODER_CNN(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now create an instance of the feature extractor and print the model\n",
    "source_cnn = ENCODER_CNN(base_resnet).to(device)\n",
    "source_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model summary\n",
    "# summary(source_cnn, (CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1024, 14, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tthe format for output of the feature extractor\n",
    "#create a random vector of size (BATCH, CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE)\n",
    "x = torch.randn(BATCH, CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "#pass the random vector through the feature extractor\n",
    "x = source_cnn(x)\n",
    "#check the output shape\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 14, 14])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will store the output shape of the feature extractor\n",
    "output_shape = x.shape\n",
    "#make the first dimension as 1 and then remove it\n",
    "output_shape = output_shape[1:]\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#now we will create the classifier, it will be same as the part of the resnet50 model after the feature extractor i.e. the last two layers of the resnet50 model\n",
    "#also the number of classes will be 10 as we have 10 classes in the MNIST dataset, the number of classes is stored in NUM_CLASSES\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.avgpool= nn.Sequential(*list(base_model.children())[-3:-1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(*list(base_model.children())[-1:])\n",
    "        #define a flatten layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print the shape of the input\n",
    "        # print(\"inside classifier: input shape\",x.shape)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (avgpool): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Classifier(base_resnet).to(device)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 512, 14, 14]         524,288\n",
      "       BatchNorm2d-2          [-1, 512, 14, 14]           1,024\n",
      "              ReLU-3          [-1, 512, 14, 14]               0\n",
      "            Conv2d-4            [-1, 512, 7, 7]       2,359,296\n",
      "       BatchNorm2d-5            [-1, 512, 7, 7]           1,024\n",
      "              ReLU-6            [-1, 512, 7, 7]               0\n",
      "            Conv2d-7           [-1, 2048, 7, 7]       1,048,576\n",
      "       BatchNorm2d-8           [-1, 2048, 7, 7]           4,096\n",
      "            Conv2d-9           [-1, 2048, 7, 7]       2,097,152\n",
      "      BatchNorm2d-10           [-1, 2048, 7, 7]           4,096\n",
      "             ReLU-11           [-1, 2048, 7, 7]               0\n",
      "       Bottleneck-12           [-1, 2048, 7, 7]               0\n",
      "           Conv2d-13            [-1, 512, 7, 7]       1,048,576\n",
      "      BatchNorm2d-14            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-15            [-1, 512, 7, 7]               0\n",
      "           Conv2d-16            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-17            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-18            [-1, 512, 7, 7]               0\n",
      "           Conv2d-19           [-1, 2048, 7, 7]       1,048,576\n",
      "      BatchNorm2d-20           [-1, 2048, 7, 7]           4,096\n",
      "             ReLU-21           [-1, 2048, 7, 7]               0\n",
      "       Bottleneck-22           [-1, 2048, 7, 7]               0\n",
      "           Conv2d-23            [-1, 512, 7, 7]       1,048,576\n",
      "      BatchNorm2d-24            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-25            [-1, 512, 7, 7]               0\n",
      "           Conv2d-26            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-27            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-28            [-1, 512, 7, 7]               0\n",
      "           Conv2d-29           [-1, 2048, 7, 7]       1,048,576\n",
      "      BatchNorm2d-30           [-1, 2048, 7, 7]           4,096\n",
      "             ReLU-31           [-1, 2048, 7, 7]               0\n",
      "       Bottleneck-32           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-33           [-1, 2048, 1, 1]               0\n",
      "          Flatten-34                 [-1, 2048]               0\n",
      "           Linear-35                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 14,985,226\n",
      "Trainable params: 14,985,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 15.92\n",
      "Params size (MB): 57.16\n",
      "Estimated Total Size (MB): 73.85\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11702/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "#print summary of the classifier\n",
    "#input shape is the output shape of the feature extractor\n",
    "summary(classifier, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will create the domain classifier: named as DomainClassifier\n",
    "#it will have the rest of the resnet model after the feature extractor and the classifier and will have an additional layer at the end to output the domain label: 0 for source and 1 for target\n",
    "#it will have gradient reversal layer in between the feature extractor and the classifier, i.e the first layer of the domain classifier \n",
    "#it will be exactly same as ClassClassifier except for the last layer, which is not number of classes but 2 for domain labels and sigmoid activation function instead of softmax\n",
    "#it will also do same [-2:] to get the classifier part of the model, first layer be gradient reversal layer\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #first layer of the domain classifier be the gradient reversal layer\n",
    "\n",
    "        self.avgpool = nn.Sequential(*list(base_model.children())[-3:-1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(nn.Linear(2048, 1, bias=True))\n",
    "       \n",
    "        \n",
    "        #now add the last output layer\n",
    "        # self.domain_classifier.add_module('domain_classifier_output', nn.Linear(2048, 2))\n",
    "        #change the last layer to output 2 classes\n",
    "        # self.fc = nn.Linear(2048, 2 , bias=True)\n",
    "\n",
    "        #forward\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        #output the domain label\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # x = x.view(-1, 2)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (avgpool): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of the domain classifier\n",
    "discriminator = Discriminator(base_resnet).to(device)\n",
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print summary of the domain classifier\n",
    "# summary(discriminator, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##create a writer and pot all the model sto tensorboard\n",
    "# writer_sourcecnn = SummaryWriter('runs/plot_oh_ADDA_models_sourcecnn')\n",
    "# #plot the models\n",
    "# #create a dummy input\n",
    "# dummy_input = torch.rand(CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE).unsqueeze(0).to(device)\n",
    "# writer_sourcecnn.add_graph(source_cnn, dummy_input)\n",
    "# # writer.add_graph(class_classifier,(2048, 1, 1))\n",
    "# # writer.add_graph(domain_classifier,(2048, 1, 1))\n",
    "# #close\n",
    "# writer_sourcecnn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11702/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# #write classifier\n",
    "\n",
    "# writer_classifier = SummaryWriter('runs/plot_oh_ADDA_models_classifier')\n",
    "# #plot the models\n",
    "# #create a dummy input\n",
    "# dummy_input = torch.rand(1, 1024, 14, 14).to(device)\n",
    "# writer_classifier.add_graph(classifier, dummy_input)\n",
    "# # writer.add_graph(class_classifier,(2048, 1, 1))\n",
    "# # writer.add_graph(domain_classifier,(2048, 1, 1))\n",
    "# #close\n",
    "# writer_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #discriminator\n",
    "# writer_discriminator = SummaryWriter('runs/plot_oh_ADDA_models_discriminator')\n",
    "# #plot the models\n",
    "# #create a dummy input: torch.Size([50, 1024, 14, 14])\n",
    "# dummy_input = torch.rand(1, 1024, 14, 14).to(device)\n",
    "# writer_discriminator.add_graph(discriminator, dummy_input)\n",
    "# # writer.add_graph(class_classifier,(2048, 1, 1))\n",
    "# # writer.add_graph(domain_classifier,(2048, 1, 1))\n",
    "# #close\n",
    "# writer_discriminator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the transform for the dataset\n",
    "transform_mnist_resnet = transforms.Compose(\n",
    "    [\n",
    "  \n",
    "    # if torch tensor then leave as it is, else convert to tensor\n",
    "    transforms.Lambda(lambda x: x if isinstance(x, torch.Tensor) else transforms.functional.to_tensor(x)),\n",
    "    #\n",
    "\n",
    "    #resize to 224x224\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "\n",
    "    #check if channels are 1, then convert to 3 channels\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "\n",
    "    transforms.Lambda(lambda x: preprocess(x)),\n",
    "\n",
    "    #if channels are 3, then make them 1\n",
    "    # transforms.Lambda(lambda x: x[0].unsqueeze(0) if x.shape[0] == 3 else x),\n",
    "    \n",
    "    # normalize\n",
    "    transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training we will use MNIST dataset in pytorch library\n",
    "#for testing we will use USPS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data - Real_World\n",
    "#### test data - Clipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "#load the domain Real_World from the dataset\n",
    "\n",
    "train_data = datasets.ImageFolder(root=data_path + 'Real_World', transform=transform_mnist_resnet)\n",
    "#load train data\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load test data\n",
    "#Clipart dataset\n",
    "test_data = datasets.ImageFolder(root=data_path + 'Clipart', transform=transform_mnist_resnet)\n",
    "\n",
    "\n",
    "#load test data\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#print the length of train and test data\n",
    "print(len(train_data))\n",
    "#print the shape of train data\n",
    "print(train_data[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7291\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#print length of test data\n",
    "print(len(test_data))\n",
    "#print shape of test data\n",
    "print(test_data[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "#print number of batches in train and test data\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training by Adversarial Discriminative Domain Adaptation (ADDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Pre- training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #function to return gradient  norm\n",
    "#write a function to calculate the gradient penalty\n",
    "def gradient_norm(dnn, current_batch):\n",
    "\n",
    "    BATCH_SIZE, C, H, W = current_batch.shape\n",
    "    #print batch size, c,h,w\n",
    "    # print(\"batch size, c, h, w\", BATCH_SIZE, C, H, W)\n",
    "    if BATCH_SIZE%2==1:\n",
    "        #remove the last element\n",
    "        current_batch = current_batch[:-1]\n",
    "    #if batch size is 0 , then just return\n",
    "    if BATCH_SIZE==0:\n",
    "        return 0\n",
    "    \n",
    "    half_batch = int(BATCH_SIZE / 2)\n",
    "    # current_batch = current_batch.to(device)\n",
    "    # current_batch = Variable(current_batch, requires_grad=True)\n",
    "    #we select the first half of the batch\n",
    "    first_half = current_batch[:half_batch]\n",
    "    #we select the second half of the batch\n",
    "    second_half = current_batch[half_batch:]\n",
    "    #we create a random number between 0 and 1\n",
    "    # alpha = torch.rand(half_batch, 1)\n",
    "    #we expand the alpha to the size of the first half of the batch\n",
    "    # alpha = alpha.expand(first_half.size())\n",
    "    #we create alpha as a random number between 0 and 1 which will allow us to interpolate between the first half and the second half\n",
    "    \n",
    "    alpha = torch.rand(half_batch, 1, 1, 1).repeat(1, C, H, W)\n",
    "    #we expand the alpha to the size of the first half of the batch\n",
    "    # alpha = alpha.expand(first_half.size())\n",
    "\n",
    "\n",
    "    #we move alpha to the device\n",
    "    alpha = alpha.to(device)\n",
    "    #we interpolate between the first half and the second half\n",
    "    interpolates = alpha * first_half + ((1 - alpha) * second_half)\n",
    "    #we move interpolates to the device\n",
    "    interpolates = interpolates.to(device)\n",
    "    # interpolates = interpolates\n",
    "    #we create a variable of interpolates\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    #we pass interpolates through the cnn\n",
    "    disc_interpolates = dnn(interpolates)\n",
    "    #we calculate the gradients\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    #we calculate the gradient penalty\n",
    "    # calculate gradient norm \n",
    "    gradients_norm = gradients.norm(2, dim=1)\n",
    "    #mean of the gradient norm without subtracting 1 or lambda\n",
    "    gradient_norm_mean = (gradients_norm **2).mean()\n",
    "    #max of sqrt of the gradient norm without subtracting 1 or lambda\n",
    "    # gradient_norm_max = (gradients_norm **2).max( dim=0, keepdim=True)[0]\n",
    "\n",
    "    #delete the variables from the memory\n",
    "    del first_half\n",
    "    del second_half\n",
    "    del alpha\n",
    "    del interpolates\n",
    "    del disc_interpolates\n",
    "    del gradients\n",
    "    del gradients_norm\n",
    "    #cache the garbage\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "    # gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()    #have to check this formula    / * LAMBDA\n",
    "    #gradient penalty  should be max(0, gradient_penalty-1)\n",
    "    #we return the gradient penalty\n",
    "    return gradient_norm_mean\n",
    "    # , gradient_norm_max\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this approach, we will use the source_cnn and the classifier to train on the source data\n",
    "#write a function for pretraining the source_cnn and the classifier: we will input the source_cnn and the classifier and the train_loader, and the number of epochs, and device\n",
    "#we will use cross entropy loss for the classifier and ADAM optimizer for both the source_cnn and the classifier\n",
    "#we will also use tensorboard to visualize the training process, and plot the loss and accuracy\n",
    "#we will print the loss and accuracy after each epoch and also plot the loss and accuracy after each epoch\n",
    "def pre_train(source_cnn, classifier, train_loader, epoch=NUM_EPOCHS_PRETRAINING, device=device):\n",
    "\n",
    "    #we will store the source cnn in file at path experiment_id + temp_storage\n",
    "    #and will load this ile anytime source_cnn is needed\n",
    "    \n",
    "    #define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #define the optimizer for the source_cnn and the classifier in a list, and we use ADAM optimizer combined for both the source_cnn and the classifier\n",
    "    optimizer = optim.Adam(list(source_cnn.parameters()) + list(classifier.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "    #make both the source_cnn and the classifier in train mode\n",
    "    source_cnn.train()\n",
    "    classifier.train()\n",
    "\n",
    "    #loop for each epoch\n",
    "    epoch_tracker = 0\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batch_tracker = 0\n",
    "    ep=0\n",
    "    #we will add th loss for each batch in the epoch and then divide by the number of batches\n",
    "    for ep in range(epoch):\n",
    "        epoch_total = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_total_loss = 0\n",
    "        #loop for each batch\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #send data to gpu\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #set the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            #forward pass\n",
    "            output = classifier(source_cnn(data))\n",
    "            #calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            #calculate the gradients\n",
    "            loss.backward()\n",
    "            #update the weights\n",
    "\n",
    "            #we get the gradient norm by sending the model as sequential of source_cnn and classifier\n",
    "            #we will calculate the gradient norm\n",
    "            gradient_n = gradient_norm(nn.Sequential(source_cnn, classifier), data)\n",
    "            #we will add the gradient norm to the tensorboard\n",
    "            writer.add_scalar('Pretraining Gradient Norm', gradient_n, batch_tracker)\n",
    "            optimizer.step()\n",
    "            #write the loss to tensorboard\n",
    "            writer.add_scalar('Pretraining Training loss', loss, global_step=batch_tracker)\n",
    "\n",
    "            #calculate the total loss\n",
    "            total_loss += loss.item()\n",
    "            #total epoch loss sum\n",
    "            epoch_total_loss += loss.item()\n",
    "\n",
    "\n",
    "            #calculate the accuracy\n",
    "            #get the max value from the output\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            #calculate the total number of labels\n",
    "            temp_total = target.size(0)\n",
    "            #calculate the correct predictions\n",
    "            temp_correct = (predicted == target).sum().item()\n",
    "            #add the total and correct predictions\n",
    "            total += temp_total\n",
    "            epoch_total += temp_total\n",
    "            correct += temp_correct\n",
    "            epoch_correct += temp_correct\n",
    "            #calculate the accuracy\n",
    "            epoch_accuracy = 100 * epoch_correct / epoch_total\n",
    "            #write the accuracy to tensorboard\n",
    "            writer.add_scalar('Pretraining Training accuracy', epoch_accuracy, global_step=batch_tracker)\n",
    "            #print the loss and accuracy\n",
    "            #and\n",
    "            #print the gradient norm\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.2f}%\\tGradient Norm: {:.6f}'.format(\n",
    "                ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "                epoch_accuracy, gradient_n))\n",
    "            \n",
    "\n",
    "            \n",
    "            #print the loss\n",
    "            # if batch_idx % log_interval == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "        \n",
    "            #write the epoch loss to tensorboard\n",
    "            #first average the loss over the batches in the epoch\n",
    "            batch_tracker += 1\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "\n",
    "        #write the loss to tensorboard\n",
    "        writer.add_scalar('Pretraining  Training - Epoch loss', epoch_loss, global_step=ep)\n",
    "        #calculate the accuracy\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        #write the accuracy to tensorboard\n",
    "        writer.add_scalar('Pretraining  Training - Epoch accuracy', epoch_accuracy, global_step=ep)\n",
    "\n",
    "        # #save the model after every epoch, the name be experiment_id_epoch\n",
    "        # #wew will save in the folder saved_models\n",
    "        # torch.save(model.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'.pth')\n",
    "        # #we will also save the optimizer\n",
    "        # torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'_optimizer.pth')\n",
    "        #we will save the best model till now based on loss\n",
    "        #check if first epoch, then save the model anyway\n",
    "        if ep == 0:\n",
    "            #save the model\n",
    "            torch.save(source_cnn.state_dict(), 'saved_models/'+experiment_id+'_source_cnn.pth')\n",
    "            torch.save(classifier.state_dict(), 'saved_models/'+experiment_id+'_classifier.pth')\n",
    "            #save the optimizer\n",
    "            torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_optimizer.pth')\n",
    "            #save the loss\n",
    "            best_loss = epoch_total_loss\n",
    "            #save the epoch\n",
    "            best_epoch = ep\n",
    "        #if not first epoch, then check if the loss is less than the best loss\n",
    "        else:\n",
    "            #if loss is less than the best loss, then save the model\n",
    "            if epoch_total_loss < best_loss:\n",
    "                #save the model\n",
    "                torch.save(source_cnn.state_dict(), 'saved_models/'+experiment_id+'_source_cnn.pth')\n",
    "                torch.save(classifier.state_dict(), 'saved_models/'+experiment_id+'_classifier.pth')\n",
    "                #save the optimizer\n",
    "                torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_optimizer.pth')\n",
    "                #save the loss\n",
    "                best_loss = epoch_total_loss\n",
    "                #save the epoch\n",
    "                best_epoch = ep\n",
    "\n",
    "        epoch_tracker += 1\n",
    "\n",
    "    #print the accuracy\n",
    "    total_accuracy = 100 * correct / total\n",
    "    print('Accuracy: ', total_accuracy)\n",
    "\n",
    "    #close the tensorboard writer\n",
    "    writer.close()\n",
    "    #save the model with name experiment_id and then the last epoch\n",
    "    torch.save(source_cnn.state_dict(), 'saved_models/'+experiment_id+'_source_cnn_'+str(ep)+'.pth')\n",
    "    torch.save(classifier.state_dict(), 'saved_models/'+experiment_id+'_classifier_'+str(ep)+'.pth')\n",
    "    #save the optimizer\n",
    "    torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_optimizer_'+str(ep)+'.pth')\n",
    "    \n",
    "\n",
    "    #return the models\n",
    "    return source_cnn, classifier\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [0/60000 (0%)]\tLoss: 2.300536\tAccuracy: 18.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50/60000 (0%)]\tLoss: 2.297848\tAccuracy: 15.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [100/60000 (0%)]\tLoss: 2.277404\tAccuracy: 22.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [150/60000 (0%)]\tLoss: 2.190504\tAccuracy: 26.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [200/60000 (0%)]\tLoss: 2.131189\tAccuracy: 26.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [250/60000 (0%)]\tLoss: 2.083789\tAccuracy: 27.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [300/60000 (0%)]\tLoss: 2.001318\tAccuracy: 30.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [350/60000 (1%)]\tLoss: 2.083127\tAccuracy: 33.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [400/60000 (1%)]\tLoss: 1.966553\tAccuracy: 36.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [450/60000 (1%)]\tLoss: 1.893026\tAccuracy: 40.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [500/60000 (1%)]\tLoss: 1.798171\tAccuracy: 43.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [550/60000 (1%)]\tLoss: 1.797086\tAccuracy: 45.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [600/60000 (1%)]\tLoss: 1.800857\tAccuracy: 47.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [650/60000 (1%)]\tLoss: 1.639575\tAccuracy: 50.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [700/60000 (1%)]\tLoss: 1.650470\tAccuracy: 52.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [750/60000 (1%)]\tLoss: 1.646719\tAccuracy: 54.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [800/60000 (1%)]\tLoss: 1.584382\tAccuracy: 56.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [850/60000 (1%)]\tLoss: 1.553466\tAccuracy: 58.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [900/60000 (2%)]\tLoss: 1.552649\tAccuracy: 60.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [950/60000 (2%)]\tLoss: 1.577331\tAccuracy: 61.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1000/60000 (2%)]\tLoss: 1.625465\tAccuracy: 62.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1050/60000 (2%)]\tLoss: 1.593590\tAccuracy: 63.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1100/60000 (2%)]\tLoss: 1.570496\tAccuracy: 64.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1150/60000 (2%)]\tLoss: 1.565998\tAccuracy: 66.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1200/60000 (2%)]\tLoss: 1.691390\tAccuracy: 66.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1250/60000 (2%)]\tLoss: 1.556685\tAccuracy: 67.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1300/60000 (2%)]\tLoss: 1.590231\tAccuracy: 68.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1350/60000 (2%)]\tLoss: 1.567214\tAccuracy: 68.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1400/60000 (2%)]\tLoss: 1.538064\tAccuracy: 69.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1450/60000 (2%)]\tLoss: 1.590270\tAccuracy: 70.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1500/60000 (2%)]\tLoss: 1.573454\tAccuracy: 70.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1550/60000 (3%)]\tLoss: 1.605040\tAccuracy: 71.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1600/60000 (3%)]\tLoss: 1.506989\tAccuracy: 72.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1650/60000 (3%)]\tLoss: 1.538143\tAccuracy: 72.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1700/60000 (3%)]\tLoss: 1.551025\tAccuracy: 73.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1750/60000 (3%)]\tLoss: 1.516888\tAccuracy: 73.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1800/60000 (3%)]\tLoss: 1.528491\tAccuracy: 74.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1850/60000 (3%)]\tLoss: 1.584736\tAccuracy: 74.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1900/60000 (3%)]\tLoss: 1.556303\tAccuracy: 75.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1950/60000 (3%)]\tLoss: 1.550505\tAccuracy: 75.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2000/60000 (3%)]\tLoss: 1.598651\tAccuracy: 75.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2050/60000 (3%)]\tLoss: 1.565757\tAccuracy: 76.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2100/60000 (4%)]\tLoss: 1.476396\tAccuracy: 76.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2150/60000 (4%)]\tLoss: 1.509695\tAccuracy: 77.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2200/60000 (4%)]\tLoss: 1.513767\tAccuracy: 77.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2250/60000 (4%)]\tLoss: 1.667345\tAccuracy: 77.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2300/60000 (4%)]\tLoss: 1.569885\tAccuracy: 77.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2350/60000 (4%)]\tLoss: 1.534778\tAccuracy: 78.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2400/60000 (4%)]\tLoss: 1.608343\tAccuracy: 78.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2450/60000 (4%)]\tLoss: 1.565755\tAccuracy: 78.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2500/60000 (4%)]\tLoss: 1.538819\tAccuracy: 78.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2550/60000 (4%)]\tLoss: 1.547161\tAccuracy: 79.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2600/60000 (4%)]\tLoss: 1.526364\tAccuracy: 79.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2650/60000 (4%)]\tLoss: 1.548101\tAccuracy: 79.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2700/60000 (4%)]\tLoss: 1.609186\tAccuracy: 79.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2750/60000 (5%)]\tLoss: 1.482992\tAccuracy: 80.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2800/60000 (5%)]\tLoss: 1.546223\tAccuracy: 80.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2850/60000 (5%)]\tLoss: 1.566038\tAccuracy: 80.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2900/60000 (5%)]\tLoss: 1.557995\tAccuracy: 80.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2950/60000 (5%)]\tLoss: 1.542345\tAccuracy: 80.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3000/60000 (5%)]\tLoss: 1.509454\tAccuracy: 81.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3050/60000 (5%)]\tLoss: 1.592400\tAccuracy: 81.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3100/60000 (5%)]\tLoss: 1.516706\tAccuracy: 81.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3150/60000 (5%)]\tLoss: 1.532692\tAccuracy: 81.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3200/60000 (5%)]\tLoss: 1.541368\tAccuracy: 81.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3250/60000 (5%)]\tLoss: 1.511675\tAccuracy: 82.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3300/60000 (6%)]\tLoss: 1.542230\tAccuracy: 82.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3350/60000 (6%)]\tLoss: 1.509036\tAccuracy: 82.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3400/60000 (6%)]\tLoss: 1.517882\tAccuracy: 82.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3450/60000 (6%)]\tLoss: 1.525715\tAccuracy: 82.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3500/60000 (6%)]\tLoss: 1.504734\tAccuracy: 82.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3550/60000 (6%)]\tLoss: 1.520941\tAccuracy: 83.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3600/60000 (6%)]\tLoss: 1.476506\tAccuracy: 83.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3650/60000 (6%)]\tLoss: 1.561178\tAccuracy: 83.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3700/60000 (6%)]\tLoss: 1.554713\tAccuracy: 83.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3750/60000 (6%)]\tLoss: 1.565589\tAccuracy: 83.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3800/60000 (6%)]\tLoss: 1.584869\tAccuracy: 83.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3850/60000 (6%)]\tLoss: 1.557793\tAccuracy: 83.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3900/60000 (6%)]\tLoss: 1.548387\tAccuracy: 83.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3950/60000 (7%)]\tLoss: 1.509645\tAccuracy: 84.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4000/60000 (7%)]\tLoss: 1.477886\tAccuracy: 84.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4050/60000 (7%)]\tLoss: 1.488384\tAccuracy: 84.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4100/60000 (7%)]\tLoss: 1.538458\tAccuracy: 84.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4150/60000 (7%)]\tLoss: 1.552894\tAccuracy: 84.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4200/60000 (7%)]\tLoss: 1.578228\tAccuracy: 84.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4250/60000 (7%)]\tLoss: 1.499586\tAccuracy: 84.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4300/60000 (7%)]\tLoss: 1.558587\tAccuracy: 84.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4350/60000 (7%)]\tLoss: 1.473076\tAccuracy: 85.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4400/60000 (7%)]\tLoss: 1.498479\tAccuracy: 85.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4450/60000 (7%)]\tLoss: 1.511019\tAccuracy: 85.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4500/60000 (8%)]\tLoss: 1.535333\tAccuracy: 85.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4550/60000 (8%)]\tLoss: 1.517015\tAccuracy: 85.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4600/60000 (8%)]\tLoss: 1.518255\tAccuracy: 85.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4650/60000 (8%)]\tLoss: 1.655418\tAccuracy: 85.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4700/60000 (8%)]\tLoss: 1.496347\tAccuracy: 85.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4750/60000 (8%)]\tLoss: 1.562844\tAccuracy: 85.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4800/60000 (8%)]\tLoss: 1.604569\tAccuracy: 85.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4850/60000 (8%)]\tLoss: 1.506721\tAccuracy: 85.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4900/60000 (8%)]\tLoss: 1.503597\tAccuracy: 85.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4950/60000 (8%)]\tLoss: 1.491328\tAccuracy: 86.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5000/60000 (8%)]\tLoss: 1.558017\tAccuracy: 86.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5050/60000 (8%)]\tLoss: 1.509318\tAccuracy: 86.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5100/60000 (8%)]\tLoss: 1.550844\tAccuracy: 86.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5150/60000 (9%)]\tLoss: 1.515117\tAccuracy: 86.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5200/60000 (9%)]\tLoss: 1.496067\tAccuracy: 86.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5250/60000 (9%)]\tLoss: 1.483200\tAccuracy: 86.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5300/60000 (9%)]\tLoss: 1.603201\tAccuracy: 86.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5350/60000 (9%)]\tLoss: 1.535014\tAccuracy: 86.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5400/60000 (9%)]\tLoss: 1.512239\tAccuracy: 86.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5450/60000 (9%)]\tLoss: 1.481958\tAccuracy: 86.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5500/60000 (9%)]\tLoss: 1.469656\tAccuracy: 86.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5550/60000 (9%)]\tLoss: 1.508240\tAccuracy: 86.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5600/60000 (9%)]\tLoss: 1.511516\tAccuracy: 87.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5650/60000 (9%)]\tLoss: 1.499350\tAccuracy: 87.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5700/60000 (10%)]\tLoss: 1.489262\tAccuracy: 87.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5750/60000 (10%)]\tLoss: 1.478206\tAccuracy: 87.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5800/60000 (10%)]\tLoss: 1.566557\tAccuracy: 87.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5850/60000 (10%)]\tLoss: 1.522251\tAccuracy: 87.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5900/60000 (10%)]\tLoss: 1.494029\tAccuracy: 87.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5950/60000 (10%)]\tLoss: 1.528132\tAccuracy: 87.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6000/60000 (10%)]\tLoss: 1.503981\tAccuracy: 87.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6050/60000 (10%)]\tLoss: 1.498354\tAccuracy: 87.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6100/60000 (10%)]\tLoss: 1.508919\tAccuracy: 87.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6150/60000 (10%)]\tLoss: 1.532606\tAccuracy: 87.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6200/60000 (10%)]\tLoss: 1.516589\tAccuracy: 87.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6250/60000 (10%)]\tLoss: 1.507340\tAccuracy: 87.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6300/60000 (10%)]\tLoss: 1.469277\tAccuracy: 88.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6350/60000 (11%)]\tLoss: 1.525441\tAccuracy: 88.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6400/60000 (11%)]\tLoss: 1.531975\tAccuracy: 88.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6450/60000 (11%)]\tLoss: 1.548862\tAccuracy: 88.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6500/60000 (11%)]\tLoss: 1.504058\tAccuracy: 88.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6550/60000 (11%)]\tLoss: 1.606734\tAccuracy: 88.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6600/60000 (11%)]\tLoss: 1.548705\tAccuracy: 88.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6650/60000 (11%)]\tLoss: 1.523326\tAccuracy: 88.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6700/60000 (11%)]\tLoss: 1.501844\tAccuracy: 88.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6750/60000 (11%)]\tLoss: 1.542310\tAccuracy: 88.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6800/60000 (11%)]\tLoss: 1.526400\tAccuracy: 88.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6850/60000 (11%)]\tLoss: 1.500827\tAccuracy: 88.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6900/60000 (12%)]\tLoss: 1.525927\tAccuracy: 88.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6950/60000 (12%)]\tLoss: 1.495453\tAccuracy: 88.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7000/60000 (12%)]\tLoss: 1.541462\tAccuracy: 88.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7050/60000 (12%)]\tLoss: 1.477583\tAccuracy: 88.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7100/60000 (12%)]\tLoss: 1.467430\tAccuracy: 88.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7150/60000 (12%)]\tLoss: 1.547817\tAccuracy: 88.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7200/60000 (12%)]\tLoss: 1.508299\tAccuracy: 88.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7250/60000 (12%)]\tLoss: 1.515514\tAccuracy: 88.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7300/60000 (12%)]\tLoss: 1.530922\tAccuracy: 88.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7350/60000 (12%)]\tLoss: 1.503916\tAccuracy: 88.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7400/60000 (12%)]\tLoss: 1.480556\tAccuracy: 88.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7450/60000 (12%)]\tLoss: 1.508868\tAccuracy: 88.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7500/60000 (12%)]\tLoss: 1.522051\tAccuracy: 89.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7550/60000 (13%)]\tLoss: 1.526478\tAccuracy: 89.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7600/60000 (13%)]\tLoss: 1.549963\tAccuracy: 89.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7650/60000 (13%)]\tLoss: 1.490599\tAccuracy: 89.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7700/60000 (13%)]\tLoss: 1.533240\tAccuracy: 89.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7750/60000 (13%)]\tLoss: 1.492128\tAccuracy: 89.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7800/60000 (13%)]\tLoss: 1.505994\tAccuracy: 89.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7850/60000 (13%)]\tLoss: 1.521872\tAccuracy: 89.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7900/60000 (13%)]\tLoss: 1.522517\tAccuracy: 89.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7950/60000 (13%)]\tLoss: 1.471876\tAccuracy: 89.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8000/60000 (13%)]\tLoss: 1.532573\tAccuracy: 89.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8050/60000 (13%)]\tLoss: 1.507633\tAccuracy: 89.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8100/60000 (14%)]\tLoss: 1.475535\tAccuracy: 89.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8150/60000 (14%)]\tLoss: 1.550597\tAccuracy: 89.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8200/60000 (14%)]\tLoss: 1.493028\tAccuracy: 89.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8250/60000 (14%)]\tLoss: 1.528290\tAccuracy: 89.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8300/60000 (14%)]\tLoss: 1.548810\tAccuracy: 89.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8350/60000 (14%)]\tLoss: 1.546206\tAccuracy: 89.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8400/60000 (14%)]\tLoss: 1.511881\tAccuracy: 89.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8450/60000 (14%)]\tLoss: 1.502206\tAccuracy: 89.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8500/60000 (14%)]\tLoss: 1.615617\tAccuracy: 89.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8550/60000 (14%)]\tLoss: 1.543528\tAccuracy: 89.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8600/60000 (14%)]\tLoss: 1.534973\tAccuracy: 89.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8650/60000 (14%)]\tLoss: 1.502231\tAccuracy: 89.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8700/60000 (14%)]\tLoss: 1.498638\tAccuracy: 89.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8750/60000 (15%)]\tLoss: 1.528268\tAccuracy: 89.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8800/60000 (15%)]\tLoss: 1.510545\tAccuracy: 89.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8850/60000 (15%)]\tLoss: 1.490027\tAccuracy: 89.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8900/60000 (15%)]\tLoss: 1.505124\tAccuracy: 89.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8950/60000 (15%)]\tLoss: 1.490989\tAccuracy: 89.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9000/60000 (15%)]\tLoss: 1.507775\tAccuracy: 89.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9050/60000 (15%)]\tLoss: 1.484561\tAccuracy: 90.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9100/60000 (15%)]\tLoss: 1.522457\tAccuracy: 90.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9150/60000 (15%)]\tLoss: 1.501306\tAccuracy: 90.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9200/60000 (15%)]\tLoss: 1.463803\tAccuracy: 90.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9250/60000 (15%)]\tLoss: 1.528220\tAccuracy: 90.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9300/60000 (16%)]\tLoss: 1.551977\tAccuracy: 90.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9350/60000 (16%)]\tLoss: 1.496558\tAccuracy: 90.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9400/60000 (16%)]\tLoss: 1.465391\tAccuracy: 90.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9450/60000 (16%)]\tLoss: 1.508420\tAccuracy: 90.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9500/60000 (16%)]\tLoss: 1.461794\tAccuracy: 90.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9550/60000 (16%)]\tLoss: 1.512433\tAccuracy: 90.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9600/60000 (16%)]\tLoss: 1.485320\tAccuracy: 90.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9650/60000 (16%)]\tLoss: 1.584068\tAccuracy: 90.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9700/60000 (16%)]\tLoss: 1.538793\tAccuracy: 90.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9750/60000 (16%)]\tLoss: 1.520358\tAccuracy: 90.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9800/60000 (16%)]\tLoss: 1.594120\tAccuracy: 90.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9850/60000 (16%)]\tLoss: 1.481407\tAccuracy: 90.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9900/60000 (16%)]\tLoss: 1.516877\tAccuracy: 90.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9950/60000 (17%)]\tLoss: 1.496572\tAccuracy: 90.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10000/60000 (17%)]\tLoss: 1.514858\tAccuracy: 90.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10050/60000 (17%)]\tLoss: 1.461375\tAccuracy: 90.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10100/60000 (17%)]\tLoss: 1.500349\tAccuracy: 90.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10150/60000 (17%)]\tLoss: 1.513712\tAccuracy: 90.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10200/60000 (17%)]\tLoss: 1.547323\tAccuracy: 90.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10250/60000 (17%)]\tLoss: 1.461876\tAccuracy: 90.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10300/60000 (17%)]\tLoss: 1.493866\tAccuracy: 90.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10350/60000 (17%)]\tLoss: 1.475213\tAccuracy: 90.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10400/60000 (17%)]\tLoss: 1.562516\tAccuracy: 90.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10450/60000 (17%)]\tLoss: 1.480321\tAccuracy: 90.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10500/60000 (18%)]\tLoss: 1.468482\tAccuracy: 90.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10550/60000 (18%)]\tLoss: 1.539128\tAccuracy: 90.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10600/60000 (18%)]\tLoss: 1.510401\tAccuracy: 90.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10650/60000 (18%)]\tLoss: 1.504042\tAccuracy: 90.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10700/60000 (18%)]\tLoss: 1.497310\tAccuracy: 90.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10750/60000 (18%)]\tLoss: 1.484895\tAccuracy: 90.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10800/60000 (18%)]\tLoss: 1.504359\tAccuracy: 90.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10850/60000 (18%)]\tLoss: 1.480627\tAccuracy: 90.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10900/60000 (18%)]\tLoss: 1.514150\tAccuracy: 90.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10950/60000 (18%)]\tLoss: 1.501543\tAccuracy: 90.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11000/60000 (18%)]\tLoss: 1.491964\tAccuracy: 90.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11050/60000 (18%)]\tLoss: 1.501271\tAccuracy: 91.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11100/60000 (18%)]\tLoss: 1.501257\tAccuracy: 91.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11150/60000 (19%)]\tLoss: 1.461203\tAccuracy: 91.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11200/60000 (19%)]\tLoss: 1.492231\tAccuracy: 91.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11250/60000 (19%)]\tLoss: 1.566033\tAccuracy: 91.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11300/60000 (19%)]\tLoss: 1.521797\tAccuracy: 91.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11350/60000 (19%)]\tLoss: 1.529354\tAccuracy: 91.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11400/60000 (19%)]\tLoss: 1.518429\tAccuracy: 91.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11450/60000 (19%)]\tLoss: 1.516469\tAccuracy: 91.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11500/60000 (19%)]\tLoss: 1.501676\tAccuracy: 91.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11550/60000 (19%)]\tLoss: 1.481402\tAccuracy: 91.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11600/60000 (19%)]\tLoss: 1.462110\tAccuracy: 91.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11650/60000 (19%)]\tLoss: 1.575983\tAccuracy: 91.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11700/60000 (20%)]\tLoss: 1.482267\tAccuracy: 91.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11750/60000 (20%)]\tLoss: 1.480503\tAccuracy: 91.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11800/60000 (20%)]\tLoss: 1.487570\tAccuracy: 91.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11850/60000 (20%)]\tLoss: 1.461588\tAccuracy: 91.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11900/60000 (20%)]\tLoss: 1.466110\tAccuracy: 91.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11950/60000 (20%)]\tLoss: 1.501544\tAccuracy: 91.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12000/60000 (20%)]\tLoss: 1.494134\tAccuracy: 91.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12050/60000 (20%)]\tLoss: 1.498057\tAccuracy: 91.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12100/60000 (20%)]\tLoss: 1.519304\tAccuracy: 91.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12150/60000 (20%)]\tLoss: 1.484037\tAccuracy: 91.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12200/60000 (20%)]\tLoss: 1.466335\tAccuracy: 91.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12250/60000 (20%)]\tLoss: 1.515326\tAccuracy: 91.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12300/60000 (20%)]\tLoss: 1.472334\tAccuracy: 91.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12350/60000 (21%)]\tLoss: 1.516446\tAccuracy: 91.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12400/60000 (21%)]\tLoss: 1.500077\tAccuracy: 91.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12450/60000 (21%)]\tLoss: 1.514216\tAccuracy: 91.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12500/60000 (21%)]\tLoss: 1.550143\tAccuracy: 91.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12550/60000 (21%)]\tLoss: 1.561900\tAccuracy: 91.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12600/60000 (21%)]\tLoss: 1.524064\tAccuracy: 91.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12650/60000 (21%)]\tLoss: 1.496516\tAccuracy: 91.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12700/60000 (21%)]\tLoss: 1.522028\tAccuracy: 91.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12750/60000 (21%)]\tLoss: 1.481614\tAccuracy: 91.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12800/60000 (21%)]\tLoss: 1.491073\tAccuracy: 91.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12850/60000 (21%)]\tLoss: 1.613870\tAccuracy: 91.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12900/60000 (22%)]\tLoss: 1.515971\tAccuracy: 91.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12950/60000 (22%)]\tLoss: 1.461459\tAccuracy: 91.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13000/60000 (22%)]\tLoss: 1.532091\tAccuracy: 91.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13050/60000 (22%)]\tLoss: 1.537724\tAccuracy: 91.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13100/60000 (22%)]\tLoss: 1.512186\tAccuracy: 91.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13150/60000 (22%)]\tLoss: 1.521663\tAccuracy: 91.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13200/60000 (22%)]\tLoss: 1.480023\tAccuracy: 91.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13250/60000 (22%)]\tLoss: 1.461951\tAccuracy: 91.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13300/60000 (22%)]\tLoss: 1.462422\tAccuracy: 91.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13350/60000 (22%)]\tLoss: 1.506410\tAccuracy: 91.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13400/60000 (22%)]\tLoss: 1.498263\tAccuracy: 91.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13450/60000 (22%)]\tLoss: 1.481350\tAccuracy: 91.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13500/60000 (22%)]\tLoss: 1.511913\tAccuracy: 91.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13550/60000 (23%)]\tLoss: 1.573093\tAccuracy: 91.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13600/60000 (23%)]\tLoss: 1.507450\tAccuracy: 91.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13650/60000 (23%)]\tLoss: 1.533498\tAccuracy: 91.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13700/60000 (23%)]\tLoss: 1.461621\tAccuracy: 91.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13750/60000 (23%)]\tLoss: 1.515307\tAccuracy: 91.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13800/60000 (23%)]\tLoss: 1.539578\tAccuracy: 91.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13850/60000 (23%)]\tLoss: 1.530132\tAccuracy: 91.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13900/60000 (23%)]\tLoss: 1.481598\tAccuracy: 91.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13950/60000 (23%)]\tLoss: 1.499612\tAccuracy: 91.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14000/60000 (23%)]\tLoss: 1.545369\tAccuracy: 91.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14050/60000 (23%)]\tLoss: 1.521653\tAccuracy: 91.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14100/60000 (24%)]\tLoss: 1.502694\tAccuracy: 91.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14150/60000 (24%)]\tLoss: 1.529328\tAccuracy: 91.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14200/60000 (24%)]\tLoss: 1.481720\tAccuracy: 91.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14250/60000 (24%)]\tLoss: 1.480867\tAccuracy: 92.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14300/60000 (24%)]\tLoss: 1.526069\tAccuracy: 92.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14350/60000 (24%)]\tLoss: 1.478318\tAccuracy: 92.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14400/60000 (24%)]\tLoss: 1.493473\tAccuracy: 92.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14450/60000 (24%)]\tLoss: 1.508507\tAccuracy: 92.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14500/60000 (24%)]\tLoss: 1.513729\tAccuracy: 92.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14550/60000 (24%)]\tLoss: 1.508062\tAccuracy: 92.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14600/60000 (24%)]\tLoss: 1.477757\tAccuracy: 92.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14650/60000 (24%)]\tLoss: 1.490606\tAccuracy: 92.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14700/60000 (24%)]\tLoss: 1.465610\tAccuracy: 92.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14750/60000 (25%)]\tLoss: 1.486908\tAccuracy: 92.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14800/60000 (25%)]\tLoss: 1.513989\tAccuracy: 92.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14850/60000 (25%)]\tLoss: 1.463340\tAccuracy: 92.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14900/60000 (25%)]\tLoss: 1.547029\tAccuracy: 92.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14950/60000 (25%)]\tLoss: 1.462509\tAccuracy: 92.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15000/60000 (25%)]\tLoss: 1.505292\tAccuracy: 92.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15050/60000 (25%)]\tLoss: 1.495839\tAccuracy: 92.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15100/60000 (25%)]\tLoss: 1.570328\tAccuracy: 92.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15150/60000 (25%)]\tLoss: 1.561212\tAccuracy: 92.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15200/60000 (25%)]\tLoss: 1.488888\tAccuracy: 92.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15250/60000 (25%)]\tLoss: 1.482687\tAccuracy: 92.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15300/60000 (26%)]\tLoss: 1.497550\tAccuracy: 92.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15350/60000 (26%)]\tLoss: 1.514352\tAccuracy: 92.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15400/60000 (26%)]\tLoss: 1.492679\tAccuracy: 92.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15450/60000 (26%)]\tLoss: 1.495391\tAccuracy: 92.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15500/60000 (26%)]\tLoss: 1.501487\tAccuracy: 92.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15550/60000 (26%)]\tLoss: 1.479781\tAccuracy: 92.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15600/60000 (26%)]\tLoss: 1.481620\tAccuracy: 92.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15650/60000 (26%)]\tLoss: 1.485857\tAccuracy: 92.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15700/60000 (26%)]\tLoss: 1.475933\tAccuracy: 92.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15750/60000 (26%)]\tLoss: 1.486438\tAccuracy: 92.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15800/60000 (26%)]\tLoss: 1.530768\tAccuracy: 92.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15850/60000 (26%)]\tLoss: 1.481007\tAccuracy: 92.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15900/60000 (26%)]\tLoss: 1.481733\tAccuracy: 92.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15950/60000 (27%)]\tLoss: 1.480217\tAccuracy: 92.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16000/60000 (27%)]\tLoss: 1.492374\tAccuracy: 92.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16050/60000 (27%)]\tLoss: 1.537288\tAccuracy: 92.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16100/60000 (27%)]\tLoss: 1.499915\tAccuracy: 92.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16150/60000 (27%)]\tLoss: 1.477716\tAccuracy: 92.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16200/60000 (27%)]\tLoss: 1.521378\tAccuracy: 92.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16250/60000 (27%)]\tLoss: 1.513710\tAccuracy: 92.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16300/60000 (27%)]\tLoss: 1.483992\tAccuracy: 92.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16350/60000 (27%)]\tLoss: 1.485241\tAccuracy: 92.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16400/60000 (27%)]\tLoss: 1.497036\tAccuracy: 92.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16450/60000 (27%)]\tLoss: 1.518419\tAccuracy: 92.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16500/60000 (28%)]\tLoss: 1.488743\tAccuracy: 92.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16550/60000 (28%)]\tLoss: 1.465692\tAccuracy: 92.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16600/60000 (28%)]\tLoss: 1.503305\tAccuracy: 92.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16650/60000 (28%)]\tLoss: 1.464307\tAccuracy: 92.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16700/60000 (28%)]\tLoss: 1.537902\tAccuracy: 92.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16750/60000 (28%)]\tLoss: 1.475300\tAccuracy: 92.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16800/60000 (28%)]\tLoss: 1.480727\tAccuracy: 92.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16850/60000 (28%)]\tLoss: 1.490579\tAccuracy: 92.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16900/60000 (28%)]\tLoss: 1.461193\tAccuracy: 92.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16950/60000 (28%)]\tLoss: 1.501650\tAccuracy: 92.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17000/60000 (28%)]\tLoss: 1.486806\tAccuracy: 92.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17050/60000 (28%)]\tLoss: 1.477346\tAccuracy: 92.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17100/60000 (28%)]\tLoss: 1.479952\tAccuracy: 92.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17150/60000 (29%)]\tLoss: 1.501768\tAccuracy: 92.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17200/60000 (29%)]\tLoss: 1.509047\tAccuracy: 92.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17250/60000 (29%)]\tLoss: 1.461205\tAccuracy: 92.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17300/60000 (29%)]\tLoss: 1.489020\tAccuracy: 92.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17350/60000 (29%)]\tLoss: 1.491789\tAccuracy: 92.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17400/60000 (29%)]\tLoss: 1.480270\tAccuracy: 92.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17450/60000 (29%)]\tLoss: 1.521474\tAccuracy: 92.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17500/60000 (29%)]\tLoss: 1.527847\tAccuracy: 92.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17550/60000 (29%)]\tLoss: 1.479101\tAccuracy: 92.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17600/60000 (29%)]\tLoss: 1.539030\tAccuracy: 92.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17650/60000 (29%)]\tLoss: 1.502164\tAccuracy: 92.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17700/60000 (30%)]\tLoss: 1.501300\tAccuracy: 92.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17750/60000 (30%)]\tLoss: 1.503607\tAccuracy: 92.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17800/60000 (30%)]\tLoss: 1.540092\tAccuracy: 92.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17850/60000 (30%)]\tLoss: 1.511712\tAccuracy: 92.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17900/60000 (30%)]\tLoss: 1.461832\tAccuracy: 92.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17950/60000 (30%)]\tLoss: 1.503356\tAccuracy: 92.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18000/60000 (30%)]\tLoss: 1.493042\tAccuracy: 92.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18050/60000 (30%)]\tLoss: 1.470177\tAccuracy: 92.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18100/60000 (30%)]\tLoss: 1.480557\tAccuracy: 92.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18150/60000 (30%)]\tLoss: 1.487371\tAccuracy: 93.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18200/60000 (30%)]\tLoss: 1.485265\tAccuracy: 93.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18250/60000 (30%)]\tLoss: 1.521510\tAccuracy: 93.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18300/60000 (30%)]\tLoss: 1.523525\tAccuracy: 93.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18350/60000 (31%)]\tLoss: 1.499829\tAccuracy: 93.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18400/60000 (31%)]\tLoss: 1.499108\tAccuracy: 93.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18450/60000 (31%)]\tLoss: 1.503633\tAccuracy: 93.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18500/60000 (31%)]\tLoss: 1.488679\tAccuracy: 93.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18550/60000 (31%)]\tLoss: 1.465769\tAccuracy: 93.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18600/60000 (31%)]\tLoss: 1.468784\tAccuracy: 93.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18650/60000 (31%)]\tLoss: 1.542217\tAccuracy: 93.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18700/60000 (31%)]\tLoss: 1.501544\tAccuracy: 93.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18750/60000 (31%)]\tLoss: 1.473465\tAccuracy: 93.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18800/60000 (31%)]\tLoss: 1.510753\tAccuracy: 93.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18850/60000 (31%)]\tLoss: 1.500586\tAccuracy: 93.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18900/60000 (32%)]\tLoss: 1.491354\tAccuracy: 93.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18950/60000 (32%)]\tLoss: 1.467602\tAccuracy: 93.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19000/60000 (32%)]\tLoss: 1.486897\tAccuracy: 93.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19050/60000 (32%)]\tLoss: 1.478203\tAccuracy: 93.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19100/60000 (32%)]\tLoss: 1.506817\tAccuracy: 93.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19150/60000 (32%)]\tLoss: 1.472531\tAccuracy: 93.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19200/60000 (32%)]\tLoss: 1.461722\tAccuracy: 93.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19250/60000 (32%)]\tLoss: 1.462239\tAccuracy: 93.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19300/60000 (32%)]\tLoss: 1.473346\tAccuracy: 93.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19350/60000 (32%)]\tLoss: 1.481150\tAccuracy: 93.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19400/60000 (32%)]\tLoss: 1.523347\tAccuracy: 93.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19450/60000 (32%)]\tLoss: 1.496892\tAccuracy: 93.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19500/60000 (32%)]\tLoss: 1.478110\tAccuracy: 93.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19550/60000 (33%)]\tLoss: 1.522161\tAccuracy: 93.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19600/60000 (33%)]\tLoss: 1.466775\tAccuracy: 93.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19650/60000 (33%)]\tLoss: 1.484213\tAccuracy: 93.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19700/60000 (33%)]\tLoss: 1.517290\tAccuracy: 93.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19750/60000 (33%)]\tLoss: 1.489460\tAccuracy: 93.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19800/60000 (33%)]\tLoss: 1.481183\tAccuracy: 93.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19850/60000 (33%)]\tLoss: 1.481209\tAccuracy: 93.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19900/60000 (33%)]\tLoss: 1.514117\tAccuracy: 93.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19950/60000 (33%)]\tLoss: 1.481131\tAccuracy: 93.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20000/60000 (33%)]\tLoss: 1.505192\tAccuracy: 93.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20050/60000 (33%)]\tLoss: 1.469238\tAccuracy: 93.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20100/60000 (34%)]\tLoss: 1.468261\tAccuracy: 93.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20150/60000 (34%)]\tLoss: 1.539546\tAccuracy: 93.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20200/60000 (34%)]\tLoss: 1.481167\tAccuracy: 93.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20250/60000 (34%)]\tLoss: 1.505889\tAccuracy: 93.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20300/60000 (34%)]\tLoss: 1.467709\tAccuracy: 93.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20350/60000 (34%)]\tLoss: 1.486506\tAccuracy: 93.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20400/60000 (34%)]\tLoss: 1.465220\tAccuracy: 93.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20450/60000 (34%)]\tLoss: 1.500621\tAccuracy: 93.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20500/60000 (34%)]\tLoss: 1.497176\tAccuracy: 93.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20550/60000 (34%)]\tLoss: 1.467288\tAccuracy: 93.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20600/60000 (34%)]\tLoss: 1.486070\tAccuracy: 93.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20650/60000 (34%)]\tLoss: 1.481241\tAccuracy: 93.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20700/60000 (34%)]\tLoss: 1.480869\tAccuracy: 93.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20750/60000 (35%)]\tLoss: 1.479840\tAccuracy: 93.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20800/60000 (35%)]\tLoss: 1.461364\tAccuracy: 93.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20850/60000 (35%)]\tLoss: 1.502791\tAccuracy: 93.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20900/60000 (35%)]\tLoss: 1.465182\tAccuracy: 93.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20950/60000 (35%)]\tLoss: 1.543177\tAccuracy: 93.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21000/60000 (35%)]\tLoss: 1.461377\tAccuracy: 93.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21050/60000 (35%)]\tLoss: 1.461191\tAccuracy: 93.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21100/60000 (35%)]\tLoss: 1.502309\tAccuracy: 93.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21150/60000 (35%)]\tLoss: 1.470193\tAccuracy: 93.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21200/60000 (35%)]\tLoss: 1.504009\tAccuracy: 93.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21250/60000 (35%)]\tLoss: 1.470914\tAccuracy: 93.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21300/60000 (36%)]\tLoss: 1.529067\tAccuracy: 93.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21350/60000 (36%)]\tLoss: 1.462592\tAccuracy: 93.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21400/60000 (36%)]\tLoss: 1.555079\tAccuracy: 93.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21450/60000 (36%)]\tLoss: 1.469233\tAccuracy: 93.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21500/60000 (36%)]\tLoss: 1.508420\tAccuracy: 93.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21550/60000 (36%)]\tLoss: 1.502554\tAccuracy: 93.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21600/60000 (36%)]\tLoss: 1.499076\tAccuracy: 93.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21650/60000 (36%)]\tLoss: 1.482229\tAccuracy: 93.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21700/60000 (36%)]\tLoss: 1.461876\tAccuracy: 93.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21750/60000 (36%)]\tLoss: 1.556296\tAccuracy: 93.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21800/60000 (36%)]\tLoss: 1.475212\tAccuracy: 93.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21850/60000 (36%)]\tLoss: 1.481272\tAccuracy: 93.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21900/60000 (36%)]\tLoss: 1.498429\tAccuracy: 93.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21950/60000 (37%)]\tLoss: 1.509507\tAccuracy: 93.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22000/60000 (37%)]\tLoss: 1.523210\tAccuracy: 93.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22050/60000 (37%)]\tLoss: 1.505340\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22100/60000 (37%)]\tLoss: 1.552818\tAccuracy: 93.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22150/60000 (37%)]\tLoss: 1.497607\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22200/60000 (37%)]\tLoss: 1.500933\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22250/60000 (37%)]\tLoss: 1.572716\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22300/60000 (37%)]\tLoss: 1.513973\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22350/60000 (37%)]\tLoss: 1.519497\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22400/60000 (37%)]\tLoss: 1.494104\tAccuracy: 93.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22450/60000 (37%)]\tLoss: 1.488914\tAccuracy: 93.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22500/60000 (38%)]\tLoss: 1.482887\tAccuracy: 93.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22550/60000 (38%)]\tLoss: 1.557072\tAccuracy: 93.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22600/60000 (38%)]\tLoss: 1.491786\tAccuracy: 93.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22650/60000 (38%)]\tLoss: 1.484306\tAccuracy: 93.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22700/60000 (38%)]\tLoss: 1.527113\tAccuracy: 93.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22750/60000 (38%)]\tLoss: 1.495702\tAccuracy: 93.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22800/60000 (38%)]\tLoss: 1.485867\tAccuracy: 93.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22850/60000 (38%)]\tLoss: 1.535472\tAccuracy: 93.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22900/60000 (38%)]\tLoss: 1.482608\tAccuracy: 93.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22950/60000 (38%)]\tLoss: 1.554323\tAccuracy: 93.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23000/60000 (38%)]\tLoss: 1.467348\tAccuracy: 93.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23050/60000 (38%)]\tLoss: 1.476831\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23100/60000 (38%)]\tLoss: 1.517092\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23150/60000 (39%)]\tLoss: 1.478675\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23200/60000 (39%)]\tLoss: 1.482981\tAccuracy: 93.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23250/60000 (39%)]\tLoss: 1.535702\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23300/60000 (39%)]\tLoss: 1.530233\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23350/60000 (39%)]\tLoss: 1.561564\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23400/60000 (39%)]\tLoss: 1.515324\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23450/60000 (39%)]\tLoss: 1.503135\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23500/60000 (39%)]\tLoss: 1.503235\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23550/60000 (39%)]\tLoss: 1.485669\tAccuracy: 93.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23600/60000 (39%)]\tLoss: 1.591641\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23650/60000 (39%)]\tLoss: 1.520648\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23700/60000 (40%)]\tLoss: 1.502621\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23750/60000 (40%)]\tLoss: 1.600719\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23800/60000 (40%)]\tLoss: 1.477519\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23850/60000 (40%)]\tLoss: 1.519196\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23900/60000 (40%)]\tLoss: 1.541221\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23950/60000 (40%)]\tLoss: 1.558723\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24000/60000 (40%)]\tLoss: 1.513490\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24050/60000 (40%)]\tLoss: 1.532393\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24100/60000 (40%)]\tLoss: 1.464950\tAccuracy: 93.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24150/60000 (40%)]\tLoss: 1.509551\tAccuracy: 93.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24200/60000 (40%)]\tLoss: 1.529256\tAccuracy: 93.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24250/60000 (40%)]\tLoss: 1.474698\tAccuracy: 93.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24300/60000 (40%)]\tLoss: 1.556107\tAccuracy: 93.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24350/60000 (41%)]\tLoss: 1.499973\tAccuracy: 93.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24400/60000 (41%)]\tLoss: 1.504257\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24450/60000 (41%)]\tLoss: 1.525340\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24500/60000 (41%)]\tLoss: 1.544088\tAccuracy: 93.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24550/60000 (41%)]\tLoss: 1.507684\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24600/60000 (41%)]\tLoss: 1.510092\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24650/60000 (41%)]\tLoss: 1.479301\tAccuracy: 93.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24700/60000 (41%)]\tLoss: 1.581096\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24750/60000 (41%)]\tLoss: 1.537353\tAccuracy: 93.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24800/60000 (41%)]\tLoss: 1.518029\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24850/60000 (41%)]\tLoss: 1.500513\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24900/60000 (42%)]\tLoss: 1.494685\tAccuracy: 93.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24950/60000 (42%)]\tLoss: 1.522014\tAccuracy: 93.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25000/60000 (42%)]\tLoss: 1.505007\tAccuracy: 93.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25050/60000 (42%)]\tLoss: 1.490365\tAccuracy: 93.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25100/60000 (42%)]\tLoss: 1.481430\tAccuracy: 93.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25150/60000 (42%)]\tLoss: 1.489568\tAccuracy: 93.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25200/60000 (42%)]\tLoss: 1.524363\tAccuracy: 93.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25250/60000 (42%)]\tLoss: 1.462669\tAccuracy: 93.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25300/60000 (42%)]\tLoss: 1.461471\tAccuracy: 93.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25350/60000 (42%)]\tLoss: 1.463006\tAccuracy: 93.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25400/60000 (42%)]\tLoss: 1.517080\tAccuracy: 93.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25450/60000 (42%)]\tLoss: 1.487587\tAccuracy: 93.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25500/60000 (42%)]\tLoss: 1.465094\tAccuracy: 93.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25550/60000 (43%)]\tLoss: 1.493695\tAccuracy: 93.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25600/60000 (43%)]\tLoss: 1.474813\tAccuracy: 93.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25650/60000 (43%)]\tLoss: 1.501506\tAccuracy: 93.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25700/60000 (43%)]\tLoss: 1.470485\tAccuracy: 93.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25750/60000 (43%)]\tLoss: 1.546330\tAccuracy: 93.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25800/60000 (43%)]\tLoss: 1.491972\tAccuracy: 93.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25850/60000 (43%)]\tLoss: 1.507754\tAccuracy: 93.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25900/60000 (43%)]\tLoss: 1.500687\tAccuracy: 93.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25950/60000 (43%)]\tLoss: 1.483913\tAccuracy: 93.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26000/60000 (43%)]\tLoss: 1.461238\tAccuracy: 93.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26050/60000 (43%)]\tLoss: 1.515772\tAccuracy: 93.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26100/60000 (44%)]\tLoss: 1.486649\tAccuracy: 94.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26150/60000 (44%)]\tLoss: 1.481831\tAccuracy: 94.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26200/60000 (44%)]\tLoss: 1.484937\tAccuracy: 94.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26250/60000 (44%)]\tLoss: 1.509688\tAccuracy: 94.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26300/60000 (44%)]\tLoss: 1.505081\tAccuracy: 94.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26350/60000 (44%)]\tLoss: 1.505298\tAccuracy: 94.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26400/60000 (44%)]\tLoss: 1.480456\tAccuracy: 94.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26450/60000 (44%)]\tLoss: 1.508370\tAccuracy: 94.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26500/60000 (44%)]\tLoss: 1.467208\tAccuracy: 94.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26550/60000 (44%)]\tLoss: 1.501350\tAccuracy: 94.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26600/60000 (44%)]\tLoss: 1.531027\tAccuracy: 94.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26650/60000 (44%)]\tLoss: 1.508745\tAccuracy: 94.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26700/60000 (44%)]\tLoss: 1.479229\tAccuracy: 94.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26750/60000 (45%)]\tLoss: 1.515194\tAccuracy: 94.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26800/60000 (45%)]\tLoss: 1.499551\tAccuracy: 94.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26850/60000 (45%)]\tLoss: 1.500226\tAccuracy: 94.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26900/60000 (45%)]\tLoss: 1.483336\tAccuracy: 94.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26950/60000 (45%)]\tLoss: 1.462532\tAccuracy: 94.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27000/60000 (45%)]\tLoss: 1.497697\tAccuracy: 94.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27050/60000 (45%)]\tLoss: 1.481153\tAccuracy: 94.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27100/60000 (45%)]\tLoss: 1.506817\tAccuracy: 94.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27150/60000 (45%)]\tLoss: 1.514979\tAccuracy: 94.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27200/60000 (45%)]\tLoss: 1.464087\tAccuracy: 94.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27250/60000 (45%)]\tLoss: 1.480474\tAccuracy: 94.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27300/60000 (46%)]\tLoss: 1.484561\tAccuracy: 94.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27350/60000 (46%)]\tLoss: 1.483463\tAccuracy: 94.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27400/60000 (46%)]\tLoss: 1.461502\tAccuracy: 94.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27450/60000 (46%)]\tLoss: 1.499494\tAccuracy: 94.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27500/60000 (46%)]\tLoss: 1.468007\tAccuracy: 94.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27550/60000 (46%)]\tLoss: 1.480677\tAccuracy: 94.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27600/60000 (46%)]\tLoss: 1.489513\tAccuracy: 94.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27650/60000 (46%)]\tLoss: 1.477384\tAccuracy: 94.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27700/60000 (46%)]\tLoss: 1.485019\tAccuracy: 94.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27750/60000 (46%)]\tLoss: 1.484071\tAccuracy: 94.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27800/60000 (46%)]\tLoss: 1.463699\tAccuracy: 94.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27850/60000 (46%)]\tLoss: 1.564770\tAccuracy: 94.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27900/60000 (46%)]\tLoss: 1.535192\tAccuracy: 94.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27950/60000 (47%)]\tLoss: 1.524168\tAccuracy: 94.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28000/60000 (47%)]\tLoss: 1.483606\tAccuracy: 94.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28050/60000 (47%)]\tLoss: 1.502239\tAccuracy: 94.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28100/60000 (47%)]\tLoss: 1.508139\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28150/60000 (47%)]\tLoss: 1.500901\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28200/60000 (47%)]\tLoss: 1.481293\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28250/60000 (47%)]\tLoss: 1.465516\tAccuracy: 94.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28300/60000 (47%)]\tLoss: 1.479762\tAccuracy: 94.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28350/60000 (47%)]\tLoss: 1.483746\tAccuracy: 94.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28400/60000 (47%)]\tLoss: 1.472193\tAccuracy: 94.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28450/60000 (47%)]\tLoss: 1.487323\tAccuracy: 94.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28500/60000 (48%)]\tLoss: 1.496641\tAccuracy: 94.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28550/60000 (48%)]\tLoss: 1.498106\tAccuracy: 94.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28600/60000 (48%)]\tLoss: 1.524520\tAccuracy: 94.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28650/60000 (48%)]\tLoss: 1.468556\tAccuracy: 94.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28700/60000 (48%)]\tLoss: 1.505362\tAccuracy: 94.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28750/60000 (48%)]\tLoss: 1.471845\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28800/60000 (48%)]\tLoss: 1.483160\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28850/60000 (48%)]\tLoss: 1.462770\tAccuracy: 94.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28900/60000 (48%)]\tLoss: 1.508238\tAccuracy: 94.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28950/60000 (48%)]\tLoss: 1.497182\tAccuracy: 94.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29000/60000 (48%)]\tLoss: 1.480437\tAccuracy: 94.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29050/60000 (48%)]\tLoss: 1.490420\tAccuracy: 94.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29100/60000 (48%)]\tLoss: 1.501688\tAccuracy: 94.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29150/60000 (49%)]\tLoss: 1.495912\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29200/60000 (49%)]\tLoss: 1.494628\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29250/60000 (49%)]\tLoss: 1.507203\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29300/60000 (49%)]\tLoss: 1.469343\tAccuracy: 94.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29350/60000 (49%)]\tLoss: 1.546014\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29400/60000 (49%)]\tLoss: 1.536677\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29450/60000 (49%)]\tLoss: 1.486829\tAccuracy: 94.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29500/60000 (49%)]\tLoss: 1.514680\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29550/60000 (49%)]\tLoss: 1.462989\tAccuracy: 94.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29600/60000 (49%)]\tLoss: 1.492187\tAccuracy: 94.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29650/60000 (49%)]\tLoss: 1.462040\tAccuracy: 94.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29700/60000 (50%)]\tLoss: 1.482592\tAccuracy: 94.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29750/60000 (50%)]\tLoss: 1.463971\tAccuracy: 94.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29800/60000 (50%)]\tLoss: 1.482176\tAccuracy: 94.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29850/60000 (50%)]\tLoss: 1.469248\tAccuracy: 94.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29900/60000 (50%)]\tLoss: 1.507325\tAccuracy: 94.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29950/60000 (50%)]\tLoss: 1.501866\tAccuracy: 94.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30000/60000 (50%)]\tLoss: 1.482153\tAccuracy: 94.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30050/60000 (50%)]\tLoss: 1.481968\tAccuracy: 94.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30100/60000 (50%)]\tLoss: 1.555868\tAccuracy: 94.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30150/60000 (50%)]\tLoss: 1.481364\tAccuracy: 94.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30200/60000 (50%)]\tLoss: 1.469849\tAccuracy: 94.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30250/60000 (50%)]\tLoss: 1.484642\tAccuracy: 94.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30300/60000 (50%)]\tLoss: 1.509308\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30350/60000 (51%)]\tLoss: 1.495275\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30400/60000 (51%)]\tLoss: 1.489209\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30450/60000 (51%)]\tLoss: 1.510069\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30500/60000 (51%)]\tLoss: 1.486700\tAccuracy: 94.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30550/60000 (51%)]\tLoss: 1.509802\tAccuracy: 94.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30600/60000 (51%)]\tLoss: 1.484271\tAccuracy: 94.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30650/60000 (51%)]\tLoss: 1.481046\tAccuracy: 94.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30700/60000 (51%)]\tLoss: 1.504533\tAccuracy: 94.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30750/60000 (51%)]\tLoss: 1.487158\tAccuracy: 94.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30800/60000 (51%)]\tLoss: 1.539179\tAccuracy: 94.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30850/60000 (51%)]\tLoss: 1.461546\tAccuracy: 94.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30900/60000 (52%)]\tLoss: 1.516921\tAccuracy: 94.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30950/60000 (52%)]\tLoss: 1.502342\tAccuracy: 94.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31000/60000 (52%)]\tLoss: 1.482372\tAccuracy: 94.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31050/60000 (52%)]\tLoss: 1.512290\tAccuracy: 94.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31100/60000 (52%)]\tLoss: 1.511559\tAccuracy: 94.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31150/60000 (52%)]\tLoss: 1.481448\tAccuracy: 94.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31200/60000 (52%)]\tLoss: 1.502866\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31250/60000 (52%)]\tLoss: 1.501398\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31300/60000 (52%)]\tLoss: 1.519261\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31350/60000 (52%)]\tLoss: 1.572771\tAccuracy: 94.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31400/60000 (52%)]\tLoss: 1.482942\tAccuracy: 94.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31450/60000 (52%)]\tLoss: 1.461347\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31500/60000 (52%)]\tLoss: 1.544865\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31550/60000 (53%)]\tLoss: 1.500037\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31600/60000 (53%)]\tLoss: 1.525937\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31650/60000 (53%)]\tLoss: 1.481747\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31700/60000 (53%)]\tLoss: 1.497807\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31750/60000 (53%)]\tLoss: 1.494979\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31800/60000 (53%)]\tLoss: 1.527954\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31850/60000 (53%)]\tLoss: 1.557200\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31900/60000 (53%)]\tLoss: 1.473455\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31950/60000 (53%)]\tLoss: 1.521077\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32000/60000 (53%)]\tLoss: 1.461280\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32050/60000 (53%)]\tLoss: 1.546999\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32100/60000 (54%)]\tLoss: 1.461222\tAccuracy: 94.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32150/60000 (54%)]\tLoss: 1.502035\tAccuracy: 94.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32200/60000 (54%)]\tLoss: 1.473813\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32250/60000 (54%)]\tLoss: 1.483365\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32300/60000 (54%)]\tLoss: 1.541620\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32350/60000 (54%)]\tLoss: 1.489167\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32400/60000 (54%)]\tLoss: 1.485701\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32450/60000 (54%)]\tLoss: 1.559504\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32500/60000 (54%)]\tLoss: 1.462904\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32550/60000 (54%)]\tLoss: 1.513269\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32600/60000 (54%)]\tLoss: 1.539812\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32650/60000 (54%)]\tLoss: 1.501180\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32700/60000 (54%)]\tLoss: 1.560793\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32750/60000 (55%)]\tLoss: 1.492290\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32800/60000 (55%)]\tLoss: 1.502998\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32850/60000 (55%)]\tLoss: 1.522732\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32900/60000 (55%)]\tLoss: 1.462199\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32950/60000 (55%)]\tLoss: 1.485188\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33000/60000 (55%)]\tLoss: 1.522545\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33050/60000 (55%)]\tLoss: 1.531369\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33100/60000 (55%)]\tLoss: 1.551291\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33150/60000 (55%)]\tLoss: 1.503456\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33200/60000 (55%)]\tLoss: 1.481369\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33250/60000 (55%)]\tLoss: 1.518331\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33300/60000 (56%)]\tLoss: 1.546404\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33350/60000 (56%)]\tLoss: 1.482521\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33400/60000 (56%)]\tLoss: 1.483224\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33450/60000 (56%)]\tLoss: 1.461672\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33500/60000 (56%)]\tLoss: 1.480605\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33550/60000 (56%)]\tLoss: 1.461192\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33600/60000 (56%)]\tLoss: 1.515424\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33650/60000 (56%)]\tLoss: 1.473130\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33700/60000 (56%)]\tLoss: 1.493027\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33750/60000 (56%)]\tLoss: 1.517473\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33800/60000 (56%)]\tLoss: 1.489082\tAccuracy: 94.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33850/60000 (56%)]\tLoss: 1.497985\tAccuracy: 94.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33900/60000 (56%)]\tLoss: 1.490388\tAccuracy: 94.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33950/60000 (57%)]\tLoss: 1.500467\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34000/60000 (57%)]\tLoss: 1.489608\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34050/60000 (57%)]\tLoss: 1.497397\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34100/60000 (57%)]\tLoss: 1.511560\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34150/60000 (57%)]\tLoss: 1.488368\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34200/60000 (57%)]\tLoss: 1.559498\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34250/60000 (57%)]\tLoss: 1.462128\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34300/60000 (57%)]\tLoss: 1.505731\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34350/60000 (57%)]\tLoss: 1.545769\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34400/60000 (57%)]\tLoss: 1.481560\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34450/60000 (57%)]\tLoss: 1.522907\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34500/60000 (58%)]\tLoss: 1.503762\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34550/60000 (58%)]\tLoss: 1.525992\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34600/60000 (58%)]\tLoss: 1.540168\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34650/60000 (58%)]\tLoss: 1.488182\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34700/60000 (58%)]\tLoss: 1.482001\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34750/60000 (58%)]\tLoss: 1.529908\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34800/60000 (58%)]\tLoss: 1.481945\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34850/60000 (58%)]\tLoss: 1.483763\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34900/60000 (58%)]\tLoss: 1.498751\tAccuracy: 94.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34950/60000 (58%)]\tLoss: 1.524534\tAccuracy: 94.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35000/60000 (58%)]\tLoss: 1.471367\tAccuracy: 94.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35050/60000 (58%)]\tLoss: 1.481706\tAccuracy: 94.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35100/60000 (58%)]\tLoss: 1.502631\tAccuracy: 94.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35150/60000 (59%)]\tLoss: 1.487192\tAccuracy: 94.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35200/60000 (59%)]\tLoss: 1.481086\tAccuracy: 94.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35250/60000 (59%)]\tLoss: 1.500804\tAccuracy: 94.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35300/60000 (59%)]\tLoss: 1.480706\tAccuracy: 94.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35350/60000 (59%)]\tLoss: 1.481135\tAccuracy: 94.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35400/60000 (59%)]\tLoss: 1.484280\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35450/60000 (59%)]\tLoss: 1.501085\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35500/60000 (59%)]\tLoss: 1.483902\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35550/60000 (59%)]\tLoss: 1.482259\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35600/60000 (59%)]\tLoss: 1.497293\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35650/60000 (59%)]\tLoss: 1.550344\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35700/60000 (60%)]\tLoss: 1.545415\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35750/60000 (60%)]\tLoss: 1.464884\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35800/60000 (60%)]\tLoss: 1.517284\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35850/60000 (60%)]\tLoss: 1.530145\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35900/60000 (60%)]\tLoss: 1.481747\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35950/60000 (60%)]\tLoss: 1.507202\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36000/60000 (60%)]\tLoss: 1.462304\tAccuracy: 94.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36050/60000 (60%)]\tLoss: 1.462002\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36100/60000 (60%)]\tLoss: 1.500967\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36150/60000 (60%)]\tLoss: 1.483155\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36200/60000 (60%)]\tLoss: 1.541378\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36250/60000 (60%)]\tLoss: 1.484555\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36300/60000 (60%)]\tLoss: 1.507973\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36350/60000 (61%)]\tLoss: 1.492955\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36400/60000 (61%)]\tLoss: 1.468110\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36450/60000 (61%)]\tLoss: 1.481088\tAccuracy: 94.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36500/60000 (61%)]\tLoss: 1.462387\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36550/60000 (61%)]\tLoss: 1.484670\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36600/60000 (61%)]\tLoss: 1.501043\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36650/60000 (61%)]\tLoss: 1.511635\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36700/60000 (61%)]\tLoss: 1.483323\tAccuracy: 94.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36750/60000 (61%)]\tLoss: 1.487457\tAccuracy: 94.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36800/60000 (61%)]\tLoss: 1.461587\tAccuracy: 94.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36850/60000 (61%)]\tLoss: 1.514080\tAccuracy: 94.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36900/60000 (62%)]\tLoss: 1.486025\tAccuracy: 94.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36950/60000 (62%)]\tLoss: 1.480812\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37000/60000 (62%)]\tLoss: 1.482915\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37050/60000 (62%)]\tLoss: 1.485876\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37100/60000 (62%)]\tLoss: 1.502361\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37150/60000 (62%)]\tLoss: 1.524835\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37200/60000 (62%)]\tLoss: 1.545007\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37250/60000 (62%)]\tLoss: 1.493551\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37300/60000 (62%)]\tLoss: 1.463111\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37350/60000 (62%)]\tLoss: 1.481185\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37400/60000 (62%)]\tLoss: 1.490670\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37450/60000 (62%)]\tLoss: 1.480603\tAccuracy: 94.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37500/60000 (62%)]\tLoss: 1.498795\tAccuracy: 94.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37550/60000 (63%)]\tLoss: 1.484798\tAccuracy: 94.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37600/60000 (63%)]\tLoss: 1.461335\tAccuracy: 94.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37650/60000 (63%)]\tLoss: 1.463831\tAccuracy: 94.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37700/60000 (63%)]\tLoss: 1.466559\tAccuracy: 94.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37750/60000 (63%)]\tLoss: 1.483437\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37800/60000 (63%)]\tLoss: 1.483303\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37850/60000 (63%)]\tLoss: 1.478995\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37900/60000 (63%)]\tLoss: 1.520423\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37950/60000 (63%)]\tLoss: 1.491946\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38000/60000 (63%)]\tLoss: 1.499593\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38050/60000 (63%)]\tLoss: 1.501163\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38100/60000 (64%)]\tLoss: 1.469147\tAccuracy: 94.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38150/60000 (64%)]\tLoss: 1.481232\tAccuracy: 94.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38200/60000 (64%)]\tLoss: 1.476680\tAccuracy: 94.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38250/60000 (64%)]\tLoss: 1.482025\tAccuracy: 94.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38300/60000 (64%)]\tLoss: 1.461953\tAccuracy: 94.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38350/60000 (64%)]\tLoss: 1.479110\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38400/60000 (64%)]\tLoss: 1.521871\tAccuracy: 94.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38450/60000 (64%)]\tLoss: 1.480424\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38500/60000 (64%)]\tLoss: 1.543381\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38550/60000 (64%)]\tLoss: 1.482628\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38600/60000 (64%)]\tLoss: 1.506047\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38650/60000 (64%)]\tLoss: 1.501191\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38700/60000 (64%)]\tLoss: 1.471570\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38750/60000 (65%)]\tLoss: 1.481252\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38800/60000 (65%)]\tLoss: 1.527488\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38850/60000 (65%)]\tLoss: 1.480830\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38900/60000 (65%)]\tLoss: 1.461849\tAccuracy: 94.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38950/60000 (65%)]\tLoss: 1.494612\tAccuracy: 94.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39000/60000 (65%)]\tLoss: 1.464574\tAccuracy: 94.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39050/60000 (65%)]\tLoss: 1.515590\tAccuracy: 94.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39100/60000 (65%)]\tLoss: 1.512678\tAccuracy: 94.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39150/60000 (65%)]\tLoss: 1.479268\tAccuracy: 94.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39200/60000 (65%)]\tLoss: 1.491593\tAccuracy: 94.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39250/60000 (65%)]\tLoss: 1.461944\tAccuracy: 94.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39300/60000 (66%)]\tLoss: 1.461181\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39350/60000 (66%)]\tLoss: 1.505200\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39400/60000 (66%)]\tLoss: 1.477928\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39450/60000 (66%)]\tLoss: 1.499326\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39500/60000 (66%)]\tLoss: 1.481794\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39550/60000 (66%)]\tLoss: 1.505527\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39600/60000 (66%)]\tLoss: 1.509752\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39650/60000 (66%)]\tLoss: 1.478852\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39700/60000 (66%)]\tLoss: 1.563039\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39750/60000 (66%)]\tLoss: 1.560403\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39800/60000 (66%)]\tLoss: 1.557536\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39850/60000 (66%)]\tLoss: 1.514767\tAccuracy: 94.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39900/60000 (66%)]\tLoss: 1.497519\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39950/60000 (67%)]\tLoss: 1.461152\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40000/60000 (67%)]\tLoss: 1.480751\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40050/60000 (67%)]\tLoss: 1.514388\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40100/60000 (67%)]\tLoss: 1.483316\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40150/60000 (67%)]\tLoss: 1.555232\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40200/60000 (67%)]\tLoss: 1.462067\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40250/60000 (67%)]\tLoss: 1.550662\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40300/60000 (67%)]\tLoss: 1.481299\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40350/60000 (67%)]\tLoss: 1.461179\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40400/60000 (67%)]\tLoss: 1.485768\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40450/60000 (67%)]\tLoss: 1.481164\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40500/60000 (68%)]\tLoss: 1.510380\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40550/60000 (68%)]\tLoss: 1.507598\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40600/60000 (68%)]\tLoss: 1.481128\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40650/60000 (68%)]\tLoss: 1.482015\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40700/60000 (68%)]\tLoss: 1.481194\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40750/60000 (68%)]\tLoss: 1.483350\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40800/60000 (68%)]\tLoss: 1.537594\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40850/60000 (68%)]\tLoss: 1.551904\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40900/60000 (68%)]\tLoss: 1.502950\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40950/60000 (68%)]\tLoss: 1.465220\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41000/60000 (68%)]\tLoss: 1.515162\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41050/60000 (68%)]\tLoss: 1.461363\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41100/60000 (68%)]\tLoss: 1.496403\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41150/60000 (69%)]\tLoss: 1.485184\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41200/60000 (69%)]\tLoss: 1.548756\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41250/60000 (69%)]\tLoss: 1.484533\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41300/60000 (69%)]\tLoss: 1.501083\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41350/60000 (69%)]\tLoss: 1.481138\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41400/60000 (69%)]\tLoss: 1.514919\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41450/60000 (69%)]\tLoss: 1.482923\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41500/60000 (69%)]\tLoss: 1.463045\tAccuracy: 94.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41550/60000 (69%)]\tLoss: 1.461290\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41600/60000 (69%)]\tLoss: 1.519113\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41650/60000 (69%)]\tLoss: 1.520833\tAccuracy: 94.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41700/60000 (70%)]\tLoss: 1.501447\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41750/60000 (70%)]\tLoss: 1.478472\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41800/60000 (70%)]\tLoss: 1.501257\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41850/60000 (70%)]\tLoss: 1.515699\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41900/60000 (70%)]\tLoss: 1.507866\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41950/60000 (70%)]\tLoss: 1.494839\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42000/60000 (70%)]\tLoss: 1.461161\tAccuracy: 94.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42050/60000 (70%)]\tLoss: 1.482894\tAccuracy: 94.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42100/60000 (70%)]\tLoss: 1.481474\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42150/60000 (70%)]\tLoss: 1.511960\tAccuracy: 94.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42200/60000 (70%)]\tLoss: 1.463694\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42250/60000 (70%)]\tLoss: 1.498544\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42300/60000 (70%)]\tLoss: 1.501101\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42350/60000 (71%)]\tLoss: 1.461622\tAccuracy: 94.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42400/60000 (71%)]\tLoss: 1.505373\tAccuracy: 94.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42450/60000 (71%)]\tLoss: 1.480912\tAccuracy: 94.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42500/60000 (71%)]\tLoss: 1.484392\tAccuracy: 94.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42550/60000 (71%)]\tLoss: 1.461667\tAccuracy: 95.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42600/60000 (71%)]\tLoss: 1.481961\tAccuracy: 95.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42650/60000 (71%)]\tLoss: 1.480156\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42700/60000 (71%)]\tLoss: 1.497461\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42750/60000 (71%)]\tLoss: 1.522029\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42800/60000 (71%)]\tLoss: 1.497065\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42850/60000 (71%)]\tLoss: 1.461163\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42900/60000 (72%)]\tLoss: 1.506462\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42950/60000 (72%)]\tLoss: 1.481499\tAccuracy: 95.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43000/60000 (72%)]\tLoss: 1.463688\tAccuracy: 95.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43050/60000 (72%)]\tLoss: 1.478065\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43100/60000 (72%)]\tLoss: 1.504147\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43150/60000 (72%)]\tLoss: 1.465721\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43200/60000 (72%)]\tLoss: 1.523255\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43250/60000 (72%)]\tLoss: 1.482980\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43300/60000 (72%)]\tLoss: 1.480514\tAccuracy: 95.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43350/60000 (72%)]\tLoss: 1.463530\tAccuracy: 95.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43400/60000 (72%)]\tLoss: 1.486740\tAccuracy: 95.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43450/60000 (72%)]\tLoss: 1.478112\tAccuracy: 95.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43500/60000 (72%)]\tLoss: 1.497020\tAccuracy: 95.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43550/60000 (73%)]\tLoss: 1.478132\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43600/60000 (73%)]\tLoss: 1.463073\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43650/60000 (73%)]\tLoss: 1.498040\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43700/60000 (73%)]\tLoss: 1.463783\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43750/60000 (73%)]\tLoss: 1.461169\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43800/60000 (73%)]\tLoss: 1.497214\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43850/60000 (73%)]\tLoss: 1.554820\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43900/60000 (73%)]\tLoss: 1.461498\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43950/60000 (73%)]\tLoss: 1.508392\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44000/60000 (73%)]\tLoss: 1.461403\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44050/60000 (73%)]\tLoss: 1.516754\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44100/60000 (74%)]\tLoss: 1.461316\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44150/60000 (74%)]\tLoss: 1.481283\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44200/60000 (74%)]\tLoss: 1.501354\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44250/60000 (74%)]\tLoss: 1.519390\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44300/60000 (74%)]\tLoss: 1.536761\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44350/60000 (74%)]\tLoss: 1.498744\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44400/60000 (74%)]\tLoss: 1.481748\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44450/60000 (74%)]\tLoss: 1.486569\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44500/60000 (74%)]\tLoss: 1.481445\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44550/60000 (74%)]\tLoss: 1.543304\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44600/60000 (74%)]\tLoss: 1.473923\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44650/60000 (74%)]\tLoss: 1.461208\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44700/60000 (74%)]\tLoss: 1.582440\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44750/60000 (75%)]\tLoss: 1.481218\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44800/60000 (75%)]\tLoss: 1.498954\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44850/60000 (75%)]\tLoss: 1.481127\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44900/60000 (75%)]\tLoss: 1.464594\tAccuracy: 95.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44950/60000 (75%)]\tLoss: 1.485925\tAccuracy: 95.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45000/60000 (75%)]\tLoss: 1.497202\tAccuracy: 95.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45050/60000 (75%)]\tLoss: 1.462102\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45100/60000 (75%)]\tLoss: 1.462380\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45150/60000 (75%)]\tLoss: 1.461464\tAccuracy: 95.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45200/60000 (75%)]\tLoss: 1.511658\tAccuracy: 95.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45250/60000 (75%)]\tLoss: 1.461601\tAccuracy: 95.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45300/60000 (76%)]\tLoss: 1.463460\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45350/60000 (76%)]\tLoss: 1.476647\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45400/60000 (76%)]\tLoss: 1.476979\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45450/60000 (76%)]\tLoss: 1.471256\tAccuracy: 95.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45500/60000 (76%)]\tLoss: 1.463202\tAccuracy: 95.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45550/60000 (76%)]\tLoss: 1.480642\tAccuracy: 95.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45600/60000 (76%)]\tLoss: 1.499813\tAccuracy: 95.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45650/60000 (76%)]\tLoss: 1.482151\tAccuracy: 95.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45700/60000 (76%)]\tLoss: 1.473959\tAccuracy: 95.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45750/60000 (76%)]\tLoss: 1.462189\tAccuracy: 95.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45800/60000 (76%)]\tLoss: 1.461187\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45850/60000 (76%)]\tLoss: 1.502754\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45900/60000 (76%)]\tLoss: 1.501390\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45950/60000 (77%)]\tLoss: 1.461167\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46000/60000 (77%)]\tLoss: 1.482869\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46050/60000 (77%)]\tLoss: 1.490927\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46100/60000 (77%)]\tLoss: 1.461156\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46150/60000 (77%)]\tLoss: 1.480557\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46200/60000 (77%)]\tLoss: 1.501162\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46250/60000 (77%)]\tLoss: 1.486814\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46300/60000 (77%)]\tLoss: 1.461156\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46350/60000 (77%)]\tLoss: 1.486321\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46400/60000 (77%)]\tLoss: 1.517324\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46450/60000 (77%)]\tLoss: 1.477615\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46500/60000 (78%)]\tLoss: 1.481562\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46550/60000 (78%)]\tLoss: 1.518079\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46600/60000 (78%)]\tLoss: 1.537572\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46650/60000 (78%)]\tLoss: 1.541043\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46700/60000 (78%)]\tLoss: 1.487197\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46750/60000 (78%)]\tLoss: 1.469340\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46800/60000 (78%)]\tLoss: 1.469917\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46850/60000 (78%)]\tLoss: 1.490509\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46900/60000 (78%)]\tLoss: 1.489902\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46950/60000 (78%)]\tLoss: 1.514879\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47000/60000 (78%)]\tLoss: 1.481302\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47050/60000 (78%)]\tLoss: 1.481146\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47100/60000 (78%)]\tLoss: 1.473851\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47150/60000 (79%)]\tLoss: 1.465938\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47200/60000 (79%)]\tLoss: 1.492317\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47250/60000 (79%)]\tLoss: 1.481192\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47300/60000 (79%)]\tLoss: 1.475591\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47350/60000 (79%)]\tLoss: 1.522625\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47400/60000 (79%)]\tLoss: 1.480008\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47450/60000 (79%)]\tLoss: 1.502493\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47500/60000 (79%)]\tLoss: 1.461690\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47550/60000 (79%)]\tLoss: 1.501344\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47600/60000 (79%)]\tLoss: 1.504374\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47650/60000 (79%)]\tLoss: 1.486723\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47700/60000 (80%)]\tLoss: 1.494143\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47750/60000 (80%)]\tLoss: 1.482143\tAccuracy: 95.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47800/60000 (80%)]\tLoss: 1.504856\tAccuracy: 95.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47850/60000 (80%)]\tLoss: 1.461184\tAccuracy: 95.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47900/60000 (80%)]\tLoss: 1.477337\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47950/60000 (80%)]\tLoss: 1.475968\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48000/60000 (80%)]\tLoss: 1.474646\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48050/60000 (80%)]\tLoss: 1.461416\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48100/60000 (80%)]\tLoss: 1.481158\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48150/60000 (80%)]\tLoss: 1.501562\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48200/60000 (80%)]\tLoss: 1.510271\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48250/60000 (80%)]\tLoss: 1.518222\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48300/60000 (80%)]\tLoss: 1.486624\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48350/60000 (81%)]\tLoss: 1.470502\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48400/60000 (81%)]\tLoss: 1.477015\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48450/60000 (81%)]\tLoss: 1.488466\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48500/60000 (81%)]\tLoss: 1.461154\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48550/60000 (81%)]\tLoss: 1.461890\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48600/60000 (81%)]\tLoss: 1.501127\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48650/60000 (81%)]\tLoss: 1.493628\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48700/60000 (81%)]\tLoss: 1.465743\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48750/60000 (81%)]\tLoss: 1.468267\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48800/60000 (81%)]\tLoss: 1.478162\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48850/60000 (81%)]\tLoss: 1.461412\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48900/60000 (82%)]\tLoss: 1.481179\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48950/60000 (82%)]\tLoss: 1.461439\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49000/60000 (82%)]\tLoss: 1.499804\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49050/60000 (82%)]\tLoss: 1.461161\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49100/60000 (82%)]\tLoss: 1.479517\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49150/60000 (82%)]\tLoss: 1.492047\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49200/60000 (82%)]\tLoss: 1.480564\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49250/60000 (82%)]\tLoss: 1.489669\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49300/60000 (82%)]\tLoss: 1.513430\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49350/60000 (82%)]\tLoss: 1.481037\tAccuracy: 95.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49400/60000 (82%)]\tLoss: 1.473484\tAccuracy: 95.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49450/60000 (82%)]\tLoss: 1.489698\tAccuracy: 95.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49500/60000 (82%)]\tLoss: 1.484178\tAccuracy: 95.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49550/60000 (83%)]\tLoss: 1.507409\tAccuracy: 95.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49600/60000 (83%)]\tLoss: 1.501046\tAccuracy: 95.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49650/60000 (83%)]\tLoss: 1.468660\tAccuracy: 95.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49700/60000 (83%)]\tLoss: 1.461152\tAccuracy: 95.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49750/60000 (83%)]\tLoss: 1.481658\tAccuracy: 95.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49800/60000 (83%)]\tLoss: 1.501224\tAccuracy: 95.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49850/60000 (83%)]\tLoss: 1.462024\tAccuracy: 95.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49900/60000 (83%)]\tLoss: 1.519320\tAccuracy: 95.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49950/60000 (83%)]\tLoss: 1.481993\tAccuracy: 95.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50000/60000 (83%)]\tLoss: 1.481303\tAccuracy: 95.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50050/60000 (83%)]\tLoss: 1.520534\tAccuracy: 95.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50100/60000 (84%)]\tLoss: 1.486078\tAccuracy: 95.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50150/60000 (84%)]\tLoss: 1.461485\tAccuracy: 95.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50200/60000 (84%)]\tLoss: 1.462773\tAccuracy: 95.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50250/60000 (84%)]\tLoss: 1.489155\tAccuracy: 95.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50300/60000 (84%)]\tLoss: 1.462576\tAccuracy: 95.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50350/60000 (84%)]\tLoss: 1.523624\tAccuracy: 95.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50400/60000 (84%)]\tLoss: 1.481920\tAccuracy: 95.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50450/60000 (84%)]\tLoss: 1.481176\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50500/60000 (84%)]\tLoss: 1.470594\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50550/60000 (84%)]\tLoss: 1.481151\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50600/60000 (84%)]\tLoss: 1.469848\tAccuracy: 95.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50650/60000 (84%)]\tLoss: 1.552737\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50700/60000 (84%)]\tLoss: 1.537927\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50750/60000 (85%)]\tLoss: 1.542796\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50800/60000 (85%)]\tLoss: 1.504992\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50850/60000 (85%)]\tLoss: 1.473835\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50900/60000 (85%)]\tLoss: 1.464158\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50950/60000 (85%)]\tLoss: 1.539118\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51000/60000 (85%)]\tLoss: 1.513267\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51050/60000 (85%)]\tLoss: 1.461450\tAccuracy: 95.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51100/60000 (85%)]\tLoss: 1.461193\tAccuracy: 95.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51150/60000 (85%)]\tLoss: 1.469323\tAccuracy: 95.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51200/60000 (85%)]\tLoss: 1.481189\tAccuracy: 95.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51250/60000 (85%)]\tLoss: 1.482239\tAccuracy: 95.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51300/60000 (86%)]\tLoss: 1.542852\tAccuracy: 95.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51350/60000 (86%)]\tLoss: 1.461635\tAccuracy: 95.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51400/60000 (86%)]\tLoss: 1.488927\tAccuracy: 95.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51450/60000 (86%)]\tLoss: 1.472624\tAccuracy: 95.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51500/60000 (86%)]\tLoss: 1.477158\tAccuracy: 95.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51550/60000 (86%)]\tLoss: 1.481250\tAccuracy: 95.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51600/60000 (86%)]\tLoss: 1.461386\tAccuracy: 95.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51650/60000 (86%)]\tLoss: 1.486764\tAccuracy: 95.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51700/60000 (86%)]\tLoss: 1.461161\tAccuracy: 95.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51750/60000 (86%)]\tLoss: 1.481819\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51800/60000 (86%)]\tLoss: 1.480709\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51850/60000 (86%)]\tLoss: 1.510106\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51900/60000 (86%)]\tLoss: 1.541350\tAccuracy: 95.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51950/60000 (87%)]\tLoss: 1.481171\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52000/60000 (87%)]\tLoss: 1.515026\tAccuracy: 95.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52050/60000 (87%)]\tLoss: 1.506774\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52100/60000 (87%)]\tLoss: 1.505231\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52150/60000 (87%)]\tLoss: 1.481177\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52200/60000 (87%)]\tLoss: 1.481265\tAccuracy: 95.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52250/60000 (87%)]\tLoss: 1.461156\tAccuracy: 95.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52300/60000 (87%)]\tLoss: 1.461162\tAccuracy: 95.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52350/60000 (87%)]\tLoss: 1.489588\tAccuracy: 95.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52400/60000 (87%)]\tLoss: 1.473091\tAccuracy: 95.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52450/60000 (87%)]\tLoss: 1.508845\tAccuracy: 95.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52500/60000 (88%)]\tLoss: 1.478709\tAccuracy: 95.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52550/60000 (88%)]\tLoss: 1.521538\tAccuracy: 95.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52600/60000 (88%)]\tLoss: 1.461201\tAccuracy: 95.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52650/60000 (88%)]\tLoss: 1.499666\tAccuracy: 95.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52700/60000 (88%)]\tLoss: 1.500916\tAccuracy: 95.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52750/60000 (88%)]\tLoss: 1.461460\tAccuracy: 95.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52800/60000 (88%)]\tLoss: 1.471295\tAccuracy: 95.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52850/60000 (88%)]\tLoss: 1.461156\tAccuracy: 95.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52900/60000 (88%)]\tLoss: 1.501243\tAccuracy: 95.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52950/60000 (88%)]\tLoss: 1.461790\tAccuracy: 95.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53000/60000 (88%)]\tLoss: 1.504393\tAccuracy: 95.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53050/60000 (88%)]\tLoss: 1.503917\tAccuracy: 95.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53100/60000 (88%)]\tLoss: 1.478802\tAccuracy: 95.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53150/60000 (89%)]\tLoss: 1.461469\tAccuracy: 95.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53200/60000 (89%)]\tLoss: 1.481167\tAccuracy: 95.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53250/60000 (89%)]\tLoss: 1.522847\tAccuracy: 95.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53300/60000 (89%)]\tLoss: 1.485215\tAccuracy: 95.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53350/60000 (89%)]\tLoss: 1.484149\tAccuracy: 95.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53400/60000 (89%)]\tLoss: 1.461154\tAccuracy: 95.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53450/60000 (89%)]\tLoss: 1.501585\tAccuracy: 95.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53500/60000 (89%)]\tLoss: 1.472702\tAccuracy: 95.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53550/60000 (89%)]\tLoss: 1.509225\tAccuracy: 95.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53600/60000 (89%)]\tLoss: 1.476507\tAccuracy: 95.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53650/60000 (89%)]\tLoss: 1.464817\tAccuracy: 95.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53700/60000 (90%)]\tLoss: 1.495227\tAccuracy: 95.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53750/60000 (90%)]\tLoss: 1.481035\tAccuracy: 95.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53800/60000 (90%)]\tLoss: 1.470405\tAccuracy: 95.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53850/60000 (90%)]\tLoss: 1.485595\tAccuracy: 95.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53900/60000 (90%)]\tLoss: 1.531643\tAccuracy: 95.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53950/60000 (90%)]\tLoss: 1.500534\tAccuracy: 95.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54000/60000 (90%)]\tLoss: 1.480205\tAccuracy: 95.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54050/60000 (90%)]\tLoss: 1.481553\tAccuracy: 95.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54100/60000 (90%)]\tLoss: 1.478625\tAccuracy: 95.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54150/60000 (90%)]\tLoss: 1.464414\tAccuracy: 95.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54200/60000 (90%)]\tLoss: 1.481682\tAccuracy: 95.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54250/60000 (90%)]\tLoss: 1.534103\tAccuracy: 95.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54300/60000 (90%)]\tLoss: 1.463205\tAccuracy: 95.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54350/60000 (91%)]\tLoss: 1.517811\tAccuracy: 95.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54400/60000 (91%)]\tLoss: 1.461366\tAccuracy: 95.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54450/60000 (91%)]\tLoss: 1.463450\tAccuracy: 95.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54500/60000 (91%)]\tLoss: 1.461340\tAccuracy: 95.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54550/60000 (91%)]\tLoss: 1.481886\tAccuracy: 95.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54600/60000 (91%)]\tLoss: 1.487787\tAccuracy: 95.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54650/60000 (91%)]\tLoss: 1.520915\tAccuracy: 95.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54700/60000 (91%)]\tLoss: 1.461353\tAccuracy: 95.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54750/60000 (91%)]\tLoss: 1.511835\tAccuracy: 95.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54800/60000 (91%)]\tLoss: 1.464066\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54850/60000 (91%)]\tLoss: 1.481157\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54900/60000 (92%)]\tLoss: 1.481431\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54950/60000 (92%)]\tLoss: 1.461165\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55000/60000 (92%)]\tLoss: 1.528600\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55050/60000 (92%)]\tLoss: 1.528138\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55100/60000 (92%)]\tLoss: 1.499653\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55150/60000 (92%)]\tLoss: 1.549714\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55200/60000 (92%)]\tLoss: 1.544652\tAccuracy: 95.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55250/60000 (92%)]\tLoss: 1.478670\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55300/60000 (92%)]\tLoss: 1.497366\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55350/60000 (92%)]\tLoss: 1.461165\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55400/60000 (92%)]\tLoss: 1.517691\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55450/60000 (92%)]\tLoss: 1.482761\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55500/60000 (92%)]\tLoss: 1.478976\tAccuracy: 95.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55550/60000 (93%)]\tLoss: 1.481388\tAccuracy: 95.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55600/60000 (93%)]\tLoss: 1.481316\tAccuracy: 95.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55650/60000 (93%)]\tLoss: 1.461596\tAccuracy: 95.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55700/60000 (93%)]\tLoss: 1.485103\tAccuracy: 95.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55750/60000 (93%)]\tLoss: 1.482106\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55800/60000 (93%)]\tLoss: 1.481449\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55850/60000 (93%)]\tLoss: 1.541737\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55900/60000 (93%)]\tLoss: 1.506956\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55950/60000 (93%)]\tLoss: 1.482928\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56000/60000 (93%)]\tLoss: 1.578538\tAccuracy: 95.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56050/60000 (93%)]\tLoss: 1.481025\tAccuracy: 95.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56100/60000 (94%)]\tLoss: 1.502400\tAccuracy: 95.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56150/60000 (94%)]\tLoss: 1.486063\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56200/60000 (94%)]\tLoss: 1.524166\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56250/60000 (94%)]\tLoss: 1.499067\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56300/60000 (94%)]\tLoss: 1.481869\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56350/60000 (94%)]\tLoss: 1.482033\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56400/60000 (94%)]\tLoss: 1.481405\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56450/60000 (94%)]\tLoss: 1.478876\tAccuracy: 95.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56500/60000 (94%)]\tLoss: 1.470629\tAccuracy: 95.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56550/60000 (94%)]\tLoss: 1.489073\tAccuracy: 95.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56600/60000 (94%)]\tLoss: 1.520974\tAccuracy: 95.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56650/60000 (94%)]\tLoss: 1.463788\tAccuracy: 95.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56700/60000 (94%)]\tLoss: 1.466422\tAccuracy: 95.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56750/60000 (95%)]\tLoss: 1.517671\tAccuracy: 95.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56800/60000 (95%)]\tLoss: 1.482587\tAccuracy: 95.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56850/60000 (95%)]\tLoss: 1.464514\tAccuracy: 95.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56900/60000 (95%)]\tLoss: 1.478709\tAccuracy: 95.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56950/60000 (95%)]\tLoss: 1.504335\tAccuracy: 95.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57000/60000 (95%)]\tLoss: 1.501905\tAccuracy: 95.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57050/60000 (95%)]\tLoss: 1.461519\tAccuracy: 95.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57100/60000 (95%)]\tLoss: 1.463915\tAccuracy: 95.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57150/60000 (95%)]\tLoss: 1.480588\tAccuracy: 95.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57200/60000 (95%)]\tLoss: 1.462240\tAccuracy: 95.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57250/60000 (95%)]\tLoss: 1.501171\tAccuracy: 95.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57300/60000 (96%)]\tLoss: 1.463076\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57350/60000 (96%)]\tLoss: 1.485407\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57400/60000 (96%)]\tLoss: 1.492782\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57450/60000 (96%)]\tLoss: 1.486791\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57500/60000 (96%)]\tLoss: 1.501368\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57550/60000 (96%)]\tLoss: 1.549227\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57600/60000 (96%)]\tLoss: 1.506819\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57650/60000 (96%)]\tLoss: 1.555495\tAccuracy: 95.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57700/60000 (96%)]\tLoss: 1.467022\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57750/60000 (96%)]\tLoss: 1.481156\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57800/60000 (96%)]\tLoss: 1.486252\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57850/60000 (96%)]\tLoss: 1.502081\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57900/60000 (96%)]\tLoss: 1.480974\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57950/60000 (97%)]\tLoss: 1.514641\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58000/60000 (97%)]\tLoss: 1.501249\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58050/60000 (97%)]\tLoss: 1.521720\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58100/60000 (97%)]\tLoss: 1.555882\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58150/60000 (97%)]\tLoss: 1.487661\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58200/60000 (97%)]\tLoss: 1.463229\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58250/60000 (97%)]\tLoss: 1.545245\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58300/60000 (97%)]\tLoss: 1.481058\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58350/60000 (97%)]\tLoss: 1.483843\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58400/60000 (97%)]\tLoss: 1.524395\tAccuracy: 95.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58450/60000 (97%)]\tLoss: 1.484546\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58500/60000 (98%)]\tLoss: 1.485389\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58550/60000 (98%)]\tLoss: 1.463019\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58600/60000 (98%)]\tLoss: 1.483521\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58650/60000 (98%)]\tLoss: 1.481826\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58700/60000 (98%)]\tLoss: 1.520343\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58750/60000 (98%)]\tLoss: 1.526851\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58800/60000 (98%)]\tLoss: 1.498158\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58850/60000 (98%)]\tLoss: 1.519150\tAccuracy: 95.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58900/60000 (98%)]\tLoss: 1.485090\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58950/60000 (98%)]\tLoss: 1.461211\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59000/60000 (98%)]\tLoss: 1.501435\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59050/60000 (98%)]\tLoss: 1.484913\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59100/60000 (98%)]\tLoss: 1.513382\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59150/60000 (99%)]\tLoss: 1.479331\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59200/60000 (99%)]\tLoss: 1.501388\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59250/60000 (99%)]\tLoss: 1.518400\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59300/60000 (99%)]\tLoss: 1.481248\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59350/60000 (99%)]\tLoss: 1.556631\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59400/60000 (99%)]\tLoss: 1.484174\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59450/60000 (99%)]\tLoss: 1.470977\tAccuracy: 95.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59500/60000 (99%)]\tLoss: 1.486976\tAccuracy: 95.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59550/60000 (99%)]\tLoss: 1.461559\tAccuracy: 95.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59600/60000 (99%)]\tLoss: 1.481630\tAccuracy: 95.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59650/60000 (99%)]\tLoss: 1.480463\tAccuracy: 95.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59700/60000 (100%)]\tLoss: 1.491235\tAccuracy: 95.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59750/60000 (100%)]\tLoss: 1.499561\tAccuracy: 95.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59800/60000 (100%)]\tLoss: 1.527322\tAccuracy: 95.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59850/60000 (100%)]\tLoss: 1.461222\tAccuracy: 95.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59900/60000 (100%)]\tLoss: 1.466074\tAccuracy: 95.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59950/60000 (100%)]\tLoss: 1.465238\tAccuracy: 95.67%\tGradient Norm: 0.000000\n",
      "Accuracy:  95.665\n"
     ]
    }
   ],
   "source": [
    "#call the function\n",
    "source_cnn, classifier = pre_train(source_cnn, classifier,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we write a function to return accuracy, given encoder_cnn, classifier and data_loader\n",
    "def get_accuracy(encoder_cnn, classifier, data_loader):\n",
    "    #set the model to eval mode\n",
    "    encoder_cnn.eval()\n",
    "    classifier.eval()\n",
    "    #set the total and correct to zero\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    #iterate over the data\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        #send data to gpu\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #forward pass\n",
    "        output = classifier(encoder_cnn(data))\n",
    "        #get the max value from the output\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        #calculate the total number of labels\n",
    "        temp_total = target.size(0)\n",
    "        #calculate the correct predictions\n",
    "        temp_correct = (predicted == target).sum().item()\n",
    "        #add the total and correct predictions\n",
    "        total += temp_total\n",
    "        correct += temp_correct\n",
    "    #calculate the accuracy\n",
    "    total_accuracy = 100 * correct / total\n",
    "    #return the accuracy\n",
    "    #make models train mode again\n",
    "    encoder_cnn.train()\n",
    "    classifier.train()\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have the source_cnn and classifier trained\n",
    "#we will  now never train the source_cnn again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will use WGAN like training for the target_cnn and discriminator\n",
    "#the REAL DATA will come from the output of source_cnn on the source data\n",
    "#the generated data will come from the output of target_cnn on the target data\n",
    "#we will minimize the WGAN loss\n",
    "#also we use gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining gradient penalty\n",
    "def gradient_penalty(critic, source, target, device=device):\n",
    "    BATCH_SIZE, C, H, W = source.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = source * alpha + target * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will define the function to train the target_cnn and discriminator\n",
    "#it will take as arguments the target_cnn, discriminator, source_cnn, train_loader, test_loader, epochs\n",
    "#we will alos plot the loss and accuracy, per batch and per epoch\n",
    "# also we will plot the gradient penalty\n",
    "def train_adapt_target(target_cnn, discriminator, source_cnn, train_loader, test_loader, epochs=NUM_EPOCHS_GAN, device=device):\n",
    "\n",
    "    #save\n",
    "    source_nn_filename = 'saved_models/ADDA/'+experiment_id + 'temp_storage' + \"source_cnn.pt\"\n",
    "    torch.save(source_cnn.state_dict(), source_nn_filename)\n",
    "    #we will use the Adam optimizer for both the target_cnn and discriminator\n",
    "    # , but seperate\n",
    "    #we will use the same learning rate for both\n",
    "    optimizer_target_cnn = optim.Adam(target_cnn.parameters(), lr=LEARNING_RATE_GAN, betas=(0.0, 0.9) )\n",
    "    optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_GAN, betas=(0.0, 0.9) )\n",
    "\n",
    "    #make both models trainable\n",
    "    target_cnn.train()\n",
    "    discriminator.train()\n",
    "    #now make source cnn non trainable\n",
    "    #use freeze_unfreeze function\n",
    "    freeze_unfreeze_model(source_cnn, False)\n",
    "    source_cnn.eval()\n",
    "\n",
    "    #define dumy variables for keeping track of accuracy, loss and iterations through the dataset\n",
    "    step = 0\n",
    "    epoch_tracker = 0\n",
    "    batch_tracker = 0\n",
    "    #define the best loss and best epoch\n",
    "    best_loss = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    #loop through epochs\n",
    "    ep=0\n",
    "    for ep in range(epochs):\n",
    "        epoch_total_loss = 0\n",
    "        epoch_critics_loss = 0\n",
    "        epoch_target_cnn_loss = 0\n",
    "        #loop through the batches\n",
    "        #loop through batches of source data and target data combined\n",
    "        for batch_idx, (source_data, target_data) in enumerate(zip(train_loader, train_loader)):\n",
    "            #get the source and target images and we do not need labels, nbut anyway we will get them\n",
    "            source_images, source_labels = source_data\n",
    "            target_images, target_labels = target_data\n",
    "            #get batch size as min of source and target batch size\n",
    "            batch_size = min(source_images.shape[0], target_images.shape[0])\n",
    "            #make the batch size of source and target equal\n",
    "            source_images = source_images[:batch_size]\n",
    "            source_labels = source_labels[:batch_size]\n",
    "            target_images = target_images[:batch_size]\n",
    "            target_labels = target_labels[:batch_size]\n",
    "\n",
    "            #move the data to gpu\n",
    "            source_images, source_labels = source_images.to(device), source_labels.to(device)\n",
    "            target_images, target_labels = target_images.to(device), target_labels.to(device)\n",
    "\n",
    "            # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "        # equivalent to minimizing the negative of that\n",
    "            for _ in range(CRITIC_ITERATIONS):\n",
    "                #generate the features of the target images\n",
    "                target_features = target_cnn(target_images)\n",
    "                #generate the features of the source images\n",
    "                #load the source_cnn\n",
    "                source_cnn.load_state_dict(torch.load(source_nn_filename))\n",
    "                source_features = source_cnn(source_images)\n",
    "                #get gradient penalty\n",
    "                gp = gradient_penalty(discriminator, source_features, target_features)\n",
    "                critic_source = discriminator(source_features).reshape(-1)\n",
    "                critic_target = discriminator(target_features).reshape(-1)\n",
    "                critic_loss = -(torch.mean(critic_source) - torch.mean(critic_target)) + LAMBDA_GP * gp\n",
    "                #zero the gradients\n",
    "                discriminator.zero_grad()\n",
    "                #backpropagate the loss\n",
    "                critic_loss.backward(retain_graph=True)\n",
    "                #update the weights\n",
    "                optimizer_discriminator.step()\n",
    "\n",
    "            # Train target_cnn: min -E[critic(gen_fake)] <-> max E[critic(gen_fake)]\n",
    "            critic_target = discriminator(target_features).reshape(-1)\n",
    "            loss_target_cnn = -torch.mean(critic_target)\n",
    "            #zero the gradients\n",
    "            target_cnn.zero_grad()\n",
    "            #backpropagate the loss\n",
    "            loss_target_cnn.backward()\n",
    "            #update the weights\n",
    "            optimizer_target_cnn.step()\n",
    "\n",
    "            #add losses to epoch losses\n",
    "            epoch_total_loss += critic_loss.item() + loss_target_cnn.item()\n",
    "            epoch_critics_loss += critic_loss.item()\n",
    "            epoch_target_cnn_loss += loss_target_cnn.item()\n",
    "\n",
    "            #we will plot the loss on tensorboard\n",
    "            #we will plot the critic loss, target_cnn loss, gradient penalty\n",
    "            writer.add_scalar('ADDA_Loss_Critic', critic_loss, global_step=batch_tracker)\n",
    "            writer.add_scalar('ADDA_Loss_Target_CNN', loss_target_cnn, global_step=batch_tracker)\n",
    "            writer.add_scalar('ADDA_Gradient_Penalty', gp, global_step=batch_tracker)\n",
    "\n",
    "            #print losses after every 100 steps\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch [{ep}/{epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
    "                      Loss D: {critic_loss:.4f}, loss G: {loss_target_cnn:.4f}, gp: {gp:.4f}\")\n",
    "            \n",
    "                \n",
    "            #increment the batch tracker\n",
    "            batch_tracker += 1\n",
    "                \n",
    "        #print the epoch loss\n",
    "        print(f\"Epoch [{ep}/{epochs}] Loss D: {epoch_critics_loss:.4f}, loss G: {epoch_target_cnn_loss:.4f}\")\n",
    "        #add the epoch loss to tensorboard\n",
    "        writer.add_scalar('ADDA_Epoch_Total_Loss', epoch_total_loss, global_step=ep)\n",
    "        #critics loss\n",
    "        writer.add_scalar('ADDA_Epoch_Loss_Critic', epoch_critics_loss, global_step=ep)\n",
    "        #target cnn loss\n",
    "        writer.add_scalar('ADDA_Epoch_Loss_Target_CNN', epoch_target_cnn_loss, global_step=ep)\n",
    "        #every epoch we will save the model\n",
    "        #save the model with name experiment_id and epoch\n",
    "        torch.save(target_cnn.state_dict(), f\"{experiment_id}_target_cnn_{ep}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"{experiment_id}_discriminator_{ep}.pth\")\n",
    "        #test the accuracy of the model on the test set\n",
    "        test_accuracy = get_accuracy(target_cnn, classifier, test_loader)\n",
    "        #make classifier non trainable\n",
    "        freeze_unfreeze_model(classifier, False)\n",
    "        classifier.eval()\n",
    "        #make model trainable\n",
    "        target_cnn.train()\n",
    "        #print the test accuracy\n",
    "        print(f\"Epoch [{ep}/{epochs}] Test Accuracy: {test_accuracy:.4f}\")\n",
    "        #add the test accuracy to tensorboard\n",
    "        writer.add_scalar('ADDA_Test_Accuracy_Target_CNN', test_accuracy, global_step=ep)  \n",
    "        \n",
    "\n",
    "\n",
    "    #return the target cnn and discriminator\n",
    "    return target_cnn, discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now train the target cnn and discriminator\n",
    "#let us create target cnn with same weights as trained source cnn\n",
    "# target_cnn = copy.deepcopy(source_cnn)\n",
    "#but both be different and changes in one will not affect the other\n",
    "#create target cnn as same weights as source cnn\n",
    "target_cnn = copy.deepcopy(source_cnn)\n",
    "#make target cnn trainable\n",
    "freeze_unfreeze_model(target_cnn, True)\n",
    "#make source cnn non trainable\n",
    "freeze_unfreeze_model(source_cnn, False)\n",
    "#make discriminator trainable\n",
    "freeze_unfreeze_model(discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of source cnn on source data\n",
    "source_cnn_accuracy = get_accuracy(source_cnn, classifier, train_loader)\n",
    "print(f\"Source CNN Accuracy on Source Data: {source_cnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source CNN Accuracy on Target Data: 86.5176\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of source cnn on target data\n",
    "target_cnn_accuracy = get_accuracy(source_cnn, classifier, test_loader)\n",
    "#print accuracy\n",
    "print(f\"Source CNN Accuracy on Target Data: {target_cnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target CNN Accuracy on Source Data: 97.1250\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of target cnn on source data\n",
    "target_cnn_accuracy_source = get_accuracy(target_cnn, classifier, train_loader)\n",
    "#print accuracy\n",
    "print(f\"Target CNN Accuracy on Source Data: {target_cnn_accuracy_source:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target CNN Accuracy on Target Data: 86.5176\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of target cnn on target data\n",
    "target_cnn_accuracy_target = get_accuracy(target_cnn, classifier, test_loader)\n",
    "#print accuracy\n",
    "print(f\"Target CNN Accuracy on Target Data: {target_cnn_accuracy_target:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1] Batch 0/1200                       Loss D: 9.5681, loss G: 0.0308, gp: 0.9736\n",
      "Epoch [0/1] Batch 1/1200                       Loss D: 9.5314, loss G: -0.0155, gp: 0.9705\n",
      "Epoch [0/1] Batch 2/1200                       Loss D: 9.6405, loss G: -0.3049, gp: 0.9707\n",
      "Epoch [0/1] Batch 3/1200                       Loss D: 9.4853, loss G: -0.2987, gp: 0.9683\n",
      "Epoch [0/1] Batch 4/1200                       Loss D: 9.4652, loss G: -0.3833, gp: 0.9653\n",
      "Epoch [0/1] Batch 5/1200                       Loss D: 9.4533, loss G: -0.5207, gp: 0.9624\n",
      "Epoch [0/1] Batch 6/1200                       Loss D: 9.2693, loss G: -0.7964, gp: 0.9548\n",
      "Epoch [0/1] Batch 7/1200                       Loss D: 9.1132, loss G: -0.8349, gp: 0.9487\n",
      "Epoch [0/1] Batch 8/1200                       Loss D: 8.9726, loss G: -1.0169, gp: 0.9347\n",
      "Epoch [0/1] Batch 9/1200                       Loss D: 9.4778, loss G: -1.4462, gp: 0.9300\n",
      "Epoch [0/1] Batch 10/1200                       Loss D: 9.1262, loss G: -1.4719, gp: 0.9130\n",
      "Epoch [0/1] Batch 11/1200                       Loss D: 8.2092, loss G: -1.2935, gp: 0.8758\n",
      "Epoch [0/1] Batch 12/1200                       Loss D: 8.2027, loss G: -1.6647, gp: 0.8531\n",
      "Epoch [0/1] Batch 13/1200                       Loss D: 8.2429, loss G: -1.9099, gp: 0.8389\n",
      "Epoch [0/1] Batch 14/1200                       Loss D: 8.7052, loss G: -2.3434, gp: 0.8368\n",
      "Epoch [0/1] Batch 15/1200                       Loss D: 7.9909, loss G: -2.4397, gp: 0.8032\n",
      "Epoch [0/1] Batch 16/1200                       Loss D: 7.1462, loss G: -2.3380, gp: 0.7597\n",
      "Epoch [0/1] Batch 17/1200                       Loss D: 7.8634, loss G: -3.4570, gp: 0.7619\n",
      "Epoch [0/1] Batch 18/1200                       Loss D: 6.1615, loss G: -2.2153, gp: 0.7104\n",
      "Epoch [0/1] Batch 19/1200                       Loss D: 7.3909, loss G: -3.3312, gp: 0.7389\n",
      "Epoch [0/1] Batch 20/1200                       Loss D: 6.2860, loss G: -3.3100, gp: 0.6609\n",
      "Epoch [0/1] Batch 21/1200                       Loss D: 6.5374, loss G: -3.1662, gp: 0.6658\n",
      "Epoch [0/1] Batch 22/1200                       Loss D: 7.8220, loss G: -3.7022, gp: 0.7448\n",
      "Epoch [0/1] Batch 23/1200                       Loss D: 5.6714, loss G: -2.3241, gp: 0.6711\n",
      "Epoch [0/1] Batch 24/1200                       Loss D: 5.8173, loss G: -2.4359, gp: 0.7157\n",
      "Epoch [0/1] Batch 25/1200                       Loss D: 5.7615, loss G: -4.0690, gp: 0.5952\n",
      "Epoch [0/1] Batch 26/1200                       Loss D: 6.0615, loss G: -4.1824, gp: 0.6319\n",
      "Epoch [0/1] Batch 27/1200                       Loss D: 5.8021, loss G: -2.7930, gp: 0.6729\n",
      "Epoch [0/1] Batch 28/1200                       Loss D: 5.8372, loss G: -3.1283, gp: 0.5946\n",
      "Epoch [0/1] Batch 29/1200                       Loss D: 5.0343, loss G: -3.6527, gp: 0.5570\n",
      "Epoch [0/1] Batch 30/1200                       Loss D: 4.6430, loss G: -3.4881, gp: 0.5267\n",
      "Epoch [0/1] Batch 31/1200                       Loss D: 4.8181, loss G: -3.2889, gp: 0.5383\n",
      "Epoch [0/1] Batch 32/1200                       Loss D: 5.3621, loss G: -3.2015, gp: 0.6280\n",
      "Epoch [0/1] Batch 33/1200                       Loss D: 4.8434, loss G: -3.0166, gp: 0.5287\n",
      "Epoch [0/1] Batch 34/1200                       Loss D: 3.1350, loss G: -2.7307, gp: 0.4711\n",
      "Epoch [0/1] Batch 35/1200                       Loss D: 3.4449, loss G: -3.3270, gp: 0.4855\n",
      "Epoch [0/1] Batch 36/1200                       Loss D: 2.8124, loss G: -2.4836, gp: 0.4847\n",
      "Epoch [0/1] Batch 37/1200                       Loss D: 3.9761, loss G: -3.6134, gp: 0.4893\n",
      "Epoch [0/1] Batch 38/1200                       Loss D: 2.9863, loss G: -2.5736, gp: 0.4413\n",
      "Epoch [0/1] Batch 39/1200                       Loss D: 2.8251, loss G: -2.7514, gp: 0.4493\n",
      "Epoch [0/1] Batch 40/1200                       Loss D: 1.7351, loss G: -2.6052, gp: 0.4045\n",
      "Epoch [0/1] Batch 41/1200                       Loss D: 1.7882, loss G: -2.8645, gp: 0.5242\n",
      "Epoch [0/1] Batch 42/1200                       Loss D: 1.0639, loss G: -3.0091, gp: 0.3161\n",
      "Epoch [0/1] Batch 43/1200                       Loss D: 1.7326, loss G: -3.8507, gp: 0.2829\n",
      "Epoch [0/1] Batch 44/1200                       Loss D: 2.5520, loss G: -4.3053, gp: 0.3639\n",
      "Epoch [0/1] Batch 45/1200                       Loss D: 1.2271, loss G: -3.3995, gp: 0.3105\n",
      "Epoch [0/1] Batch 46/1200                       Loss D: -0.6241, loss G: -1.8051, gp: 0.2874\n",
      "Epoch [0/1] Batch 47/1200                       Loss D: 1.4141, loss G: -3.0939, gp: 0.3189\n",
      "Epoch [0/1] Batch 48/1200                       Loss D: -0.4329, loss G: -2.5627, gp: 0.2693\n",
      "Epoch [0/1] Batch 49/1200                       Loss D: -0.6061, loss G: -3.2741, gp: 0.2290\n",
      "Epoch [0/1] Batch 50/1200                       Loss D: -0.4346, loss G: -4.3233, gp: 0.2651\n",
      "Epoch [0/1] Batch 51/1200                       Loss D: 4.6656, loss G: -5.2050, gp: 0.5131\n",
      "Epoch [0/1] Batch 52/1200                       Loss D: 1.5066, loss G: -4.5977, gp: 0.2197\n",
      "Epoch [0/1] Batch 53/1200                       Loss D: 0.0649, loss G: -4.4781, gp: 0.2196\n",
      "Epoch [0/1] Batch 54/1200                       Loss D: 0.4975, loss G: -4.1349, gp: 0.1710\n",
      "Epoch [0/1] Batch 55/1200                       Loss D: 1.2452, loss G: -4.6664, gp: 0.2289\n",
      "Epoch [0/1] Batch 56/1200                       Loss D: 1.9971, loss G: -4.7606, gp: 0.2707\n",
      "Epoch [0/1] Batch 57/1200                       Loss D: -2.2877, loss G: -2.8879, gp: 0.2006\n",
      "Epoch [0/1] Batch 58/1200                       Loss D: -0.7800, loss G: -3.6661, gp: 0.1866\n",
      "Epoch [0/1] Batch 59/1200                       Loss D: -0.5220, loss G: -2.1990, gp: 0.1665\n",
      "Epoch [0/1] Batch 60/1200                       Loss D: -1.8053, loss G: -4.0493, gp: 0.3140\n",
      "Epoch [0/1] Batch 61/1200                       Loss D: -1.4776, loss G: -2.9072, gp: 0.1513\n",
      "Epoch [0/1] Batch 62/1200                       Loss D: -2.0120, loss G: -2.7356, gp: 0.1592\n",
      "Epoch [0/1] Batch 63/1200                       Loss D: -1.3881, loss G: -4.8176, gp: 0.1592\n",
      "Epoch [0/1] Batch 64/1200                       Loss D: -3.2488, loss G: -2.0976, gp: 0.2348\n",
      "Epoch [0/1] Batch 65/1200                       Loss D: -2.7728, loss G: -1.6583, gp: 0.1468\n",
      "Epoch [0/1] Batch 66/1200                       Loss D: -1.8299, loss G: -4.7705, gp: 0.1823\n",
      "Epoch [0/1] Batch 67/1200                       Loss D: -3.0214, loss G: -4.1599, gp: 0.1266\n",
      "Epoch [0/1] Batch 68/1200                       Loss D: -3.0899, loss G: -4.2306, gp: 0.0812\n",
      "Epoch [0/1] Batch 69/1200                       Loss D: -2.7220, loss G: -3.4105, gp: 0.2278\n",
      "Epoch [0/1] Batch 70/1200                       Loss D: -3.3594, loss G: -5.5818, gp: 0.1044\n",
      "Epoch [0/1] Batch 71/1200                       Loss D: -2.7877, loss G: -2.9326, gp: 0.3832\n",
      "Epoch [0/1] Batch 72/1200                       Loss D: -2.7200, loss G: -2.1181, gp: 0.1097\n",
      "Epoch [0/1] Batch 73/1200                       Loss D: -2.2646, loss G: -2.5438, gp: 0.2608\n",
      "Epoch [0/1] Batch 74/1200                       Loss D: -5.1316, loss G: -2.5575, gp: 0.0979\n",
      "Epoch [0/1] Batch 75/1200                       Loss D: -5.2643, loss G: -0.7123, gp: 0.1057\n",
      "Epoch [0/1] Batch 76/1200                       Loss D: -6.0497, loss G: -2.8763, gp: 0.0886\n",
      "Epoch [0/1] Batch 77/1200                       Loss D: -6.4920, loss G: -2.6670, gp: 0.1174\n",
      "Epoch [0/1] Batch 78/1200                       Loss D: -5.7137, loss G: -1.4685, gp: 0.2145\n",
      "Epoch [0/1] Batch 79/1200                       Loss D: -4.4665, loss G: -2.1058, gp: 0.3295\n",
      "Epoch [0/1] Batch 80/1200                       Loss D: -0.6548, loss G: -3.3322, gp: 0.2995\n",
      "Epoch [0/1] Batch 81/1200                       Loss D: -4.0821, loss G: -6.4037, gp: 0.1069\n",
      "Epoch [0/1] Batch 82/1200                       Loss D: -5.4061, loss G: -6.3515, gp: 0.0762\n",
      "Epoch [0/1] Batch 83/1200                       Loss D: -4.8730, loss G: -1.1983, gp: 0.0597\n",
      "Epoch [0/1] Batch 84/1200                       Loss D: -7.3602, loss G: -4.5124, gp: 0.0621\n",
      "Epoch [0/1] Batch 85/1200                       Loss D: -5.4949, loss G: -1.4572, gp: 0.4167\n",
      "Epoch [0/1] Batch 86/1200                       Loss D: -6.1426, loss G: -2.8692, gp: 0.0874\n",
      "Epoch [0/1] Batch 87/1200                       Loss D: -7.6772, loss G: -3.0039, gp: 0.0998\n",
      "Epoch [0/1] Batch 88/1200                       Loss D: -6.9042, loss G: -3.6567, gp: 0.1372\n",
      "Epoch [0/1] Batch 89/1200                       Loss D: -5.1111, loss G: -5.5659, gp: 0.0613\n",
      "Epoch [0/1] Batch 90/1200                       Loss D: -4.0626, loss G: -6.7447, gp: 0.0464\n",
      "Epoch [0/1] Batch 91/1200                       Loss D: -5.2441, loss G: -2.1354, gp: 0.0665\n",
      "Epoch [0/1] Batch 92/1200                       Loss D: -4.5645, loss G: -2.0512, gp: 0.0874\n",
      "Epoch [0/1] Batch 93/1200                       Loss D: -5.1147, loss G: -5.5975, gp: 0.0984\n",
      "Epoch [0/1] Batch 94/1200                       Loss D: -1.9518, loss G: -4.6634, gp: 0.0874\n",
      "Epoch [0/1] Batch 95/1200                       Loss D: -5.9063, loss G: -3.2803, gp: 0.0811\n",
      "Epoch [0/1] Batch 96/1200                       Loss D: -3.6786, loss G: -2.7830, gp: 0.0639\n",
      "Epoch [0/1] Batch 97/1200                       Loss D: -7.0222, loss G: -3.2620, gp: 0.0436\n",
      "Epoch [0/1] Batch 98/1200                       Loss D: -7.7204, loss G: -7.5779, gp: 0.1034\n",
      "Epoch [0/1] Batch 99/1200                       Loss D: -3.3593, loss G: -4.7818, gp: 0.0838\n",
      "Epoch [0/1] Batch 100/1200                       Loss D: -6.5975, loss G: -2.6747, gp: 0.0547\n",
      "Epoch [0/1] Batch 101/1200                       Loss D: -6.5587, loss G: -2.1295, gp: 0.1067\n",
      "Epoch [0/1] Batch 102/1200                       Loss D: -8.7508, loss G: -5.5183, gp: 0.0559\n",
      "Epoch [0/1] Batch 103/1200                       Loss D: -6.5682, loss G: -1.1543, gp: 0.0391\n",
      "Epoch [0/1] Batch 104/1200                       Loss D: -7.8572, loss G: -8.0235, gp: 0.0457\n",
      "Epoch [0/1] Batch 105/1200                       Loss D: -7.6348, loss G: -1.6914, gp: 0.0449\n",
      "Epoch [0/1] Batch 106/1200                       Loss D: -4.5903, loss G: -5.0852, gp: 0.0412\n",
      "Epoch [0/1] Batch 107/1200                       Loss D: -5.6240, loss G: -3.6433, gp: 0.0524\n",
      "Epoch [0/1] Batch 108/1200                       Loss D: -9.7000, loss G: -0.7778, gp: 0.0865\n",
      "Epoch [0/1] Batch 109/1200                       Loss D: -3.5848, loss G: -1.1224, gp: 0.0208\n",
      "Epoch [0/1] Batch 110/1200                       Loss D: -9.8458, loss G: -2.5942, gp: 0.0319\n",
      "Epoch [0/1] Batch 111/1200                       Loss D: -8.8138, loss G: -1.1223, gp: 0.0699\n",
      "Epoch [0/1] Batch 112/1200                       Loss D: -10.1299, loss G: -2.4906, gp: 0.0270\n",
      "Epoch [0/1] Batch 113/1200                       Loss D: -7.3902, loss G: -5.4265, gp: 0.0438\n",
      "Epoch [0/1] Batch 114/1200                       Loss D: -9.3051, loss G: -2.3284, gp: 0.0350\n",
      "Epoch [0/1] Batch 115/1200                       Loss D: -9.2406, loss G: -1.2527, gp: 0.1393\n",
      "Epoch [0/1] Batch 116/1200                       Loss D: -5.9847, loss G: -1.0849, gp: 0.2951\n",
      "Epoch [0/1] Batch 117/1200                       Loss D: -5.3462, loss G: -0.9232, gp: 0.3179\n",
      "Epoch [0/1] Batch 118/1200                       Loss D: -9.7186, loss G: -2.2895, gp: 0.1662\n",
      "Epoch [0/1] Batch 119/1200                       Loss D: -2.7061, loss G: -3.7152, gp: 0.2474\n",
      "Epoch [0/1] Batch 120/1200                       Loss D: -10.2650, loss G: -10.4815, gp: 0.0360\n",
      "Epoch [0/1] Batch 121/1200                       Loss D: -11.6171, loss G: -3.3453, gp: 0.0890\n",
      "Epoch [0/1] Batch 122/1200                       Loss D: -11.6455, loss G: -2.9599, gp: 0.0554\n",
      "Epoch [0/1] Batch 123/1200                       Loss D: -12.5348, loss G: -4.0636, gp: 0.1065\n",
      "Epoch [0/1] Batch 124/1200                       Loss D: -6.5148, loss G: -5.4773, gp: 0.6569\n",
      "Epoch [0/1] Batch 125/1200                       Loss D: 2.1478, loss G: -6.2382, gp: 0.0704\n",
      "Epoch [0/1] Batch 126/1200                       Loss D: -10.0874, loss G: -2.2450, gp: 0.0696\n",
      "Epoch [0/1] Batch 127/1200                       Loss D: -2.7020, loss G: -9.7394, gp: 0.0596\n",
      "Epoch [0/1] Batch 128/1200                       Loss D: -11.8099, loss G: -4.4514, gp: 0.0566\n",
      "Epoch [0/1] Batch 129/1200                       Loss D: -13.2650, loss G: -1.8787, gp: 0.0896\n",
      "Epoch [0/1] Batch 130/1200                       Loss D: -8.9004, loss G: -1.1146, gp: 0.1935\n",
      "Epoch [0/1] Batch 131/1200                       Loss D: -9.5058, loss G: -8.9913, gp: 0.1459\n",
      "Epoch [0/1] Batch 132/1200                       Loss D: -12.2339, loss G: -0.8793, gp: 0.0441\n",
      "Epoch [0/1] Batch 133/1200                       Loss D: -9.5559, loss G: -5.4138, gp: 0.0330\n",
      "Epoch [0/1] Batch 134/1200                       Loss D: -0.6750, loss G: -6.8040, gp: 0.1442\n",
      "Epoch [0/1] Batch 135/1200                       Loss D: -5.7495, loss G: -8.3171, gp: 0.0625\n",
      "Epoch [0/1] Batch 136/1200                       Loss D: -11.0851, loss G: -0.8413, gp: 0.0327\n",
      "Epoch [0/1] Batch 137/1200                       Loss D: -2.4542, loss G: -15.1069, gp: 0.8589\n",
      "Epoch [0/1] Batch 138/1200                       Loss D: -4.8973, loss G: -9.4548, gp: 0.0510\n",
      "Epoch [0/1] Batch 139/1200                       Loss D: -15.8694, loss G: -1.9429, gp: 0.0653\n",
      "Epoch [0/1] Batch 140/1200                       Loss D: -11.2921, loss G: -5.7656, gp: 0.0337\n",
      "Epoch [0/1] Batch 141/1200                       Loss D: -10.4249, loss G: -8.3506, gp: 0.2079\n",
      "Epoch [0/1] Batch 142/1200                       Loss D: -14.3389, loss G: -3.7402, gp: 0.0239\n",
      "Epoch [0/1] Batch 143/1200                       Loss D: -6.4634, loss G: -1.3907, gp: 0.2379\n",
      "Epoch [0/1] Batch 144/1200                       Loss D: -8.1865, loss G: -3.3035, gp: 0.0573\n",
      "Epoch [0/1] Batch 145/1200                       Loss D: -13.9748, loss G: -1.7169, gp: 0.0628\n",
      "Epoch [0/1] Batch 146/1200                       Loss D: -12.8059, loss G: -2.9939, gp: 0.1015\n",
      "Epoch [0/1] Batch 147/1200                       Loss D: -12.0916, loss G: -1.9035, gp: 0.0312\n",
      "Epoch [0/1] Batch 148/1200                       Loss D: -11.4606, loss G: -3.7543, gp: 0.0515\n",
      "Epoch [0/1] Batch 149/1200                       Loss D: -14.3559, loss G: -9.9359, gp: 0.1050\n",
      "Epoch [0/1] Batch 150/1200                       Loss D: -12.0492, loss G: -8.0816, gp: 0.0749\n",
      "Epoch [0/1] Batch 151/1200                       Loss D: -17.4519, loss G: -2.1106, gp: 0.0500\n",
      "Epoch [0/1] Batch 152/1200                       Loss D: -15.6664, loss G: -0.8486, gp: 0.0679\n",
      "Epoch [0/1] Batch 153/1200                       Loss D: -15.1059, loss G: 0.5748, gp: 0.1926\n",
      "Epoch [0/1] Batch 154/1200                       Loss D: -2.4572, loss G: -1.2625, gp: 0.2619\n",
      "Epoch [0/1] Batch 155/1200                       Loss D: -15.3202, loss G: -5.7766, gp: 0.1202\n",
      "Epoch [0/1] Batch 156/1200                       Loss D: -7.6041, loss G: -4.4916, gp: 0.0952\n",
      "Epoch [0/1] Batch 157/1200                       Loss D: -2.4904, loss G: -11.3001, gp: 0.3859\n",
      "Epoch [0/1] Batch 158/1200                       Loss D: -13.1350, loss G: -4.3706, gp: 0.0460\n",
      "Epoch [0/1] Batch 159/1200                       Loss D: -13.2532, loss G: 0.9916, gp: 0.0533\n",
      "Epoch [0/1] Batch 160/1200                       Loss D: -6.8669, loss G: -3.6980, gp: 0.1058\n",
      "Epoch [0/1] Batch 161/1200                       Loss D: -16.4328, loss G: -0.6932, gp: 0.3536\n",
      "Epoch [0/1] Batch 162/1200                       Loss D: -14.6129, loss G: -9.4677, gp: 0.3235\n",
      "Epoch [0/1] Batch 163/1200                       Loss D: -6.7135, loss G: -7.3615, gp: 0.0726\n",
      "Epoch [0/1] Batch 164/1200                       Loss D: -15.3904, loss G: -6.5750, gp: 0.0813\n",
      "Epoch [0/1] Batch 165/1200                       Loss D: -15.0110, loss G: -1.4631, gp: 0.0403\n",
      "Epoch [0/1] Batch 166/1200                       Loss D: -8.3181, loss G: -6.8777, gp: 0.1033\n",
      "Epoch [0/1] Batch 167/1200                       Loss D: -6.3414, loss G: -0.9116, gp: 0.1466\n",
      "Epoch [0/1] Batch 168/1200                       Loss D: -10.5518, loss G: -0.5206, gp: 0.0354\n",
      "Epoch [0/1] Batch 169/1200                       Loss D: -15.8621, loss G: 0.1850, gp: 0.1679\n",
      "Epoch [0/1] Batch 170/1200                       Loss D: -16.3340, loss G: -5.3593, gp: 0.1991\n",
      "Epoch [0/1] Batch 171/1200                       Loss D: -17.2481, loss G: 0.1851, gp: 0.0808\n",
      "Epoch [0/1] Batch 172/1200                       Loss D: -9.4300, loss G: 0.7944, gp: 0.2316\n",
      "Epoch [0/1] Batch 173/1200                       Loss D: -19.2310, loss G: -1.9221, gp: 0.0658\n",
      "Epoch [0/1] Batch 174/1200                       Loss D: -19.3495, loss G: -5.5777, gp: 0.0728\n",
      "Epoch [0/1] Batch 175/1200                       Loss D: -18.1668, loss G: 1.0461, gp: 0.0416\n",
      "Epoch [0/1] Batch 176/1200                       Loss D: -16.5751, loss G: 1.8398, gp: 0.4360\n",
      "Epoch [0/1] Batch 177/1200                       Loss D: -18.7898, loss G: 0.6357, gp: 0.1468\n",
      "Epoch [0/1] Batch 178/1200                       Loss D: -13.8834, loss G: 0.5129, gp: 0.0229\n",
      "Epoch [0/1] Batch 179/1200                       Loss D: -12.4480, loss G: 0.5623, gp: 0.0288\n",
      "Epoch [0/1] Batch 180/1200                       Loss D: -21.1793, loss G: -0.2602, gp: 0.0847\n",
      "Epoch [0/1] Batch 181/1200                       Loss D: -18.1339, loss G: 0.4858, gp: 0.1878\n",
      "Epoch [0/1] Batch 182/1200                       Loss D: -11.8050, loss G: -19.7946, gp: 0.2367\n",
      "Epoch [0/1] Batch 183/1200                       Loss D: -13.0375, loss G: -5.7264, gp: 0.0474\n",
      "Epoch [0/1] Batch 184/1200                       Loss D: -15.3294, loss G: 0.3222, gp: 0.0734\n",
      "Epoch [0/1] Batch 185/1200                       Loss D: -7.0595, loss G: -0.8256, gp: 0.1809\n",
      "Epoch [0/1] Batch 186/1200                       Loss D: -20.9277, loss G: -0.6496, gp: 0.0706\n",
      "Epoch [0/1] Batch 187/1200                       Loss D: -13.4378, loss G: -1.7187, gp: 0.0480\n",
      "Epoch [0/1] Batch 188/1200                       Loss D: -15.5645, loss G: -1.4366, gp: 0.0723\n",
      "Epoch [0/1] Batch 189/1200                       Loss D: -21.0563, loss G: -0.4543, gp: 0.0651\n",
      "Epoch [0/1] Batch 190/1200                       Loss D: -17.8422, loss G: 2.5743, gp: 0.0607\n",
      "Epoch [0/1] Batch 191/1200                       Loss D: -8.1161, loss G: -10.5650, gp: 0.0282\n",
      "Epoch [0/1] Batch 192/1200                       Loss D: -10.6510, loss G: -6.6855, gp: 0.3259\n",
      "Epoch [0/1] Batch 193/1200                       Loss D: -13.3731, loss G: -3.6148, gp: 0.0729\n",
      "Epoch [0/1] Batch 194/1200                       Loss D: -18.9475, loss G: -4.0039, gp: 0.0879\n",
      "Epoch [0/1] Batch 195/1200                       Loss D: -16.6493, loss G: -4.2039, gp: 0.1184\n",
      "Epoch [0/1] Batch 196/1200                       Loss D: -20.8161, loss G: -0.2383, gp: 0.0585\n",
      "Epoch [0/1] Batch 197/1200                       Loss D: -23.0153, loss G: -6.9984, gp: 0.1350\n",
      "Epoch [0/1] Batch 198/1200                       Loss D: -21.7284, loss G: 1.6014, gp: 0.2548\n",
      "Epoch [0/1] Batch 199/1200                       Loss D: -14.1128, loss G: -4.1044, gp: 0.0653\n",
      "Epoch [0/1] Batch 200/1200                       Loss D: 2.2892, loss G: -17.6804, gp: 0.0504\n",
      "Epoch [0/1] Batch 201/1200                       Loss D: -13.0451, loss G: -9.9419, gp: 0.0417\n",
      "Epoch [0/1] Batch 202/1200                       Loss D: -18.7309, loss G: -7.6053, gp: 0.0299\n",
      "Epoch [0/1] Batch 203/1200                       Loss D: -15.0652, loss G: -4.6784, gp: 0.0333\n",
      "Epoch [0/1] Batch 204/1200                       Loss D: -22.5562, loss G: -7.0582, gp: 0.1555\n",
      "Epoch [0/1] Batch 205/1200                       Loss D: -20.1919, loss G: -5.5888, gp: 0.0436\n",
      "Epoch [0/1] Batch 206/1200                       Loss D: -17.9442, loss G: -1.9444, gp: 0.2387\n",
      "Epoch [0/1] Batch 207/1200                       Loss D: -9.9755, loss G: 1.1281, gp: 0.1877\n",
      "Epoch [0/1] Batch 208/1200                       Loss D: -16.0907, loss G: 0.5597, gp: 0.2669\n",
      "Epoch [0/1] Batch 209/1200                       Loss D: -8.9347, loss G: -17.2939, gp: 1.2712\n",
      "Epoch [0/1] Batch 210/1200                       Loss D: -18.0403, loss G: -5.5165, gp: 0.0452\n",
      "Epoch [0/1] Batch 211/1200                       Loss D: -20.3353, loss G: -14.0285, gp: 0.1297\n",
      "Epoch [0/1] Batch 212/1200                       Loss D: -17.6872, loss G: 0.0984, gp: 0.0199\n",
      "Epoch [0/1] Batch 213/1200                       Loss D: -18.4125, loss G: 1.7292, gp: 0.1070\n",
      "Epoch [0/1] Batch 214/1200                       Loss D: -20.5496, loss G: -23.3810, gp: 0.0832\n",
      "Epoch [0/1] Batch 215/1200                       Loss D: -17.0711, loss G: 1.5025, gp: 0.0373\n",
      "Epoch [0/1] Batch 216/1200                       Loss D: -6.3734, loss G: -5.4513, gp: 0.0878\n",
      "Epoch [0/1] Batch 217/1200                       Loss D: -17.5444, loss G: -6.3507, gp: 0.1704\n",
      "Epoch [0/1] Batch 218/1200                       Loss D: -24.8027, loss G: 1.3032, gp: 0.0420\n",
      "Epoch [0/1] Batch 219/1200                       Loss D: -20.1201, loss G: -5.2096, gp: 0.4095\n",
      "Epoch [0/1] Batch 220/1200                       Loss D: -26.5532, loss G: 1.1673, gp: 0.0397\n",
      "Epoch [0/1] Batch 221/1200                       Loss D: -15.5377, loss G: -9.6258, gp: 0.6993\n",
      "Epoch [0/1] Batch 222/1200                       Loss D: -23.2367, loss G: -9.7196, gp: 0.1347\n",
      "Epoch [0/1] Batch 223/1200                       Loss D: -26.9580, loss G: -1.6435, gp: 0.1320\n",
      "Epoch [0/1] Batch 224/1200                       Loss D: -10.0747, loss G: 0.3765, gp: 0.0548\n",
      "Epoch [0/1] Batch 225/1200                       Loss D: -28.1171, loss G: 0.5702, gp: 0.2364\n",
      "Epoch [0/1] Batch 226/1200                       Loss D: -19.4565, loss G: 0.3278, gp: 0.0351\n",
      "Epoch [0/1] Batch 227/1200                       Loss D: -28.8131, loss G: -0.5697, gp: 0.0670\n",
      "Epoch [0/1] Batch 228/1200                       Loss D: 5.5310, loss G: -4.1157, gp: 0.1612\n",
      "Epoch [0/1] Batch 229/1200                       Loss D: -9.1668, loss G: -2.9848, gp: 0.0930\n",
      "Epoch [0/1] Batch 230/1200                       Loss D: -19.7772, loss G: 1.1120, gp: 0.0422\n",
      "Epoch [0/1] Batch 231/1200                       Loss D: -31.4359, loss G: -0.2329, gp: 0.0596\n",
      "Epoch [0/1] Batch 232/1200                       Loss D: -26.1260, loss G: 2.8953, gp: 0.0984\n",
      "Epoch [0/1] Batch 233/1200                       Loss D: -28.9902, loss G: -2.4182, gp: 0.0921\n",
      "Epoch [0/1] Batch 234/1200                       Loss D: -28.6052, loss G: -0.9314, gp: 0.0508\n",
      "Epoch [0/1] Batch 235/1200                       Loss D: -21.8870, loss G: 0.6039, gp: 0.4290\n",
      "Epoch [0/1] Batch 236/1200                       Loss D: -11.5771, loss G: -0.7928, gp: 0.1085\n",
      "Epoch [0/1] Batch 237/1200                       Loss D: -29.9434, loss G: 0.3392, gp: 0.1138\n",
      "Epoch [0/1] Batch 238/1200                       Loss D: -32.1197, loss G: -0.8791, gp: 0.1675\n",
      "Epoch [0/1] Batch 239/1200                       Loss D: -27.8362, loss G: -4.1113, gp: 0.0995\n",
      "Epoch [0/1] Batch 240/1200                       Loss D: -18.7647, loss G: -1.1587, gp: 0.0829\n",
      "Epoch [0/1] Batch 241/1200                       Loss D: -29.7283, loss G: -1.9029, gp: 0.0642\n",
      "Epoch [0/1] Batch 242/1200                       Loss D: -28.0820, loss G: -2.3329, gp: 0.0574\n",
      "Epoch [0/1] Batch 243/1200                       Loss D: -22.5239, loss G: -6.7291, gp: 0.1555\n",
      "Epoch [0/1] Batch 244/1200                       Loss D: -29.8958, loss G: 0.1436, gp: 0.0697\n",
      "Epoch [0/1] Batch 245/1200                       Loss D: -25.9461, loss G: 2.5480, gp: 0.0842\n",
      "Epoch [0/1] Batch 246/1200                       Loss D: -33.7483, loss G: -1.0363, gp: 0.0487\n",
      "Epoch [0/1] Batch 247/1200                       Loss D: -29.7933, loss G: -2.7006, gp: 0.6438\n",
      "Epoch [0/1] Batch 248/1200                       Loss D: -19.6428, loss G: -18.6643, gp: 0.1239\n",
      "Epoch [0/1] Batch 249/1200                       Loss D: -24.3002, loss G: -1.0606, gp: 0.4600\n",
      "Epoch [0/1] Batch 250/1200                       Loss D: -37.8214, loss G: 0.8228, gp: 0.0532\n",
      "Epoch [0/1] Batch 251/1200                       Loss D: -8.5451, loss G: 0.6838, gp: 0.1384\n",
      "Epoch [0/1] Batch 252/1200                       Loss D: -16.5631, loss G: 1.5116, gp: 0.1092\n",
      "Epoch [0/1] Batch 253/1200                       Loss D: 0.7108, loss G: 2.4837, gp: 0.1186\n",
      "Epoch [0/1] Batch 254/1200                       Loss D: 0.4887, loss G: 4.1360, gp: 0.1745\n",
      "Epoch [0/1] Batch 255/1200                       Loss D: 1.7471, loss G: 1.9289, gp: 0.4388\n",
      "Epoch [0/1] Batch 256/1200                       Loss D: 0.9476, loss G: 4.5353, gp: 0.2564\n",
      "Epoch [0/1] Batch 257/1200                       Loss D: -1.7154, loss G: 5.6234, gp: 0.1194\n",
      "Epoch [0/1] Batch 258/1200                       Loss D: 4.6890, loss G: 2.2889, gp: 0.2338\n",
      "Epoch [0/1] Batch 259/1200                       Loss D: -0.1080, loss G: 4.3475, gp: 0.2326\n",
      "Epoch [0/1] Batch 260/1200                       Loss D: -3.7050, loss G: 5.9016, gp: 0.0937\n",
      "Epoch [0/1] Batch 261/1200                       Loss D: 0.1861, loss G: 4.2666, gp: 0.4467\n",
      "Epoch [0/1] Batch 262/1200                       Loss D: -3.4800, loss G: 5.5935, gp: 0.0769\n",
      "Epoch [0/1] Batch 263/1200                       Loss D: -2.8465, loss G: 6.4058, gp: 0.1541\n",
      "Epoch [0/1] Batch 264/1200                       Loss D: -3.5971, loss G: 6.5035, gp: 0.1125\n",
      "Epoch [0/1] Batch 265/1200                       Loss D: -5.8176, loss G: 8.3891, gp: 0.0863\n",
      "Epoch [0/1] Batch 266/1200                       Loss D: -5.9505, loss G: 4.5235, gp: 0.1229\n",
      "Epoch [0/1] Batch 267/1200                       Loss D: -23.3386, loss G: -2.9428, gp: 0.1972\n",
      "Epoch [0/1] Batch 268/1200                       Loss D: -33.5410, loss G: 1.1819, gp: 0.2480\n",
      "Epoch [0/1] Batch 269/1200                       Loss D: -33.5109, loss G: 0.8528, gp: 0.0310\n",
      "Epoch [0/1] Batch 270/1200                       Loss D: -27.1204, loss G: -1.2709, gp: 0.0642\n",
      "Epoch [0/1] Batch 271/1200                       Loss D: -28.0830, loss G: 1.6611, gp: 0.2021\n",
      "Epoch [0/1] Batch 272/1200                       Loss D: -39.5584, loss G: 1.0021, gp: 0.0466\n",
      "Epoch [0/1] Batch 273/1200                       Loss D: -27.4263, loss G: 6.1114, gp: 0.0784\n",
      "Epoch [0/1] Batch 274/1200                       Loss D: -17.7698, loss G: -16.8389, gp: 0.1215\n",
      "Epoch [0/1] Batch 275/1200                       Loss D: -22.3113, loss G: -4.3060, gp: 0.0856\n",
      "Epoch [0/1] Batch 276/1200                       Loss D: -46.0252, loss G: 6.3241, gp: 0.0962\n",
      "Epoch [0/1] Batch 277/1200                       Loss D: -37.6230, loss G: -15.9423, gp: 0.1062\n",
      "Epoch [0/1] Batch 278/1200                       Loss D: -42.3715, loss G: 0.0503, gp: 0.0529\n",
      "Epoch [0/1] Batch 279/1200                       Loss D: -39.1813, loss G: 2.4982, gp: 0.1822\n",
      "Epoch [0/1] Batch 280/1200                       Loss D: -34.4364, loss G: 4.4732, gp: 0.1884\n",
      "Epoch [0/1] Batch 281/1200                       Loss D: -39.3840, loss G: -2.4008, gp: 0.0933\n",
      "Epoch [0/1] Batch 282/1200                       Loss D: -41.7910, loss G: 5.2421, gp: 0.0564\n",
      "Epoch [0/1] Batch 283/1200                       Loss D: -42.6804, loss G: 1.7169, gp: 0.2759\n",
      "Epoch [0/1] Batch 284/1200                       Loss D: -43.5798, loss G: 2.5787, gp: 0.2423\n",
      "Epoch [0/1] Batch 285/1200                       Loss D: -22.6195, loss G: -6.0992, gp: 0.0948\n",
      "Epoch [0/1] Batch 286/1200                       Loss D: -27.7661, loss G: -20.4831, gp: 0.0733\n",
      "Epoch [0/1] Batch 287/1200                       Loss D: -40.4043, loss G: -1.3357, gp: 0.1085\n",
      "Epoch [0/1] Batch 288/1200                       Loss D: -43.0215, loss G: 0.7338, gp: 0.1545\n",
      "Epoch [0/1] Batch 289/1200                       Loss D: -35.7575, loss G: 1.7884, gp: 0.9131\n",
      "Epoch [0/1] Batch 290/1200                       Loss D: -39.4787, loss G: 3.6920, gp: 0.0766\n",
      "Epoch [0/1] Batch 291/1200                       Loss D: -41.9564, loss G: 0.8751, gp: 0.1992\n",
      "Epoch [0/1] Batch 292/1200                       Loss D: -38.9289, loss G: -0.2837, gp: 0.3410\n",
      "Epoch [0/1] Batch 293/1200                       Loss D: -39.8319, loss G: -4.8964, gp: 0.2061\n",
      "Epoch [0/1] Batch 294/1200                       Loss D: -36.4795, loss G: -0.4469, gp: 0.8358\n",
      "Epoch [0/1] Batch 295/1200                       Loss D: -23.5802, loss G: -9.6047, gp: 0.4709\n",
      "Epoch [0/1] Batch 296/1200                       Loss D: -40.1673, loss G: -3.0489, gp: 0.1275\n",
      "Epoch [0/1] Batch 297/1200                       Loss D: -13.0264, loss G: -5.4624, gp: 1.4590\n",
      "Epoch [0/1] Batch 298/1200                       Loss D: -32.7012, loss G: 0.0837, gp: 0.1632\n",
      "Epoch [0/1] Batch 299/1200                       Loss D: -32.1134, loss G: 1.1682, gp: 0.2486\n",
      "Epoch [0/1] Batch 300/1200                       Loss D: -19.7715, loss G: -11.1353, gp: 0.0877\n",
      "Epoch [0/1] Batch 301/1200                       Loss D: -39.8101, loss G: -1.2500, gp: 0.1882\n",
      "Epoch [0/1] Batch 302/1200                       Loss D: -12.0335, loss G: 2.1665, gp: 0.1331\n",
      "Epoch [0/1] Batch 303/1200                       Loss D: -29.8291, loss G: -11.4417, gp: 0.1016\n",
      "Epoch [0/1] Batch 304/1200                       Loss D: -29.6390, loss G: -13.3825, gp: 0.0984\n",
      "Epoch [0/1] Batch 305/1200                       Loss D: -32.4919, loss G: 3.5876, gp: 0.0780\n",
      "Epoch [0/1] Batch 306/1200                       Loss D: -7.0776, loss G: -9.3379, gp: 0.1399\n",
      "Epoch [0/1] Batch 307/1200                       Loss D: -16.5106, loss G: -6.0680, gp: 0.1236\n",
      "Epoch [0/1] Batch 308/1200                       Loss D: -12.7860, loss G: 5.3348, gp: 0.0834\n",
      "Epoch [0/1] Batch 309/1200                       Loss D: -33.0260, loss G: 0.9822, gp: 0.1034\n",
      "Epoch [0/1] Batch 310/1200                       Loss D: -31.8220, loss G: 4.8936, gp: 0.1314\n",
      "Epoch [0/1] Batch 311/1200                       Loss D: -47.4091, loss G: 1.2303, gp: 0.2569\n",
      "Epoch [0/1] Batch 312/1200                       Loss D: -45.7100, loss G: -0.9275, gp: 0.1075\n",
      "Epoch [0/1] Batch 313/1200                       Loss D: -52.3580, loss G: 4.4701, gp: 0.1642\n",
      "Epoch [0/1] Batch 314/1200                       Loss D: -55.7916, loss G: 5.8382, gp: 0.1421\n",
      "Epoch [0/1] Batch 315/1200                       Loss D: -42.8214, loss G: -1.5623, gp: 0.4973\n",
      "Epoch [0/1] Batch 316/1200                       Loss D: -45.2846, loss G: -3.3635, gp: 0.1777\n",
      "Epoch [0/1] Batch 317/1200                       Loss D: -52.0799, loss G: 2.3064, gp: 0.1248\n",
      "Epoch [0/1] Batch 318/1200                       Loss D: -46.3939, loss G: -2.8362, gp: 0.1718\n",
      "Epoch [0/1] Batch 319/1200                       Loss D: -50.8073, loss G: -0.6209, gp: 0.0707\n",
      "Epoch [0/1] Batch 320/1200                       Loss D: -53.9081, loss G: 4.7171, gp: 0.1488\n",
      "Epoch [0/1] Batch 321/1200                       Loss D: -36.4239, loss G: 0.6563, gp: 0.8983\n",
      "Epoch [0/1] Batch 322/1200                       Loss D: -50.2944, loss G: 2.6934, gp: 0.1626\n",
      "Epoch [0/1] Batch 323/1200                       Loss D: -51.8849, loss G: 2.6306, gp: 0.2170\n",
      "Epoch [0/1] Batch 324/1200                       Loss D: -48.6164, loss G: 1.4462, gp: 0.4240\n",
      "Epoch [0/1] Batch 325/1200                       Loss D: -50.4280, loss G: -0.8194, gp: 0.1268\n",
      "Epoch [0/1] Batch 326/1200                       Loss D: -45.9304, loss G: 1.4694, gp: 0.1123\n",
      "Epoch [0/1] Batch 327/1200                       Loss D: -43.0825, loss G: 2.6854, gp: 0.1049\n",
      "Epoch [0/1] Batch 328/1200                       Loss D: -39.4295, loss G: 4.6438, gp: 0.1130\n",
      "Epoch [0/1] Batch 329/1200                       Loss D: -58.7061, loss G: 4.8696, gp: 0.2085\n",
      "Epoch [0/1] Batch 330/1200                       Loss D: -56.4497, loss G: 3.5459, gp: 0.2186\n",
      "Epoch [0/1] Batch 331/1200                       Loss D: -59.2524, loss G: 5.0019, gp: 0.2197\n",
      "Epoch [0/1] Batch 332/1200                       Loss D: -60.6519, loss G: 6.4929, gp: 0.1234\n",
      "Epoch [0/1] Batch 333/1200                       Loss D: -37.6557, loss G: -21.8964, gp: 0.1196\n",
      "Epoch [0/1] Batch 334/1200                       Loss D: -20.4236, loss G: 5.8684, gp: 0.0993\n",
      "Epoch [0/1] Batch 335/1200                       Loss D: -61.1723, loss G: 5.4016, gp: 0.1878\n",
      "Epoch [0/1] Batch 336/1200                       Loss D: -64.4442, loss G: 7.3151, gp: 0.1923\n",
      "Epoch [0/1] Batch 337/1200                       Loss D: -65.7203, loss G: 6.9498, gp: 0.1379\n",
      "Epoch [0/1] Batch 338/1200                       Loss D: -65.9997, loss G: 6.6772, gp: 0.1267\n",
      "Epoch [0/1] Batch 339/1200                       Loss D: 2.1458, loss G: 6.1289, gp: 0.1336\n",
      "Epoch [0/1] Batch 340/1200                       Loss D: 1.1816, loss G: 5.8422, gp: 0.1794\n",
      "Epoch [0/1] Batch 341/1200                       Loss D: -61.5261, loss G: 5.6809, gp: 0.1199\n",
      "Epoch [0/1] Batch 342/1200                       Loss D: -0.4771, loss G: 5.6511, gp: 0.0955\n",
      "Epoch [0/1] Batch 343/1200                       Loss D: -55.6924, loss G: 5.2842, gp: 0.1396\n",
      "Epoch [0/1] Batch 344/1200                       Loss D: -49.3704, loss G: -2.8804, gp: 0.0985\n",
      "Epoch [0/1] Batch 345/1200                       Loss D: -54.5736, loss G: -2.0141, gp: 0.1899\n",
      "Epoch [0/1] Batch 346/1200                       Loss D: -60.8540, loss G: 1.7719, gp: 0.1043\n",
      "Epoch [0/1] Batch 347/1200                       Loss D: -59.5536, loss G: -0.3092, gp: 0.3419\n",
      "Epoch [0/1] Batch 348/1200                       Loss D: -53.0934, loss G: -1.8757, gp: 0.2525\n",
      "Epoch [0/1] Batch 349/1200                       Loss D: -58.8236, loss G: -2.8743, gp: 0.1434\n",
      "Epoch [0/1] Batch 350/1200                       Loss D: -61.4180, loss G: 0.5663, gp: 0.1007\n",
      "Epoch [0/1] Batch 351/1200                       Loss D: -55.3791, loss G: -3.5940, gp: 0.2460\n",
      "Epoch [0/1] Batch 352/1200                       Loss D: -60.8312, loss G: -1.9258, gp: 0.1127\n",
      "Epoch [0/1] Batch 353/1200                       Loss D: -63.3934, loss G: 0.5672, gp: 0.2039\n",
      "Epoch [0/1] Batch 354/1200                       Loss D: -63.8518, loss G: -1.1677, gp: 0.0783\n",
      "Epoch [0/1] Batch 355/1200                       Loss D: -54.9084, loss G: -3.0629, gp: 0.6381\n",
      "Epoch [0/1] Batch 356/1200                       Loss D: -60.2022, loss G: -1.2411, gp: 0.2130\n",
      "Epoch [0/1] Batch 357/1200                       Loss D: -61.0961, loss G: -3.0414, gp: 0.3117\n",
      "Epoch [0/1] Batch 358/1200                       Loss D: -52.8641, loss G: -2.0259, gp: 0.0621\n",
      "Epoch [0/1] Batch 359/1200                       Loss D: -61.3737, loss G: -3.1943, gp: 0.0923\n",
      "Epoch [0/1] Batch 360/1200                       Loss D: -63.3543, loss G: -3.2719, gp: 0.0766\n",
      "Epoch [0/1] Batch 361/1200                       Loss D: -62.6066, loss G: -4.7420, gp: 0.0652\n",
      "Epoch [0/1] Batch 362/1200                       Loss D: -66.5544, loss G: 1.7542, gp: 0.2531\n",
      "Epoch [0/1] Batch 363/1200                       Loss D: -63.1553, loss G: -3.3878, gp: 0.2308\n",
      "Epoch [0/1] Batch 364/1200                       Loss D: -63.4483, loss G: -0.9790, gp: 0.2876\n",
      "Epoch [0/1] Batch 365/1200                       Loss D: -65.8483, loss G: -1.9796, gp: 0.1941\n",
      "Epoch [0/1] Batch 366/1200                       Loss D: -64.6581, loss G: -4.6040, gp: 0.0837\n",
      "Epoch [0/1] Batch 367/1200                       Loss D: -65.6208, loss G: -2.2880, gp: 0.1833\n",
      "Epoch [0/1] Batch 368/1200                       Loss D: -56.8188, loss G: -2.0107, gp: 0.2114\n",
      "Epoch [0/1] Batch 369/1200                       Loss D: -2.3937, loss G: -57.6629, gp: 0.0920\n",
      "Epoch [0/1] Batch 370/1200                       Loss D: -65.9999, loss G: -4.1244, gp: 0.0477\n",
      "Epoch [0/1] Batch 371/1200                       Loss D: -67.9071, loss G: -3.0706, gp: 0.0794\n",
      "Epoch [0/1] Batch 372/1200                       Loss D: -66.9680, loss G: -3.8249, gp: 0.0542\n",
      "Epoch [0/1] Batch 373/1200                       Loss D: -69.6715, loss G: -2.9826, gp: 0.0715\n",
      "Epoch [0/1] Batch 374/1200                       Loss D: -73.1348, loss G: 2.9554, gp: 0.2912\n",
      "Epoch [0/1] Batch 375/1200                       Loss D: -74.1251, loss G: 3.3158, gp: 0.2013\n",
      "Epoch [0/1] Batch 376/1200                       Loss D: -65.9011, loss G: 4.4597, gp: 1.1727\n",
      "Epoch [0/1] Batch 377/1200                       Loss D: -68.7641, loss G: 0.5273, gp: 0.5980\n",
      "Epoch [0/1] Batch 378/1200                       Loss D: 0.2509, loss G: 1.2944, gp: 0.2944\n",
      "Epoch [0/1] Batch 379/1200                       Loss D: -78.1322, loss G: 3.8162, gp: 0.2394\n",
      "Epoch [0/1] Batch 380/1200                       Loss D: -72.7992, loss G: -2.8721, gp: 0.2292\n",
      "Epoch [0/1] Batch 381/1200                       Loss D: -67.1272, loss G: -3.4257, gp: 0.3592\n",
      "Epoch [0/1] Batch 382/1200                       Loss D: -75.1125, loss G: -0.7405, gp: 0.1754\n",
      "Epoch [0/1] Batch 383/1200                       Loss D: -71.2194, loss G: -0.3955, gp: 0.2819\n",
      "Epoch [0/1] Batch 384/1200                       Loss D: -73.9442, loss G: -1.6547, gp: 0.0642\n",
      "Epoch [0/1] Batch 385/1200                       Loss D: -69.3748, loss G: -4.2541, gp: 0.0890\n",
      "Epoch [0/1] Batch 386/1200                       Loss D: -77.7162, loss G: 3.4215, gp: 0.1147\n",
      "Epoch [0/1] Batch 387/1200                       Loss D: -66.5502, loss G: -5.9500, gp: 0.1090\n",
      "Epoch [0/1] Batch 388/1200                       Loss D: -38.2417, loss G: -17.6182, gp: 0.0967\n",
      "Epoch [0/1] Batch 389/1200                       Loss D: -75.6670, loss G: -2.0613, gp: 0.0703\n",
      "Epoch [0/1] Batch 390/1200                       Loss D: -73.0080, loss G: -4.3255, gp: 0.0770\n",
      "Epoch [0/1] Batch 391/1200                       Loss D: -76.0830, loss G: -2.8343, gp: 0.1048\n",
      "Epoch [0/1] Batch 392/1200                       Loss D: -72.4244, loss G: -5.5081, gp: 0.0544\n",
      "Epoch [0/1] Batch 393/1200                       Loss D: -78.9693, loss G: -2.2074, gp: 0.0765\n",
      "Epoch [0/1] Batch 394/1200                       Loss D: -78.2424, loss G: -1.5269, gp: 0.1927\n",
      "Epoch [0/1] Batch 395/1200                       Loss D: -74.5608, loss G: -7.6724, gp: 0.0617\n",
      "Epoch [0/1] Batch 396/1200                       Loss D: -77.7452, loss G: -3.3452, gp: 0.0997\n",
      "Epoch [0/1] Batch 397/1200                       Loss D: -79.1803, loss G: -2.7965, gp: 0.0413\n",
      "Epoch [0/1] Batch 398/1200                       Loss D: -80.5638, loss G: -2.0161, gp: 0.1845\n",
      "Epoch [0/1] Batch 399/1200                       Loss D: 1.3334, loss G: 3.4806, gp: 0.1084\n",
      "Epoch [0/1] Batch 400/1200                       Loss D: -3.2085, loss G: 3.4805, gp: 0.0996\n",
      "Epoch [0/1] Batch 401/1200                       Loss D: -72.1878, loss G: -4.4514, gp: 0.3467\n",
      "Epoch [0/1] Batch 402/1200                       Loss D: -77.9139, loss G: -2.7484, gp: 0.2518\n",
      "Epoch [0/1] Batch 403/1200                       Loss D: -83.5166, loss G: -0.7716, gp: 0.0888\n",
      "Epoch [0/1] Batch 404/1200                       Loss D: -82.7629, loss G: -3.7142, gp: 0.0462\n",
      "Epoch [0/1] Batch 405/1200                       Loss D: -81.8943, loss G: -4.3239, gp: 0.0936\n",
      "Epoch [0/1] Batch 406/1200                       Loss D: -85.2310, loss G: 2.4835, gp: 0.0718\n",
      "Epoch [0/1] Batch 407/1200                       Loss D: -4.5631, loss G: 1.1491, gp: 0.0667\n",
      "Epoch [0/1] Batch 408/1200                       Loss D: -85.7651, loss G: -0.3137, gp: 0.0773\n",
      "Epoch [0/1] Batch 409/1200                       Loss D: -81.5853, loss G: -1.6135, gp: 0.2839\n",
      "Epoch [0/1] Batch 410/1200                       Loss D: -78.8389, loss G: -3.8133, gp: 0.2324\n",
      "Epoch [0/1] Batch 411/1200                       Loss D: -82.1270, loss G: -4.8362, gp: 0.0721\n",
      "Epoch [0/1] Batch 412/1200                       Loss D: -87.7573, loss G: 0.0371, gp: 0.0661\n",
      "Epoch [0/1] Batch 413/1200                       Loss D: -1.1379, loss G: 2.8516, gp: 0.1067\n",
      "Epoch [0/1] Batch 414/1200                       Loss D: -88.2082, loss G: 1.7293, gp: 0.0963\n",
      "Epoch [0/1] Batch 415/1200                       Loss D: -0.8507, loss G: -0.5974, gp: 0.2190\n",
      "Epoch [0/1] Batch 416/1200                       Loss D: -88.8752, loss G: -0.6126, gp: 0.0763\n",
      "Epoch [0/1] Batch 417/1200                       Loss D: -70.3438, loss G: 0.2998, gp: 0.0714\n",
      "Epoch [0/1] Batch 418/1200                       Loss D: -86.2435, loss G: -2.0110, gp: 0.1077\n",
      "Epoch [0/1] Batch 419/1200                       Loss D: -86.6591, loss G: 0.7613, gp: 0.1444\n",
      "Epoch [0/1] Batch 420/1200                       Loss D: -90.4689, loss G: 0.3943, gp: 0.0999\n",
      "Epoch [0/1] Batch 421/1200                       Loss D: -6.8565, loss G: -1.0266, gp: 0.1411\n",
      "Epoch [0/1] Batch 422/1200                       Loss D: -89.1064, loss G: -2.0239, gp: 0.1394\n",
      "Epoch [0/1] Batch 423/1200                       Loss D: -88.7551, loss G: -3.5712, gp: 0.0991\n",
      "Epoch [0/1] Batch 424/1200                       Loss D: -85.2214, loss G: -4.4731, gp: 0.2215\n",
      "Epoch [0/1] Batch 425/1200                       Loss D: -89.6720, loss G: -2.9543, gp: 0.2825\n",
      "Epoch [0/1] Batch 426/1200                       Loss D: 2.5956, loss G: -2.0234, gp: 0.5597\n",
      "Epoch [0/1] Batch 427/1200                       Loss D: -5.7107, loss G: -6.9745, gp: 0.3612\n",
      "Epoch [0/1] Batch 428/1200                       Loss D: -87.5014, loss G: -3.5927, gp: 0.1562\n",
      "Epoch [0/1] Batch 429/1200                       Loss D: -89.4337, loss G: -1.4514, gp: 0.1862\n",
      "Epoch [0/1] Batch 430/1200                       Loss D: -91.5318, loss G: -1.0036, gp: 0.1117\n",
      "Epoch [0/1] Batch 431/1200                       Loss D: -92.8867, loss G: -1.5443, gp: 0.2486\n",
      "Epoch [0/1] Batch 432/1200                       Loss D: -84.4601, loss G: -4.4545, gp: 0.1967\n",
      "Epoch [0/1] Batch 433/1200                       Loss D: -84.9865, loss G: -2.2416, gp: 0.5543\n",
      "Epoch [0/1] Batch 434/1200                       Loss D: -68.7549, loss G: -4.7445, gp: 0.4808\n",
      "Epoch [0/1] Batch 435/1200                       Loss D: -90.3792, loss G: -4.0360, gp: 0.0654\n",
      "Epoch [0/1] Batch 436/1200                       Loss D: -91.7142, loss G: -3.0772, gp: 0.0989\n",
      "Epoch [0/1] Batch 437/1200                       Loss D: -36.1449, loss G: -3.8046, gp: 0.1344\n",
      "Epoch [0/1] Batch 438/1200                       Loss D: -82.6688, loss G: -4.1591, gp: 0.2582\n",
      "Epoch [0/1] Batch 439/1200                       Loss D: -91.5056, loss G: -4.1153, gp: 0.0827\n",
      "Epoch [0/1] Batch 440/1200                       Loss D: -94.6513, loss G: -0.2668, gp: 0.1249\n",
      "Epoch [0/1] Batch 441/1200                       Loss D: -94.3349, loss G: -1.6609, gp: 0.0860\n",
      "Epoch [0/1] Batch 442/1200                       Loss D: -96.0691, loss G: -0.1755, gp: 0.1181\n",
      "Epoch [0/1] Batch 443/1200                       Loss D: -91.8289, loss G: -0.9500, gp: 0.1303\n",
      "Epoch [0/1] Batch 444/1200                       Loss D: -84.8177, loss G: -5.6718, gp: 0.1219\n",
      "Epoch [0/1] Batch 445/1200                       Loss D: -95.0269, loss G: -5.2881, gp: 0.1361\n",
      "Epoch [0/1] Batch 446/1200                       Loss D: -3.7245, loss G: -0.8740, gp: 0.0794\n",
      "Epoch [0/1] Batch 447/1200                       Loss D: -98.2246, loss G: 0.6899, gp: 0.0832\n",
      "Epoch [0/1] Batch 448/1200                       Loss D: -96.7533, loss G: -0.9510, gp: 0.0635\n",
      "Epoch [0/1] Batch 449/1200                       Loss D: -7.2076, loss G: 0.7689, gp: 0.0416\n",
      "Epoch [0/1] Batch 450/1200                       Loss D: 0.2045, loss G: 2.8888, gp: 0.1174\n",
      "Epoch [0/1] Batch 451/1200                       Loss D: -8.5469, loss G: 5.6863, gp: 0.1196\n",
      "Epoch [0/1] Batch 452/1200                       Loss D: -69.8841, loss G: 3.1548, gp: 0.1492\n",
      "Epoch [0/1] Batch 453/1200                       Loss D: -88.7436, loss G: -2.5854, gp: 0.2048\n",
      "Epoch [0/1] Batch 454/1200                       Loss D: -89.1930, loss G: -5.0611, gp: 0.2713\n",
      "Epoch [0/1] Batch 455/1200                       Loss D: -74.6217, loss G: -5.6932, gp: 0.3681\n",
      "Epoch [0/1] Batch 456/1200                       Loss D: -97.1857, loss G: -1.0461, gp: 0.1089\n",
      "Epoch [0/1] Batch 457/1200                       Loss D: -100.6124, loss G: 0.3986, gp: 0.0548\n",
      "Epoch [0/1] Batch 458/1200                       Loss D: -100.9810, loss G: -0.0976, gp: 0.1430\n",
      "Epoch [0/1] Batch 459/1200                       Loss D: -81.0084, loss G: -5.1710, gp: 0.5818\n",
      "Epoch [0/1] Batch 460/1200                       Loss D: -99.2382, loss G: 1.8706, gp: 0.2185\n",
      "Epoch [0/1] Batch 461/1200                       Loss D: -101.5215, loss G: 3.1032, gp: 0.2466\n",
      "Epoch [0/1] Batch 462/1200                       Loss D: -95.8113, loss G: -1.6735, gp: 0.0781\n",
      "Epoch [0/1] Batch 463/1200                       Loss D: -100.6437, loss G: 0.9103, gp: 0.5822\n",
      "Epoch [0/1] Batch 464/1200                       Loss D: -98.8362, loss G: -2.8042, gp: 0.1982\n",
      "Epoch [0/1] Batch 465/1200                       Loss D: -104.4324, loss G: -0.2096, gp: 0.1518\n",
      "Epoch [0/1] Batch 466/1200                       Loss D: -26.8852, loss G: -1.2337, gp: 0.1476\n",
      "Epoch [0/1] Batch 467/1200                       Loss D: -99.3986, loss G: -1.5614, gp: 0.2151\n",
      "Epoch [0/1] Batch 468/1200                       Loss D: -103.7878, loss G: -0.5581, gp: 0.0727\n",
      "Epoch [0/1] Batch 469/1200                       Loss D: -5.6095, loss G: 2.4783, gp: 0.2215\n",
      "Epoch [0/1] Batch 470/1200                       Loss D: -97.9720, loss G: -0.1383, gp: 0.1210\n",
      "Epoch [0/1] Batch 471/1200                       Loss D: -96.5840, loss G: -2.1512, gp: 0.1152\n",
      "Epoch [0/1] Batch 472/1200                       Loss D: -60.3621, loss G: -2.5155, gp: 0.5287\n",
      "Epoch [0/1] Batch 473/1200                       Loss D: -94.5320, loss G: -4.0472, gp: 0.0793\n",
      "Epoch [0/1] Batch 474/1200                       Loss D: -108.2618, loss G: 1.0616, gp: 0.0604\n",
      "Epoch [0/1] Batch 475/1200                       Loss D: -100.7121, loss G: -5.1290, gp: 0.1464\n",
      "Epoch [0/1] Batch 476/1200                       Loss D: -105.6545, loss G: -0.4803, gp: 0.1607\n",
      "Epoch [0/1] Batch 477/1200                       Loss D: -84.8817, loss G: -1.6651, gp: 0.2217\n",
      "Epoch [0/1] Batch 478/1200                       Loss D: -105.8760, loss G: 2.3611, gp: 0.1922\n",
      "Epoch [0/1] Batch 479/1200                       Loss D: -105.7446, loss G: -2.9339, gp: 0.0520\n",
      "Epoch [0/1] Batch 480/1200                       Loss D: -115.2374, loss G: 3.7875, gp: 0.1168\n",
      "Epoch [0/1] Batch 481/1200                       Loss D: -115.4601, loss G: 4.5252, gp: 0.1081\n",
      "Epoch [0/1] Batch 482/1200                       Loss D: -1.4782, loss G: 3.9807, gp: 0.1388\n",
      "Epoch [0/1] Batch 483/1200                       Loss D: -111.7005, loss G: 2.4463, gp: 0.0863\n",
      "Epoch [0/1] Batch 484/1200                       Loss D: -106.1123, loss G: -4.4334, gp: 0.1248\n",
      "Epoch [0/1] Batch 485/1200                       Loss D: -110.4877, loss G: 3.1325, gp: 0.1501\n",
      "Epoch [0/1] Batch 486/1200                       Loss D: -112.9726, loss G: 1.5641, gp: 0.1817\n",
      "Epoch [0/1] Batch 487/1200                       Loss D: -110.0972, loss G: 0.2879, gp: 0.0932\n",
      "Epoch [0/1] Batch 488/1200                       Loss D: -108.8778, loss G: -6.3743, gp: 0.0964\n",
      "Epoch [0/1] Batch 489/1200                       Loss D: -110.7246, loss G: -3.8512, gp: 0.0758\n",
      "Epoch [0/1] Batch 490/1200                       Loss D: -16.2682, loss G: -2.1795, gp: 0.0494\n",
      "Epoch [0/1] Batch 491/1200                       Loss D: -112.2174, loss G: -3.4193, gp: 0.0805\n",
      "Epoch [0/1] Batch 492/1200                       Loss D: -100.6079, loss G: -5.7248, gp: 0.0827\n",
      "Epoch [0/1] Batch 493/1200                       Loss D: -114.0878, loss G: -3.5698, gp: 0.1453\n",
      "Epoch [0/1] Batch 494/1200                       Loss D: -118.0821, loss G: 1.1611, gp: 0.1682\n",
      "Epoch [0/1] Batch 495/1200                       Loss D: -112.5202, loss G: -2.9581, gp: 0.1218\n",
      "Epoch [0/1] Batch 496/1200                       Loss D: -113.2272, loss G: 1.8502, gp: 0.1324\n",
      "Epoch [0/1] Batch 497/1200                       Loss D: -120.4648, loss G: 3.2939, gp: 0.1285\n",
      "Epoch [0/1] Batch 498/1200                       Loss D: -43.7818, loss G: -1.9332, gp: 0.1373\n",
      "Epoch [0/1] Batch 499/1200                       Loss D: -52.9614, loss G: -2.8723, gp: 0.0468\n",
      "Epoch [0/1] Batch 500/1200                       Loss D: -108.0938, loss G: 1.5599, gp: 0.0742\n",
      "Epoch [0/1] Batch 501/1200                       Loss D: -106.7706, loss G: -0.5356, gp: 0.0266\n",
      "Epoch [0/1] Batch 502/1200                       Loss D: -109.0329, loss G: -4.8174, gp: 0.1982\n",
      "Epoch [0/1] Batch 503/1200                       Loss D: -119.3243, loss G: 0.9645, gp: 0.1227\n",
      "Epoch [0/1] Batch 504/1200                       Loss D: -116.7044, loss G: 3.0335, gp: 0.1595\n",
      "Epoch [0/1] Batch 505/1200                       Loss D: -121.5922, loss G: 3.8161, gp: 0.1093\n",
      "Epoch [0/1] Batch 506/1200                       Loss D: -120.7399, loss G: 2.5507, gp: 0.0755\n",
      "Epoch [0/1] Batch 507/1200                       Loss D: -125.9417, loss G: 6.4783, gp: 0.0591\n",
      "Epoch [0/1] Batch 508/1200                       Loss D: -119.7982, loss G: -1.4639, gp: 0.1708\n",
      "Epoch [0/1] Batch 509/1200                       Loss D: -118.6910, loss G: -5.8785, gp: 0.1273\n",
      "Epoch [0/1] Batch 510/1200                       Loss D: -119.1163, loss G: 0.2011, gp: 0.0856\n",
      "Epoch [0/1] Batch 511/1200                       Loss D: -117.8968, loss G: 7.6757, gp: 0.2039\n",
      "Epoch [0/1] Batch 512/1200                       Loss D: -109.9315, loss G: 5.4695, gp: 0.1261\n",
      "Epoch [0/1] Batch 513/1200                       Loss D: -0.4116, loss G: 6.0934, gp: 0.1473\n",
      "Epoch [0/1] Batch 514/1200                       Loss D: -11.9956, loss G: -9.4038, gp: 0.2610\n",
      "Epoch [0/1] Batch 515/1200                       Loss D: -3.0532, loss G: 4.4378, gp: 0.1299\n",
      "Epoch [0/1] Batch 516/1200                       Loss D: -1.8075, loss G: 7.5018, gp: 0.0926\n",
      "Epoch [0/1] Batch 517/1200                       Loss D: -89.9121, loss G: 5.9370, gp: 0.1981\n",
      "Epoch [0/1] Batch 518/1200                       Loss D: -127.7981, loss G: 8.7912, gp: 0.1499\n",
      "Epoch [0/1] Batch 519/1200                       Loss D: -132.5129, loss G: 8.4660, gp: 0.0923\n",
      "Epoch [0/1] Batch 520/1200                       Loss D: -137.5858, loss G: 11.4316, gp: 0.1155\n",
      "Epoch [0/1] Batch 521/1200                       Loss D: -2.2121, loss G: 14.2650, gp: 0.0871\n",
      "Epoch [0/1] Batch 522/1200                       Loss D: -64.3058, loss G: -18.8179, gp: 0.1409\n",
      "Epoch [0/1] Batch 523/1200                       Loss D: -115.4767, loss G: -1.5244, gp: 0.5145\n",
      "Epoch [0/1] Batch 524/1200                       Loss D: -135.6095, loss G: 10.9618, gp: 0.1151\n",
      "Epoch [0/1] Batch 525/1200                       Loss D: -138.1783, loss G: 12.6520, gp: 0.0924\n",
      "Epoch [0/1] Batch 526/1200                       Loss D: -136.1490, loss G: 12.7130, gp: 0.2104\n",
      "Epoch [0/1] Batch 527/1200                       Loss D: -90.8966, loss G: 5.8947, gp: 0.1138\n",
      "Epoch [0/1] Batch 528/1200                       Loss D: -33.9854, loss G: -29.2157, gp: 0.0506\n",
      "Epoch [0/1] Batch 529/1200                       Loss D: -142.6666, loss G: 16.0381, gp: 0.0572\n",
      "Epoch [0/1] Batch 530/1200                       Loss D: -125.5213, loss G: 13.2250, gp: 0.0930\n",
      "Epoch [0/1] Batch 531/1200                       Loss D: -36.8370, loss G: 7.9772, gp: 0.2254\n",
      "Epoch [0/1] Batch 532/1200                       Loss D: -127.0158, loss G: 11.8048, gp: 0.0728\n",
      "Epoch [0/1] Batch 533/1200                       Loss D: -145.7747, loss G: 16.2975, gp: 0.0426\n",
      "Epoch [0/1] Batch 534/1200                       Loss D: -150.4060, loss G: 18.5704, gp: 0.0587\n",
      "Epoch [0/1] Batch 535/1200                       Loss D: -155.2573, loss G: 19.2619, gp: 0.0505\n",
      "Epoch [0/1] Batch 536/1200                       Loss D: -6.6002, loss G: 18.9687, gp: 0.0594\n",
      "Epoch [0/1] Batch 537/1200                       Loss D: -1.0421, loss G: 19.2728, gp: 0.0603\n",
      "Epoch [0/1] Batch 538/1200                       Loss D: -4.6195, loss G: 19.4280, gp: 0.0693\n",
      "Epoch [0/1] Batch 539/1200                       Loss D: -12.6069, loss G: 17.8500, gp: 0.0370\n",
      "Epoch [0/1] Batch 540/1200                       Loss D: -141.9633, loss G: 17.4962, gp: 0.3389\n",
      "Epoch [0/1] Batch 541/1200                       Loss D: -8.6714, loss G: 18.8459, gp: 0.1274\n",
      "Epoch [0/1] Batch 542/1200                       Loss D: -89.8122, loss G: 16.8688, gp: 1.2999\n",
      "Epoch [0/1] Batch 543/1200                       Loss D: -124.0429, loss G: 1.2288, gp: 0.0571\n",
      "Epoch [0/1] Batch 544/1200                       Loss D: -136.0364, loss G: 8.1454, gp: 0.0550\n",
      "Epoch [0/1] Batch 545/1200                       Loss D: -129.8046, loss G: 0.8439, gp: 0.2835\n",
      "Epoch [0/1] Batch 546/1200                       Loss D: -147.9237, loss G: 13.8980, gp: 0.1005\n",
      "Epoch [0/1] Batch 547/1200                       Loss D: 0.5564, loss G: 12.1316, gp: 0.2478\n",
      "Epoch [0/1] Batch 548/1200                       Loss D: -126.6182, loss G: 7.3620, gp: 0.0609\n",
      "Epoch [0/1] Batch 549/1200                       Loss D: -1.4340, loss G: 10.6463, gp: 0.0771\n",
      "Epoch [0/1] Batch 550/1200                       Loss D: -18.9545, loss G: 12.6882, gp: 0.1201\n",
      "Epoch [0/1] Batch 551/1200                       Loss D: -141.5696, loss G: 6.3546, gp: 0.1830\n",
      "Epoch [0/1] Batch 552/1200                       Loss D: -152.7153, loss G: 14.4454, gp: 0.1006\n",
      "Epoch [0/1] Batch 553/1200                       Loss D: -133.5831, loss G: -30.4348, gp: 0.1207\n",
      "Epoch [0/1] Batch 554/1200                       Loss D: -135.1395, loss G: 7.9308, gp: 0.0799\n",
      "Epoch [0/1] Batch 555/1200                       Loss D: -146.4163, loss G: 7.1412, gp: 0.0649\n",
      "Epoch [0/1] Batch 556/1200                       Loss D: -152.5409, loss G: 16.7013, gp: 0.0841\n",
      "Epoch [0/1] Batch 557/1200                       Loss D: -141.2275, loss G: 6.6268, gp: 0.0778\n",
      "Epoch [0/1] Batch 558/1200                       Loss D: -149.3039, loss G: 8.0728, gp: 0.0723\n",
      "Epoch [0/1] Batch 559/1200                       Loss D: -158.4821, loss G: 17.0675, gp: 0.0565\n",
      "Epoch [0/1] Batch 560/1200                       Loss D: -155.4149, loss G: 17.2912, gp: 0.2951\n",
      "Epoch [0/1] Batch 561/1200                       Loss D: -18.7916, loss G: -88.7222, gp: 0.0497\n",
      "Epoch [0/1] Batch 562/1200                       Loss D: -2.1899, loss G: 22.1327, gp: 0.0411\n",
      "Epoch [0/1] Batch 563/1200                       Loss D: -151.0900, loss G: 8.3459, gp: 0.0941\n",
      "Epoch [0/1] Batch 564/1200                       Loss D: -108.8684, loss G: 12.4575, gp: 0.6065\n",
      "Epoch [0/1] Batch 565/1200                       Loss D: -78.2447, loss G: 1.5855, gp: 0.0898\n",
      "Epoch [0/1] Batch 566/1200                       Loss D: -153.9977, loss G: 9.7511, gp: 0.0587\n",
      "Epoch [0/1] Batch 567/1200                       Loss D: -142.2652, loss G: 0.0334, gp: 0.0814\n",
      "Epoch [0/1] Batch 568/1200                       Loss D: -10.5715, loss G: 9.7285, gp: 0.1249\n",
      "Epoch [0/1] Batch 569/1200                       Loss D: -91.0724, loss G: 10.1149, gp: 0.0770\n",
      "Epoch [0/1] Batch 570/1200                       Loss D: -147.6605, loss G: 5.0615, gp: 0.0598\n",
      "Epoch [0/1] Batch 571/1200                       Loss D: -99.7566, loss G: 11.7047, gp: 0.4193\n",
      "Epoch [0/1] Batch 572/1200                       Loss D: -159.1683, loss G: 14.0424, gp: 0.0568\n",
      "Epoch [0/1] Batch 573/1200                       Loss D: -145.5183, loss G: 10.6673, gp: 0.0715\n",
      "Epoch [0/1] Batch 574/1200                       Loss D: -102.1316, loss G: -9.0203, gp: 0.3272\n",
      "Epoch [0/1] Batch 575/1200                       Loss D: -130.5547, loss G: -0.6535, gp: 0.3273\n",
      "Epoch [0/1] Batch 576/1200                       Loss D: -150.2829, loss G: 12.8099, gp: 0.2185\n",
      "Epoch [0/1] Batch 577/1200                       Loss D: -161.8183, loss G: 14.2823, gp: 0.0696\n",
      "Epoch [0/1] Batch 578/1200                       Loss D: -2.5800, loss G: 17.7896, gp: 0.2606\n",
      "Epoch [0/1] Batch 579/1200                       Loss D: -18.8050, loss G: 3.1724, gp: 0.0370\n",
      "Epoch [0/1] Batch 580/1200                       Loss D: -126.0814, loss G: -1.0669, gp: 0.0464\n",
      "Epoch [0/1] Batch 581/1200                       Loss D: -169.6994, loss G: 18.2955, gp: 0.1102\n",
      "Epoch [0/1] Batch 582/1200                       Loss D: -165.9754, loss G: 17.4509, gp: 0.0684\n",
      "Epoch [0/1] Batch 583/1200                       Loss D: -143.4463, loss G: 11.3690, gp: 0.0508\n",
      "Epoch [0/1] Batch 584/1200                       Loss D: -161.8545, loss G: 6.8157, gp: 0.0446\n",
      "Epoch [0/1] Batch 585/1200                       Loss D: -169.5237, loss G: 17.3451, gp: 0.0291\n",
      "Epoch [0/1] Batch 586/1200                       Loss D: -174.0135, loss G: 19.8667, gp: 0.0811\n",
      "Epoch [0/1] Batch 587/1200                       Loss D: -171.1283, loss G: 19.1126, gp: 0.0403\n",
      "Epoch [0/1] Batch 588/1200                       Loss D: -171.4711, loss G: 17.7458, gp: 0.0707\n",
      "Epoch [0/1] Batch 589/1200                       Loss D: -178.0607, loss G: 18.7243, gp: 0.0443\n",
      "Epoch [0/1] Batch 590/1200                       Loss D: -175.0346, loss G: 15.2909, gp: 0.0812\n",
      "Epoch [0/1] Batch 591/1200                       Loss D: -176.8763, loss G: 20.6141, gp: 0.0712\n",
      "Epoch [0/1] Batch 592/1200                       Loss D: -181.3279, loss G: 21.5937, gp: 0.0951\n",
      "Epoch [0/1] Batch 593/1200                       Loss D: 3.2079, loss G: 19.2189, gp: 0.0482\n",
      "Epoch [0/1] Batch 594/1200                       Loss D: -16.7430, loss G: 20.9764, gp: 0.0472\n",
      "Epoch [0/1] Batch 595/1200                       Loss D: -179.6682, loss G: 17.4675, gp: 0.0432\n",
      "Epoch [0/1] Batch 596/1200                       Loss D: -21.6062, loss G: 18.6217, gp: 0.0312\n",
      "Epoch [0/1] Batch 597/1200                       Loss D: 3.0506, loss G: 14.0561, gp: 0.0570\n",
      "Epoch [0/1] Batch 598/1200                       Loss D: -9.9411, loss G: 15.2138, gp: 0.0377\n",
      "Epoch [0/1] Batch 599/1200                       Loss D: -87.0577, loss G: 8.0345, gp: 0.0217\n",
      "Epoch [0/1] Batch 600/1200                       Loss D: -179.0643, loss G: 16.4276, gp: 0.0272\n",
      "Epoch [0/1] Batch 601/1200                       Loss D: -175.9822, loss G: 15.8960, gp: 0.0332\n",
      "Epoch [0/1] Batch 602/1200                       Loss D: -103.1020, loss G: 11.6306, gp: 0.0754\n",
      "Epoch [0/1] Batch 603/1200                       Loss D: -102.6010, loss G: 15.6977, gp: 0.0446\n",
      "Epoch [0/1] Batch 604/1200                       Loss D: -152.7834, loss G: 19.9377, gp: 0.1005\n",
      "Epoch [0/1] Batch 605/1200                       Loss D: -125.1749, loss G: -38.5084, gp: 0.0548\n",
      "Epoch [0/1] Batch 606/1200                       Loss D: -75.5229, loss G: 18.7035, gp: 0.6361\n",
      "Epoch [0/1] Batch 607/1200                       Loss D: -146.3841, loss G: 15.0539, gp: 0.3415\n",
      "Epoch [0/1] Batch 608/1200                       Loss D: -20.4653, loss G: 10.3711, gp: 0.3039\n",
      "Epoch [0/1] Batch 609/1200                       Loss D: -159.3417, loss G: 19.6042, gp: 0.0492\n",
      "Epoch [0/1] Batch 610/1200                       Loss D: -167.7516, loss G: 20.8869, gp: 0.3017\n",
      "Epoch [0/1] Batch 611/1200                       Loss D: -102.5685, loss G: -22.6132, gp: 0.0493\n",
      "Epoch [0/1] Batch 612/1200                       Loss D: -140.3586, loss G: 17.7116, gp: 0.0763\n",
      "Epoch [0/1] Batch 613/1200                       Loss D: -137.7102, loss G: 24.7264, gp: 0.0377\n",
      "Epoch [0/1] Batch 614/1200                       Loss D: -175.9120, loss G: 23.6833, gp: 0.0518\n",
      "Epoch [0/1] Batch 615/1200                       Loss D: -170.8154, loss G: 17.0543, gp: 0.0549\n",
      "Epoch [0/1] Batch 616/1200                       Loss D: -141.7084, loss G: 3.6147, gp: 0.1374\n",
      "Epoch [0/1] Batch 617/1200                       Loss D: -77.8003, loss G: -49.4834, gp: 0.0770\n",
      "Epoch [0/1] Batch 618/1200                       Loss D: -173.0609, loss G: 14.1253, gp: 0.0329\n",
      "Epoch [0/1] Batch 619/1200                       Loss D: -177.4316, loss G: 19.1711, gp: 0.2436\n",
      "Epoch [0/1] Batch 620/1200                       Loss D: -184.8488, loss G: 18.7828, gp: 0.0456\n",
      "Epoch [0/1] Batch 621/1200                       Loss D: -186.6279, loss G: 17.9874, gp: 0.0795\n",
      "Epoch [0/1] Batch 622/1200                       Loss D: -83.6968, loss G: -36.1588, gp: 0.1238\n",
      "Epoch [0/1] Batch 623/1200                       Loss D: -192.5349, loss G: 27.4213, gp: 0.0491\n",
      "Epoch [0/1] Batch 624/1200                       Loss D: -159.4225, loss G: 24.6363, gp: 0.0614\n",
      "Epoch [0/1] Batch 625/1200                       Loss D: -194.4441, loss G: 27.5038, gp: 0.0413\n",
      "Epoch [0/1] Batch 626/1200                       Loss D: -208.5799, loss G: 33.8557, gp: 0.1470\n",
      "Epoch [0/1] Batch 627/1200                       Loss D: -213.9618, loss G: 36.7871, gp: 0.0638\n",
      "Epoch [0/1] Batch 628/1200                       Loss D: -208.5521, loss G: 31.7940, gp: 0.0648\n",
      "Epoch [0/1] Batch 629/1200                       Loss D: -204.0225, loss G: 29.6778, gp: 0.1562\n",
      "Epoch [0/1] Batch 630/1200                       Loss D: -217.4683, loss G: 36.5505, gp: 0.2557\n",
      "Epoch [0/1] Batch 631/1200                       Loss D: -225.5271, loss G: 39.7590, gp: 0.1017\n",
      "Epoch [0/1] Batch 632/1200                       Loss D: -227.1061, loss G: 40.7873, gp: 0.0412\n",
      "Epoch [0/1] Batch 633/1200                       Loss D: -232.1810, loss G: 44.4610, gp: 0.0433\n",
      "Epoch [0/1] Batch 634/1200                       Loss D: -237.9464, loss G: 46.5493, gp: 0.0533\n",
      "Epoch [0/1] Batch 635/1200                       Loss D: -238.7896, loss G: 48.8983, gp: 0.0410\n",
      "Epoch [0/1] Batch 636/1200                       Loss D: -239.8700, loss G: 46.6435, gp: 0.0332\n",
      "Epoch [0/1] Batch 637/1200                       Loss D: -244.1146, loss G: 48.9446, gp: 0.2948\n",
      "Epoch [0/1] Batch 638/1200                       Loss D: -252.9634, loss G: 51.8119, gp: 0.1057\n",
      "Epoch [0/1] Batch 639/1200                       Loss D: -255.0863, loss G: 52.9489, gp: 0.0374\n",
      "Epoch [0/1] Batch 640/1200                       Loss D: -249.3778, loss G: 49.0941, gp: 0.0807\n",
      "Epoch [0/1] Batch 641/1200                       Loss D: -259.0157, loss G: 53.6658, gp: 0.0355\n",
      "Epoch [0/1] Batch 642/1200                       Loss D: -256.9300, loss G: 52.0079, gp: 0.2004\n",
      "Epoch [0/1] Batch 643/1200                       Loss D: -263.0617, loss G: 53.4367, gp: 0.1116\n",
      "Epoch [0/1] Batch 644/1200                       Loss D: -270.1031, loss G: 57.5051, gp: 0.0862\n",
      "Epoch [0/1] Batch 645/1200                       Loss D: -268.9346, loss G: 56.5471, gp: 0.0528\n",
      "Epoch [0/1] Batch 646/1200                       Loss D: -271.0624, loss G: 58.1341, gp: 0.2190\n",
      "Epoch [0/1] Batch 647/1200                       Loss D: -273.2999, loss G: 59.4745, gp: 0.2096\n",
      "Epoch [0/1] Batch 648/1200                       Loss D: -280.6627, loss G: 60.7867, gp: 0.0364\n",
      "Epoch [0/1] Batch 649/1200                       Loss D: -285.5571, loss G: 63.0409, gp: 0.0329\n",
      "Epoch [0/1] Batch 650/1200                       Loss D: -285.8828, loss G: 62.7030, gp: 0.0385\n",
      "Epoch [0/1] Batch 651/1200                       Loss D: -296.7985, loss G: 66.8564, gp: 0.0280\n",
      "Epoch [0/1] Batch 652/1200                       Loss D: -292.5980, loss G: 65.8822, gp: 0.0355\n",
      "Epoch [0/1] Batch 653/1200                       Loss D: -298.8137, loss G: 67.2194, gp: 0.0303\n",
      "Epoch [0/1] Batch 654/1200                       Loss D: -304.2194, loss G: 70.7390, gp: 0.0522\n",
      "Epoch [0/1] Batch 655/1200                       Loss D: -305.0664, loss G: 55.0685, gp: 0.6242\n",
      "Epoch [0/1] Batch 656/1200                       Loss D: -272.0160, loss G: 57.7112, gp: 0.0763\n",
      "Epoch [0/1] Batch 657/1200                       Loss D: -282.3109, loss G: 45.7941, gp: 0.0852\n",
      "Epoch [0/1] Batch 658/1200                       Loss D: -298.6940, loss G: 63.2119, gp: 0.2545\n",
      "Epoch [0/1] Batch 659/1200                       Loss D: -298.3111, loss G: 61.2647, gp: 0.4350\n",
      "Epoch [0/1] Batch 660/1200                       Loss D: -293.4949, loss G: 50.8144, gp: 2.2849\n",
      "Epoch [0/1] Batch 661/1200                       Loss D: -311.4328, loss G: 67.9451, gp: 0.8714\n",
      "Epoch [0/1] Batch 662/1200                       Loss D: -325.7575, loss G: 76.1050, gp: 0.0888\n",
      "Epoch [0/1] Batch 663/1200                       Loss D: -321.6907, loss G: 51.8239, gp: 1.4937\n",
      "Epoch [0/1] Batch 664/1200                       Loss D: -329.4239, loss G: 79.2429, gp: 0.1119\n",
      "Epoch [0/1] Batch 665/1200                       Loss D: -313.1219, loss G: 60.0023, gp: 0.0592\n",
      "Epoch [0/1] Batch 666/1200                       Loss D: -343.1693, loss G: 82.6550, gp: 0.3334\n",
      "Epoch [0/1] Batch 667/1200                       Loss D: -348.7335, loss G: 83.6744, gp: 0.0848\n",
      "Epoch [0/1] Batch 668/1200                       Loss D: -357.7687, loss G: 87.7025, gp: 0.0343\n",
      "Epoch [0/1] Batch 669/1200                       Loss D: -353.3809, loss G: 86.4383, gp: 0.0909\n",
      "Epoch [0/1] Batch 670/1200                       Loss D: -330.5419, loss G: 53.9979, gp: 0.0498\n",
      "Epoch [0/1] Batch 671/1200                       Loss D: -366.5788, loss G: 90.1036, gp: 0.0713\n",
      "Epoch [0/1] Batch 672/1200                       Loss D: -377.9839, loss G: 93.0500, gp: 0.0627\n",
      "Epoch [0/1] Batch 673/1200                       Loss D: -385.7279, loss G: 94.9663, gp: 0.0824\n",
      "Epoch [0/1] Batch 674/1200                       Loss D: -390.4225, loss G: 97.9125, gp: 0.0846\n",
      "Epoch [0/1] Batch 675/1200                       Loss D: -344.2655, loss G: 52.5298, gp: 0.0445\n",
      "Epoch [0/1] Batch 676/1200                       Loss D: -355.6991, loss G: 59.1213, gp: 0.0465\n",
      "Epoch [0/1] Batch 677/1200                       Loss D: -364.8160, loss G: 70.2365, gp: 0.0651\n",
      "Epoch [0/1] Batch 678/1200                       Loss D: -385.4847, loss G: 91.8781, gp: 0.0795\n",
      "Epoch [0/1] Batch 679/1200                       Loss D: -403.2164, loss G: 99.4174, gp: 0.2052\n",
      "Epoch [0/1] Batch 680/1200                       Loss D: -408.9129, loss G: 100.4263, gp: 0.1490\n",
      "Epoch [0/1] Batch 681/1200                       Loss D: -413.5197, loss G: 102.7906, gp: 0.0660\n",
      "Epoch [0/1] Batch 682/1200                       Loss D: -417.3345, loss G: 101.8635, gp: 0.0533\n",
      "Epoch [0/1] Batch 683/1200                       Loss D: -415.9346, loss G: 101.1745, gp: 0.3913\n",
      "Epoch [0/1] Batch 684/1200                       Loss D: -427.4778, loss G: 103.5716, gp: 0.0443\n",
      "Epoch [0/1] Batch 685/1200                       Loss D: -419.1532, loss G: 102.8623, gp: 0.5248\n",
      "Epoch [0/1] Batch 686/1200                       Loss D: -436.6379, loss G: 105.9617, gp: 0.0487\n",
      "Epoch [0/1] Batch 687/1200                       Loss D: -439.6417, loss G: 105.0707, gp: 0.0417\n",
      "Epoch [0/1] Batch 688/1200                       Loss D: -439.0596, loss G: 80.7467, gp: 0.5406\n",
      "Epoch [0/1] Batch 689/1200                       Loss D: -422.9696, loss G: 81.0240, gp: 2.6318\n",
      "Epoch [0/1] Batch 690/1200                       Loss D: -454.4018, loss G: 113.8287, gp: 0.0930\n",
      "Epoch [0/1] Batch 691/1200                       Loss D: -465.2360, loss G: 115.9218, gp: 0.0648\n",
      "Epoch [0/1] Batch 692/1200                       Loss D: -469.8252, loss G: 117.2783, gp: 0.2995\n",
      "Epoch [0/1] Batch 693/1200                       Loss D: -426.0098, loss G: 76.0197, gp: 0.0592\n",
      "Epoch [0/1] Batch 694/1200                       Loss D: -428.7892, loss G: 76.4690, gp: 0.0461\n",
      "Epoch [0/1] Batch 695/1200                       Loss D: -430.6628, loss G: 77.5012, gp: 0.0600\n",
      "Epoch [0/1] Batch 696/1200                       Loss D: -433.1626, loss G: 79.5910, gp: 0.0797\n",
      "Epoch [0/1] Batch 697/1200                       Loss D: -436.1972, loss G: 84.6102, gp: 0.1206\n",
      "Epoch [0/1] Batch 698/1200                       Loss D: -447.1830, loss G: 89.2386, gp: 0.1728\n",
      "Epoch [0/1] Batch 699/1200                       Loss D: -453.2052, loss G: 100.1885, gp: 0.1577\n",
      "Epoch [0/1] Batch 700/1200                       Loss D: -469.7811, loss G: 114.7721, gp: 0.3059\n",
      "Epoch [0/1] Batch 701/1200                       Loss D: -469.7440, loss G: 119.0021, gp: 0.7518\n",
      "Epoch [0/1] Batch 702/1200                       Loss D: -488.2652, loss G: 123.2017, gp: 0.2408\n",
      "Epoch [0/1] Batch 703/1200                       Loss D: -474.0837, loss G: 97.3463, gp: 2.1522\n",
      "Epoch [0/1] Batch 704/1200                       Loss D: -488.5187, loss G: 126.3429, gp: 0.1146\n",
      "Epoch [0/1] Batch 705/1200                       Loss D: -473.5680, loss G: 96.1488, gp: 2.8709\n",
      "Epoch [0/1] Batch 706/1200                       Loss D: -507.4014, loss G: 130.4464, gp: 0.1504\n",
      "Epoch [0/1] Batch 707/1200                       Loss D: -505.7774, loss G: 128.1680, gp: 0.1495\n",
      "Epoch [0/1] Batch 708/1200                       Loss D: -516.2710, loss G: 132.1581, gp: 0.1004\n",
      "Epoch [0/1] Batch 709/1200                       Loss D: -526.6977, loss G: 132.8843, gp: 0.1199\n",
      "Epoch [0/1] Batch 710/1200                       Loss D: -522.6348, loss G: 132.0874, gp: 0.1563\n",
      "Epoch [0/1] Batch 711/1200                       Loss D: -488.5869, loss G: 102.2737, gp: 0.1178\n",
      "Epoch [0/1] Batch 712/1200                       Loss D: -533.7463, loss G: 134.4911, gp: 0.1260\n",
      "Epoch [0/1] Batch 713/1200                       Loss D: -531.1987, loss G: 135.5486, gp: 0.1224\n",
      "Epoch [0/1] Batch 714/1200                       Loss D: -541.1125, loss G: 137.8180, gp: 0.1122\n",
      "Epoch [0/1] Batch 715/1200                       Loss D: -547.7407, loss G: 139.9904, gp: 0.0999\n",
      "Epoch [0/1] Batch 716/1200                       Loss D: -554.3172, loss G: 138.6621, gp: 0.0940\n",
      "Epoch [0/1] Batch 717/1200                       Loss D: -501.6560, loss G: 94.8894, gp: 0.0838\n",
      "Epoch [0/1] Batch 718/1200                       Loss D: -519.9562, loss G: 111.5670, gp: 0.0794\n",
      "Epoch [0/1] Batch 719/1200                       Loss D: -550.5452, loss G: 139.1235, gp: 0.1437\n",
      "Epoch [0/1] Batch 720/1200                       Loss D: -559.9172, loss G: 139.3486, gp: 0.1782\n",
      "Epoch [0/1] Batch 721/1200                       Loss D: -561.8663, loss G: 139.4806, gp: 0.1028\n",
      "Epoch [0/1] Batch 722/1200                       Loss D: -564.1816, loss G: 140.9256, gp: 0.1078\n",
      "Epoch [0/1] Batch 723/1200                       Loss D: -565.9875, loss G: 138.2227, gp: 0.1667\n",
      "Epoch [0/1] Batch 724/1200                       Loss D: -569.7581, loss G: 137.9915, gp: 0.1285\n",
      "Epoch [0/1] Batch 725/1200                       Loss D: -567.6269, loss G: 132.7815, gp: 0.0945\n",
      "Epoch [0/1] Batch 726/1200                       Loss D: -567.7408, loss G: 131.2430, gp: 0.2064\n",
      "Epoch [0/1] Batch 727/1200                       Loss D: -571.5378, loss G: 129.6728, gp: 0.0998\n",
      "Epoch [0/1] Batch 728/1200                       Loss D: -573.1174, loss G: 128.4787, gp: 0.0771\n",
      "Epoch [0/1] Batch 729/1200                       Loss D: -579.3193, loss G: 130.8750, gp: 0.0766\n",
      "Epoch [0/1] Batch 730/1200                       Loss D: -575.0058, loss G: 129.2416, gp: 0.1615\n",
      "Epoch [0/1] Batch 731/1200                       Loss D: -578.4604, loss G: 118.6736, gp: 0.5582\n",
      "Epoch [0/1] Batch 732/1200                       Loss D: -583.8693, loss G: 127.3294, gp: 0.0857\n",
      "Epoch [0/1] Batch 733/1200                       Loss D: -593.8418, loss G: 128.9902, gp: 0.1753\n",
      "Epoch [0/1] Batch 734/1200                       Loss D: -599.4005, loss G: 131.6870, gp: 0.2865\n",
      "Epoch [0/1] Batch 735/1200                       Loss D: -609.0062, loss G: 135.5329, gp: 0.0808\n",
      "Epoch [0/1] Batch 736/1200                       Loss D: -614.2614, loss G: 136.4374, gp: 0.1123\n",
      "Epoch [0/1] Batch 737/1200                       Loss D: -616.8094, loss G: 138.9534, gp: 0.1500\n",
      "Epoch [0/1] Batch 738/1200                       Loss D: -628.7784, loss G: 141.9538, gp: 0.0963\n",
      "Epoch [0/1] Batch 739/1200                       Loss D: -632.5608, loss G: 145.1821, gp: 0.0722\n",
      "Epoch [0/1] Batch 740/1200                       Loss D: -641.8347, loss G: 147.5368, gp: 0.1876\n",
      "Epoch [0/1] Batch 741/1200                       Loss D: -644.3508, loss G: 149.0249, gp: 0.0997\n",
      "Epoch [0/1] Batch 742/1200                       Loss D: -653.0887, loss G: 150.9418, gp: 0.2961\n",
      "Epoch [0/1] Batch 743/1200                       Loss D: -660.4225, loss G: 152.9940, gp: 0.1746\n",
      "Epoch [0/1] Batch 744/1200                       Loss D: -667.4694, loss G: 155.9303, gp: 0.1293\n",
      "Epoch [0/1] Batch 745/1200                       Loss D: -674.6430, loss G: 157.8262, gp: 0.1978\n",
      "Epoch [0/1] Batch 746/1200                       Loss D: -677.7910, loss G: 160.3973, gp: 0.1466\n",
      "Epoch [0/1] Batch 747/1200                       Loss D: -682.4729, loss G: 158.8208, gp: 0.0624\n",
      "Epoch [0/1] Batch 748/1200                       Loss D: -686.8214, loss G: 159.6659, gp: 0.1457\n",
      "Epoch [0/1] Batch 749/1200                       Loss D: -692.1016, loss G: 157.7889, gp: 0.2071\n",
      "Epoch [0/1] Batch 750/1200                       Loss D: -692.7281, loss G: 156.1741, gp: 0.2302\n",
      "Epoch [0/1] Batch 751/1200                       Loss D: -687.8513, loss G: 152.0509, gp: 0.1367\n",
      "Epoch [0/1] Batch 752/1200                       Loss D: -687.4362, loss G: 146.9831, gp: 0.3592\n",
      "Epoch [0/1] Batch 753/1200                       Loss D: -692.7095, loss G: 145.9815, gp: 0.2076\n",
      "Epoch [0/1] Batch 754/1200                       Loss D: -692.4623, loss G: 145.1028, gp: 0.2768\n",
      "Epoch [0/1] Batch 755/1200                       Loss D: -696.4941, loss G: 144.8634, gp: 0.3286\n",
      "Epoch [0/1] Batch 756/1200                       Loss D: -700.9608, loss G: 144.4110, gp: 0.2896\n",
      "Epoch [0/1] Batch 757/1200                       Loss D: -701.1671, loss G: 140.3524, gp: 0.3484\n",
      "Epoch [0/1] Batch 758/1200                       Loss D: -703.9650, loss G: 139.4332, gp: 0.3080\n",
      "Epoch [0/1] Batch 759/1200                       Loss D: -704.2098, loss G: 138.3423, gp: 0.2150\n",
      "Epoch [0/1] Batch 760/1200                       Loss D: -702.0840, loss G: 139.5859, gp: 0.2448\n",
      "Epoch [0/1] Batch 761/1200                       Loss D: -704.1002, loss G: 137.5708, gp: 0.2242\n",
      "Epoch [0/1] Batch 762/1200                       Loss D: -705.7863, loss G: 136.2182, gp: 0.3389\n",
      "Epoch [0/1] Batch 763/1200                       Loss D: -714.9799, loss G: 135.9650, gp: 0.1129\n",
      "Epoch [0/1] Batch 764/1200                       Loss D: -715.7988, loss G: 137.6992, gp: 0.2685\n",
      "Epoch [0/1] Batch 765/1200                       Loss D: -714.4008, loss G: 135.0119, gp: 0.1634\n",
      "Epoch [0/1] Batch 766/1200                       Loss D: -719.3118, loss G: 134.4440, gp: 0.2555\n",
      "Epoch [0/1] Batch 767/1200                       Loss D: -715.1983, loss G: 128.7227, gp: 0.3381\n",
      "Epoch [0/1] Batch 768/1200                       Loss D: -718.1269, loss G: 129.3882, gp: 0.4130\n",
      "Epoch [0/1] Batch 769/1200                       Loss D: -717.3959, loss G: 126.6219, gp: 0.1897\n",
      "Epoch [0/1] Batch 770/1200                       Loss D: -717.7599, loss G: 123.9658, gp: 0.1483\n",
      "Epoch [0/1] Batch 771/1200                       Loss D: -712.9394, loss G: 124.0450, gp: 0.5878\n",
      "Epoch [0/1] Batch 772/1200                       Loss D: -702.2142, loss G: 106.0446, gp: 0.1722\n",
      "Epoch [0/1] Batch 773/1200                       Loss D: -712.9067, loss G: 111.4966, gp: 0.2375\n",
      "Epoch [0/1] Batch 774/1200                       Loss D: -715.0180, loss G: 109.6429, gp: 0.2488\n",
      "Epoch [0/1] Batch 775/1200                       Loss D: -713.5000, loss G: 108.4324, gp: 0.1275\n",
      "Epoch [0/1] Batch 776/1200                       Loss D: -722.0491, loss G: 116.5394, gp: 0.3723\n",
      "Epoch [0/1] Batch 777/1200                       Loss D: -722.4584, loss G: 114.9256, gp: 0.1572\n",
      "Epoch [0/1] Batch 778/1200                       Loss D: -726.6428, loss G: 110.5377, gp: 0.2959\n",
      "Epoch [0/1] Batch 779/1200                       Loss D: -715.9390, loss G: 105.8727, gp: 0.7041\n",
      "Epoch [0/1] Batch 780/1200                       Loss D: -717.1179, loss G: 103.0672, gp: 0.4260\n",
      "Epoch [0/1] Batch 781/1200                       Loss D: -754.1844, loss G: 130.8838, gp: 0.2087\n",
      "Epoch [0/1] Batch 782/1200                       Loss D: -749.4194, loss G: 127.7278, gp: 0.1578\n",
      "Epoch [0/1] Batch 783/1200                       Loss D: -728.6063, loss G: 103.6221, gp: 0.3210\n",
      "Epoch [0/1] Batch 784/1200                       Loss D: -738.9330, loss G: 113.3727, gp: 0.2577\n",
      "Epoch [0/1] Batch 785/1200                       Loss D: -747.9023, loss G: 117.3634, gp: 0.1919\n",
      "Epoch [0/1] Batch 786/1200                       Loss D: -743.4238, loss G: 115.4519, gp: 0.4646\n",
      "Epoch [0/1] Batch 787/1200                       Loss D: -730.8031, loss G: 93.9572, gp: 0.1321\n",
      "Epoch [0/1] Batch 788/1200                       Loss D: -746.7752, loss G: 111.6113, gp: 0.8040\n",
      "Epoch [0/1] Batch 789/1200                       Loss D: -740.1249, loss G: 108.8478, gp: 1.2387\n",
      "Epoch [0/1] Batch 790/1200                       Loss D: -745.4933, loss G: 105.1735, gp: 0.2831\n",
      "Epoch [0/1] Batch 791/1200                       Loss D: -769.5394, loss G: 130.3012, gp: 0.2872\n",
      "Epoch [0/1] Batch 792/1200                       Loss D: -749.7405, loss G: 104.9590, gp: 0.4474\n",
      "Epoch [0/1] Batch 793/1200                       Loss D: -779.8673, loss G: 131.3755, gp: 0.2343\n",
      "Epoch [0/1] Batch 794/1200                       Loss D: -759.6201, loss G: 112.1108, gp: 0.2362\n",
      "Epoch [0/1] Batch 795/1200                       Loss D: -779.3708, loss G: 122.9314, gp: 0.1951\n",
      "Epoch [0/1] Batch 796/1200                       Loss D: -734.0107, loss G: 77.2472, gp: 0.2818\n",
      "Epoch [0/1] Batch 797/1200                       Loss D: -789.9276, loss G: 133.4631, gp: 0.3054\n",
      "Epoch [0/1] Batch 798/1200                       Loss D: -764.2764, loss G: 108.6960, gp: 0.1959\n",
      "Epoch [0/1] Batch 799/1200                       Loss D: -792.7894, loss G: 131.8618, gp: 0.4954\n",
      "Epoch [0/1] Batch 800/1200                       Loss D: -766.2500, loss G: 112.0697, gp: 0.7563\n",
      "Epoch [0/1] Batch 801/1200                       Loss D: -782.3443, loss G: 117.7724, gp: 0.2645\n",
      "Epoch [0/1] Batch 802/1200                       Loss D: -795.6450, loss G: 130.8819, gp: 0.2935\n",
      "Epoch [0/1] Batch 803/1200                       Loss D: -804.5512, loss G: 131.4167, gp: 0.1950\n",
      "Epoch [0/1] Batch 804/1200                       Loss D: -805.0815, loss G: 134.1212, gp: 0.4560\n",
      "Epoch [0/1] Batch 805/1200                       Loss D: -806.1267, loss G: 131.5259, gp: 0.2325\n",
      "Epoch [0/1] Batch 806/1200                       Loss D: -809.1671, loss G: 135.1266, gp: 0.3161\n",
      "Epoch [0/1] Batch 807/1200                       Loss D: -816.7753, loss G: 135.8208, gp: 0.2007\n",
      "Epoch [0/1] Batch 808/1200                       Loss D: -817.5916, loss G: 138.3417, gp: 0.4203\n",
      "Epoch [0/1] Batch 809/1200                       Loss D: -818.8802, loss G: 138.8157, gp: 0.1557\n",
      "Epoch [0/1] Batch 810/1200                       Loss D: -828.6701, loss G: 143.1163, gp: 0.2183\n",
      "Epoch [0/1] Batch 811/1200                       Loss D: -827.7051, loss G: 145.8452, gp: 0.6171\n",
      "Epoch [0/1] Batch 812/1200                       Loss D: -833.3921, loss G: 144.3932, gp: 0.1493\n",
      "Epoch [0/1] Batch 813/1200                       Loss D: -841.8122, loss G: 148.3817, gp: 0.1763\n",
      "Epoch [0/1] Batch 814/1200                       Loss D: -842.9223, loss G: 148.2845, gp: 0.0881\n",
      "Epoch [0/1] Batch 815/1200                       Loss D: -842.2415, loss G: 142.9146, gp: 0.1245\n",
      "Epoch [0/1] Batch 816/1200                       Loss D: -825.4951, loss G: 124.8710, gp: 0.4133\n",
      "Epoch [0/1] Batch 817/1200                       Loss D: -838.3232, loss G: 133.6635, gp: 0.1111\n",
      "Epoch [0/1] Batch 818/1200                       Loss D: -830.9834, loss G: 128.8818, gp: 0.2782\n",
      "Epoch [0/1] Batch 819/1200                       Loss D: -841.9185, loss G: 133.0722, gp: 0.1781\n",
      "Epoch [0/1] Batch 820/1200                       Loss D: -835.8373, loss G: 126.2259, gp: 0.1367\n",
      "Epoch [0/1] Batch 821/1200                       Loss D: -866.4897, loss G: 153.4130, gp: 0.0863\n",
      "Epoch [0/1] Batch 822/1200                       Loss D: -868.9904, loss G: 152.7816, gp: 0.1391\n",
      "Epoch [0/1] Batch 823/1200                       Loss D: -869.1791, loss G: 148.0224, gp: 0.0604\n",
      "Epoch [0/1] Batch 824/1200                       Loss D: -855.9911, loss G: 141.6655, gp: 0.4666\n",
      "Epoch [0/1] Batch 825/1200                       Loss D: -866.8550, loss G: 149.1932, gp: 0.3759\n",
      "Epoch [0/1] Batch 826/1200                       Loss D: -869.8663, loss G: 145.6819, gp: 0.1488\n",
      "Epoch [0/1] Batch 827/1200                       Loss D: -868.2021, loss G: 141.3400, gp: 0.1525\n",
      "Epoch [0/1] Batch 828/1200                       Loss D: -864.2171, loss G: 143.2672, gp: 0.2702\n",
      "Epoch [0/1] Batch 829/1200                       Loss D: -870.4238, loss G: 143.6099, gp: 0.2739\n",
      "Epoch [0/1] Batch 830/1200                       Loss D: -883.2634, loss G: 150.9351, gp: 0.1829\n",
      "Epoch [0/1] Batch 831/1200                       Loss D: -888.1597, loss G: 154.1241, gp: 0.2435\n",
      "Epoch [0/1] Batch 832/1200                       Loss D: -894.9576, loss G: 155.4493, gp: 0.0733\n",
      "Epoch [0/1] Batch 833/1200                       Loss D: -887.5446, loss G: 148.0905, gp: 0.1371\n",
      "Epoch [0/1] Batch 834/1200                       Loss D: -887.8494, loss G: 152.0329, gp: 0.4652\n",
      "Epoch [0/1] Batch 835/1200                       Loss D: -893.6110, loss G: 152.6329, gp: 0.3231\n",
      "Epoch [0/1] Batch 836/1200                       Loss D: -897.7844, loss G: 151.2316, gp: 0.1696\n",
      "Epoch [0/1] Batch 837/1200                       Loss D: -906.2697, loss G: 159.2665, gp: 0.2195\n",
      "Epoch [0/1] Batch 838/1200                       Loss D: -900.3445, loss G: 154.2122, gp: 0.2459\n",
      "Epoch [0/1] Batch 839/1200                       Loss D: -898.8762, loss G: 151.0050, gp: 0.2302\n",
      "Epoch [0/1] Batch 840/1200                       Loss D: -909.9343, loss G: 157.4510, gp: 0.4209\n",
      "Epoch [0/1] Batch 841/1200                       Loss D: -898.5362, loss G: 155.8346, gp: 0.4234\n",
      "Epoch [0/1] Batch 842/1200                       Loss D: -912.5091, loss G: 159.0535, gp: 0.1413\n",
      "Epoch [0/1] Batch 843/1200                       Loss D: -913.0986, loss G: 155.0247, gp: 0.1336\n",
      "Epoch [0/1] Batch 844/1200                       Loss D: -917.9794, loss G: 155.2588, gp: 0.1439\n",
      "Epoch [0/1] Batch 845/1200                       Loss D: -913.5238, loss G: 156.5761, gp: 0.1092\n",
      "Epoch [0/1] Batch 846/1200                       Loss D: -923.8074, loss G: 159.3958, gp: 0.2440\n",
      "Epoch [0/1] Batch 847/1200                       Loss D: -928.8277, loss G: 162.9546, gp: 0.1464\n",
      "Epoch [0/1] Batch 848/1200                       Loss D: -932.7067, loss G: 163.7108, gp: 0.3163\n",
      "Epoch [0/1] Batch 849/1200                       Loss D: -938.1977, loss G: 166.2913, gp: 0.0786\n",
      "Epoch [0/1] Batch 850/1200                       Loss D: -939.6904, loss G: 166.1877, gp: 0.2479\n",
      "Epoch [0/1] Batch 851/1200                       Loss D: -939.0888, loss G: 167.9542, gp: 0.1298\n",
      "Epoch [0/1] Batch 852/1200                       Loss D: -942.4459, loss G: 169.2643, gp: 0.1292\n",
      "Epoch [0/1] Batch 853/1200                       Loss D: -945.7526, loss G: 170.8628, gp: 0.1623\n",
      "Epoch [0/1] Batch 854/1200                       Loss D: -947.5128, loss G: 173.2629, gp: 0.4132\n",
      "Epoch [0/1] Batch 855/1200                       Loss D: -954.0400, loss G: 172.4790, gp: 0.1572\n",
      "Epoch [0/1] Batch 856/1200                       Loss D: -960.9196, loss G: 175.7593, gp: 0.1595\n",
      "Epoch [0/1] Batch 857/1200                       Loss D: -959.8945, loss G: 174.5804, gp: 0.2614\n",
      "Epoch [0/1] Batch 858/1200                       Loss D: -953.1844, loss G: 175.3428, gp: 0.3359\n",
      "Epoch [0/1] Batch 859/1200                       Loss D: -966.0712, loss G: 173.1865, gp: 0.0915\n",
      "Epoch [0/1] Batch 860/1200                       Loss D: -966.1729, loss G: 174.6315, gp: 0.0894\n",
      "Epoch [0/1] Batch 861/1200                       Loss D: -972.3374, loss G: 177.5614, gp: 0.1365\n",
      "Epoch [0/1] Batch 862/1200                       Loss D: -971.0735, loss G: 178.8320, gp: 0.2902\n",
      "Epoch [0/1] Batch 863/1200                       Loss D: -982.9580, loss G: 185.2658, gp: 0.2054\n",
      "Epoch [0/1] Batch 864/1200                       Loss D: -983.9957, loss G: 182.8463, gp: 0.0980\n",
      "Epoch [0/1] Batch 865/1200                       Loss D: -983.4910, loss G: 182.1937, gp: 0.1957\n",
      "Epoch [0/1] Batch 866/1200                       Loss D: -990.6406, loss G: 184.0455, gp: 0.0523\n",
      "Epoch [0/1] Batch 867/1200                       Loss D: -991.8642, loss G: 185.8968, gp: 0.1172\n",
      "Epoch [0/1] Batch 868/1200                       Loss D: -996.2708, loss G: 187.7094, gp: 0.1041\n",
      "Epoch [0/1] Batch 869/1200                       Loss D: -995.6835, loss G: 189.5974, gp: 0.1671\n",
      "Epoch [0/1] Batch 870/1200                       Loss D: -1003.9667, loss G: 192.0365, gp: 0.0807\n",
      "Epoch [0/1] Batch 871/1200                       Loss D: -1006.2224, loss G: 191.9288, gp: 0.1536\n",
      "Epoch [0/1] Batch 872/1200                       Loss D: -1012.6318, loss G: 193.1573, gp: 0.0804\n",
      "Epoch [0/1] Batch 873/1200                       Loss D: -1011.1372, loss G: 192.1729, gp: 0.0998\n",
      "Epoch [0/1] Batch 874/1200                       Loss D: -1017.7792, loss G: 195.9701, gp: 0.1099\n",
      "Epoch [0/1] Batch 875/1200                       Loss D: -1013.2992, loss G: 194.2368, gp: 0.1970\n",
      "Epoch [0/1] Batch 876/1200                       Loss D: -1017.9846, loss G: 195.3149, gp: 0.1374\n",
      "Epoch [0/1] Batch 877/1200                       Loss D: -1021.2904, loss G: 196.4955, gp: 0.1343\n",
      "Epoch [0/1] Batch 878/1200                       Loss D: -1023.3414, loss G: 200.9265, gp: 0.0945\n",
      "Epoch [0/1] Batch 879/1200                       Loss D: -1032.3286, loss G: 199.9619, gp: 0.0637\n",
      "Epoch [0/1] Batch 880/1200                       Loss D: -1038.7148, loss G: 201.5194, gp: 0.0592\n",
      "Epoch [0/1] Batch 881/1200                       Loss D: -1035.5823, loss G: 200.0682, gp: 0.2117\n",
      "Epoch [0/1] Batch 882/1200                       Loss D: -1037.7482, loss G: 202.6911, gp: 0.1120\n",
      "Epoch [0/1] Batch 883/1200                       Loss D: -1039.8275, loss G: 203.5836, gp: 0.0923\n",
      "Epoch [0/1] Batch 884/1200                       Loss D: -1047.5894, loss G: 205.2532, gp: 0.1209\n",
      "Epoch [0/1] Batch 885/1200                       Loss D: -1027.6312, loss G: 187.5543, gp: 0.0432\n",
      "Epoch [0/1] Batch 886/1200                       Loss D: -1014.2203, loss G: 164.8381, gp: 0.0648\n",
      "Epoch [0/1] Batch 887/1200                       Loss D: -1006.5378, loss G: 157.1039, gp: 0.0814\n",
      "Epoch [0/1] Batch 888/1200                       Loss D: -1020.0573, loss G: 168.0552, gp: 0.0648\n",
      "Epoch [0/1] Batch 889/1200                       Loss D: -1025.0929, loss G: 171.2202, gp: 0.0987\n",
      "Epoch [0/1] Batch 890/1200                       Loss D: -1020.7627, loss G: 172.6628, gp: 0.3519\n",
      "Epoch [0/1] Batch 891/1200                       Loss D: -1035.3496, loss G: 174.3515, gp: 0.1062\n",
      "Epoch [0/1] Batch 892/1200                       Loss D: -1028.7314, loss G: 172.9813, gp: 0.3150\n",
      "Epoch [0/1] Batch 893/1200                       Loss D: -1043.4034, loss G: 183.5749, gp: 0.2078\n",
      "Epoch [0/1] Batch 894/1200                       Loss D: -1036.3146, loss G: 176.3000, gp: 0.2639\n",
      "Epoch [0/1] Batch 895/1200                       Loss D: -1040.0100, loss G: 178.2617, gp: 0.3546\n",
      "Epoch [0/1] Batch 896/1200                       Loss D: -1050.2495, loss G: 188.3589, gp: 0.2776\n",
      "Epoch [0/1] Batch 897/1200                       Loss D: -1043.4750, loss G: 178.9181, gp: 0.3389\n",
      "Epoch [0/1] Batch 898/1200                       Loss D: -1040.5581, loss G: 176.0092, gp: 0.1574\n",
      "Epoch [0/1] Batch 899/1200                       Loss D: -1056.0367, loss G: 181.3778, gp: 0.1131\n",
      "Epoch [0/1] Batch 900/1200                       Loss D: -1060.5131, loss G: 187.0651, gp: 0.1730\n",
      "Epoch [0/1] Batch 901/1200                       Loss D: -1061.6866, loss G: 188.1346, gp: 0.2091\n",
      "Epoch [0/1] Batch 902/1200                       Loss D: -1065.3862, loss G: 188.9935, gp: 0.1446\n",
      "Epoch [0/1] Batch 903/1200                       Loss D: -1056.4690, loss G: 176.9042, gp: 0.2728\n",
      "Epoch [0/1] Batch 904/1200                       Loss D: -1071.0836, loss G: 192.6115, gp: 0.2812\n",
      "Epoch [0/1] Batch 905/1200                       Loss D: -1073.7958, loss G: 194.0915, gp: 0.1107\n",
      "Epoch [0/1] Batch 906/1200                       Loss D: -1082.4004, loss G: 198.3765, gp: 0.3376\n",
      "Epoch [0/1] Batch 907/1200                       Loss D: -1083.8418, loss G: 196.1395, gp: 0.1106\n",
      "Epoch [0/1] Batch 908/1200                       Loss D: -1080.6239, loss G: 193.7269, gp: 0.1991\n",
      "Epoch [0/1] Batch 909/1200                       Loss D: -1086.2064, loss G: 194.6029, gp: 0.1877\n",
      "Epoch [0/1] Batch 910/1200                       Loss D: -1094.6799, loss G: 198.9857, gp: 0.0364\n",
      "Epoch [0/1] Batch 911/1200                       Loss D: -1109.3501, loss G: 215.5517, gp: 0.2698\n",
      "Epoch [0/1] Batch 912/1200                       Loss D: -1094.0404, loss G: 200.4945, gp: 0.1001\n",
      "Epoch [0/1] Batch 913/1200                       Loss D: -1111.7886, loss G: 215.6549, gp: 0.1251\n",
      "Epoch [0/1] Batch 914/1200                       Loss D: -1110.6980, loss G: 212.8553, gp: 0.2316\n",
      "Epoch [0/1] Batch 915/1200                       Loss D: -1116.8564, loss G: 210.0289, gp: 0.0739\n",
      "Epoch [0/1] Batch 916/1200                       Loss D: -1122.5422, loss G: 213.7894, gp: 0.0660\n",
      "Epoch [0/1] Batch 917/1200                       Loss D: -1103.4612, loss G: 199.7216, gp: 0.1863\n",
      "Epoch [0/1] Batch 918/1200                       Loss D: -1128.1659, loss G: 218.1796, gp: 0.0626\n",
      "Epoch [0/1] Batch 919/1200                       Loss D: -1118.8192, loss G: 213.2878, gp: 0.1300\n",
      "Epoch [0/1] Batch 920/1200                       Loss D: -1116.6934, loss G: 204.8281, gp: 0.1102\n",
      "Epoch [0/1] Batch 921/1200                       Loss D: -1133.8972, loss G: 217.4950, gp: 0.0825\n",
      "Epoch [0/1] Batch 922/1200                       Loss D: -1136.4779, loss G: 217.2984, gp: 0.1121\n",
      "Epoch [0/1] Batch 923/1200                       Loss D: -1139.8171, loss G: 218.2458, gp: 0.0891\n",
      "Epoch [0/1] Batch 924/1200                       Loss D: -1147.9673, loss G: 228.1131, gp: 0.1205\n",
      "Epoch [0/1] Batch 925/1200                       Loss D: -1130.9202, loss G: 206.1812, gp: 0.1644\n",
      "Epoch [0/1] Batch 926/1200                       Loss D: -1149.0828, loss G: 222.2718, gp: 0.0351\n",
      "Epoch [0/1] Batch 927/1200                       Loss D: -1151.1100, loss G: 226.2677, gp: 0.0738\n",
      "Epoch [0/1] Batch 928/1200                       Loss D: -1154.8929, loss G: 227.2784, gp: 0.1215\n",
      "Epoch [0/1] Batch 929/1200                       Loss D: -1157.7280, loss G: 228.2265, gp: 0.1524\n",
      "Epoch [0/1] Batch 930/1200                       Loss D: -1163.8135, loss G: 229.7844, gp: 0.0738\n",
      "Epoch [0/1] Batch 931/1200                       Loss D: -1166.6615, loss G: 231.5607, gp: 0.0315\n",
      "Epoch [0/1] Batch 932/1200                       Loss D: -1177.7931, loss G: 238.5470, gp: 0.0925\n",
      "Epoch [0/1] Batch 933/1200                       Loss D: -1179.1606, loss G: 234.6111, gp: 0.0822\n",
      "Epoch [0/1] Batch 934/1200                       Loss D: -1181.6681, loss G: 237.8622, gp: 0.0386\n",
      "Epoch [0/1] Batch 935/1200                       Loss D: -1181.7095, loss G: 238.2490, gp: 0.0925\n",
      "Epoch [0/1] Batch 936/1200                       Loss D: -1185.3706, loss G: 240.4735, gp: 0.0930\n",
      "Epoch [0/1] Batch 937/1200                       Loss D: -1194.2076, loss G: 242.2966, gp: 0.0505\n",
      "Epoch [0/1] Batch 938/1200                       Loss D: -1198.4086, loss G: 245.2589, gp: 0.2644\n",
      "Epoch [0/1] Batch 939/1200                       Loss D: -1195.2957, loss G: 244.5043, gp: 0.2294\n",
      "Epoch [0/1] Batch 940/1200                       Loss D: -1204.9952, loss G: 246.0334, gp: 0.0765\n",
      "Epoch [0/1] Batch 941/1200                       Loss D: -1206.6445, loss G: 250.6888, gp: 0.0928\n",
      "Epoch [0/1] Batch 942/1200                       Loss D: -1211.1996, loss G: 248.4603, gp: 0.0734\n",
      "Epoch [0/1] Batch 943/1200                       Loss D: -1208.0355, loss G: 248.6262, gp: 0.1018\n",
      "Epoch [0/1] Batch 944/1200                       Loss D: -1212.1910, loss G: 253.6330, gp: 0.1204\n",
      "Epoch [0/1] Batch 945/1200                       Loss D: -1219.2482, loss G: 253.4186, gp: 0.0578\n",
      "Epoch [0/1] Batch 946/1200                       Loss D: -1222.5729, loss G: 254.9501, gp: 0.4314\n",
      "Epoch [0/1] Batch 947/1200                       Loss D: -1224.4420, loss G: 254.7567, gp: 0.0295\n",
      "Epoch [0/1] Batch 948/1200                       Loss D: -1231.6567, loss G: 256.8134, gp: 0.0385\n",
      "Epoch [0/1] Batch 949/1200                       Loss D: -1229.7208, loss G: 256.5053, gp: 0.0737\n",
      "Epoch [0/1] Batch 950/1200                       Loss D: -1238.4769, loss G: 262.4883, gp: 0.0709\n",
      "Epoch [0/1] Batch 951/1200                       Loss D: -1240.0238, loss G: 259.8507, gp: 0.0814\n",
      "Epoch [0/1] Batch 952/1200                       Loss D: -1238.5022, loss G: 260.1605, gp: 0.0543\n",
      "Epoch [0/1] Batch 953/1200                       Loss D: -1247.9749, loss G: 265.7609, gp: 0.0730\n",
      "Epoch [0/1] Batch 954/1200                       Loss D: -1249.8623, loss G: 266.4872, gp: 0.0518\n",
      "Epoch [0/1] Batch 955/1200                       Loss D: -1238.4352, loss G: 263.1022, gp: 0.4952\n",
      "Epoch [0/1] Batch 956/1200                       Loss D: -1249.4718, loss G: 263.5428, gp: 0.2854\n",
      "Epoch [0/1] Batch 957/1200                       Loss D: -1252.1078, loss G: 263.3087, gp: 0.0997\n",
      "Epoch [0/1] Batch 958/1200                       Loss D: -1253.8224, loss G: 263.9405, gp: 0.0360\n",
      "Epoch [0/1] Batch 959/1200                       Loss D: -1259.5580, loss G: 265.5573, gp: 0.0539\n",
      "Epoch [0/1] Batch 960/1200                       Loss D: -1261.4740, loss G: 267.2902, gp: 0.0437\n",
      "Epoch [0/1] Batch 961/1200                       Loss D: -1269.4873, loss G: 268.7688, gp: 0.0591\n",
      "Epoch [0/1] Batch 962/1200                       Loss D: -1269.0399, loss G: 268.3659, gp: 0.0613\n",
      "Epoch [0/1] Batch 963/1200                       Loss D: -1270.9756, loss G: 273.2801, gp: 0.0645\n",
      "Epoch [0/1] Batch 964/1200                       Loss D: -1277.1372, loss G: 272.4722, gp: 0.2093\n",
      "Epoch [0/1] Batch 965/1200                       Loss D: -1278.4359, loss G: 272.1000, gp: 0.1239\n",
      "Epoch [0/1] Batch 966/1200                       Loss D: -1281.4844, loss G: 274.3823, gp: 0.0836\n",
      "Epoch [0/1] Batch 967/1200                       Loss D: -1283.7833, loss G: 273.4873, gp: 0.2031\n",
      "Epoch [0/1] Batch 968/1200                       Loss D: -1288.8380, loss G: 274.4910, gp: 0.0855\n",
      "Epoch [0/1] Batch 969/1200                       Loss D: -1293.5559, loss G: 276.5295, gp: 0.0855\n",
      "Epoch [0/1] Batch 970/1200                       Loss D: -1293.2025, loss G: 276.9294, gp: 0.0870\n",
      "Epoch [0/1] Batch 971/1200                       Loss D: -1296.1150, loss G: 276.6891, gp: 0.1152\n",
      "Epoch [0/1] Batch 972/1200                       Loss D: -1301.3167, loss G: 278.2069, gp: 0.0805\n",
      "Epoch [0/1] Batch 973/1200                       Loss D: -1301.8549, loss G: 277.0156, gp: 0.0997\n",
      "Epoch [0/1] Batch 974/1200                       Loss D: -1305.9261, loss G: 282.4232, gp: 0.0928\n",
      "Epoch [0/1] Batch 975/1200                       Loss D: -1297.4540, loss G: 280.3322, gp: 0.2851\n",
      "Epoch [0/1] Batch 976/1200                       Loss D: -1307.0057, loss G: 278.4318, gp: 0.1170\n",
      "Epoch [0/1] Batch 977/1200                       Loss D: -1311.0872, loss G: 281.2494, gp: 0.1129\n",
      "Epoch [0/1] Batch 978/1200                       Loss D: -1312.3295, loss G: 280.7260, gp: 0.0832\n",
      "Epoch [0/1] Batch 979/1200                       Loss D: -1315.6731, loss G: 280.0391, gp: 0.0419\n",
      "Epoch [0/1] Batch 980/1200                       Loss D: -1321.2751, loss G: 281.2101, gp: 0.0432\n",
      "Epoch [0/1] Batch 981/1200                       Loss D: -1324.8066, loss G: 282.1977, gp: 0.0468\n",
      "Epoch [0/1] Batch 982/1200                       Loss D: -1322.7139, loss G: 281.2164, gp: 0.0468\n",
      "Epoch [0/1] Batch 983/1200                       Loss D: -1328.8700, loss G: 285.5885, gp: 0.0951\n",
      "Epoch [0/1] Batch 984/1200                       Loss D: -1326.8883, loss G: 284.2885, gp: 0.1300\n",
      "Epoch [0/1] Batch 985/1200                       Loss D: -1338.9624, loss G: 286.5738, gp: 0.1000\n",
      "Epoch [0/1] Batch 986/1200                       Loss D: -1337.8066, loss G: 286.5303, gp: 0.0663\n",
      "Epoch [0/1] Batch 987/1200                       Loss D: -1338.3531, loss G: 286.8125, gp: 0.0713\n",
      "Epoch [0/1] Batch 988/1200                       Loss D: -1342.4813, loss G: 288.0937, gp: 0.0402\n",
      "Epoch [0/1] Batch 989/1200                       Loss D: -1343.4015, loss G: 286.9345, gp: 0.0551\n",
      "Epoch [0/1] Batch 990/1200                       Loss D: -1349.2228, loss G: 289.3906, gp: 0.0545\n",
      "Epoch [0/1] Batch 991/1200                       Loss D: -1350.2200, loss G: 289.6017, gp: 0.1632\n",
      "Epoch [0/1] Batch 992/1200                       Loss D: -1357.4623, loss G: 293.3451, gp: 0.0547\n",
      "Epoch [0/1] Batch 993/1200                       Loss D: -1352.5604, loss G: 290.3073, gp: 0.0626\n",
      "Epoch [0/1] Batch 994/1200                       Loss D: -1360.1086, loss G: 294.4582, gp: 0.0510\n",
      "Epoch [0/1] Batch 995/1200                       Loss D: -1357.1873, loss G: 291.3596, gp: 0.0885\n",
      "Epoch [0/1] Batch 996/1200                       Loss D: -1370.8031, loss G: 298.6270, gp: 0.0335\n",
      "Epoch [0/1] Batch 997/1200                       Loss D: -1368.9430, loss G: 296.3485, gp: 0.0344\n",
      "Epoch [0/1] Batch 998/1200                       Loss D: -1370.8982, loss G: 295.2874, gp: 0.0292\n",
      "Epoch [0/1] Batch 999/1200                       Loss D: -1375.2169, loss G: 295.2545, gp: 0.0316\n",
      "Epoch [0/1] Batch 1000/1200                       Loss D: -1382.7014, loss G: 301.0981, gp: 0.1219\n",
      "Epoch [0/1] Batch 1001/1200                       Loss D: -1374.6919, loss G: 300.6750, gp: 0.1935\n",
      "Epoch [0/1] Batch 1002/1200                       Loss D: -1383.2072, loss G: 298.4620, gp: 0.0605\n",
      "Epoch [0/1] Batch 1003/1200                       Loss D: -1382.1841, loss G: 297.5941, gp: 0.0338\n",
      "Epoch [0/1] Batch 1004/1200                       Loss D: -1390.8401, loss G: 304.1189, gp: 0.0324\n",
      "Epoch [0/1] Batch 1005/1200                       Loss D: -1393.8685, loss G: 302.2158, gp: 0.1179\n",
      "Epoch [0/1] Batch 1006/1200                       Loss D: -1392.6100, loss G: 302.5986, gp: 0.0594\n",
      "Epoch [0/1] Batch 1007/1200                       Loss D: -1398.3978, loss G: 300.7244, gp: 0.0411\n",
      "Epoch [0/1] Batch 1008/1200                       Loss D: -1396.2302, loss G: 299.4263, gp: 0.0300\n",
      "Epoch [0/1] Batch 1009/1200                       Loss D: -1400.2363, loss G: 300.6034, gp: 0.0406\n",
      "Epoch [0/1] Batch 1010/1200                       Loss D: -1402.4392, loss G: 303.4489, gp: 0.1148\n",
      "Epoch [0/1] Batch 1011/1200                       Loss D: -1410.3082, loss G: 307.1585, gp: 0.0516\n",
      "Epoch [0/1] Batch 1012/1200                       Loss D: -1412.2010, loss G: 305.6243, gp: 0.0695\n",
      "Epoch [0/1] Batch 1013/1200                       Loss D: -1413.2809, loss G: 303.8793, gp: 0.0490\n",
      "Epoch [0/1] Batch 1014/1200                       Loss D: -1411.4873, loss G: 304.9236, gp: 0.0821\n",
      "Epoch [0/1] Batch 1015/1200                       Loss D: -1414.1689, loss G: 303.7655, gp: 0.0849\n",
      "Epoch [0/1] Batch 1016/1200                       Loss D: -1419.4440, loss G: 304.6425, gp: 0.0540\n",
      "Epoch [0/1] Batch 1017/1200                       Loss D: -1415.8965, loss G: 307.7689, gp: 0.0930\n",
      "Epoch [0/1] Batch 1018/1200                       Loss D: -1425.0773, loss G: 310.7155, gp: 0.0559\n",
      "Epoch [0/1] Batch 1019/1200                       Loss D: -1432.5342, loss G: 311.2781, gp: 0.0587\n",
      "Epoch [0/1] Batch 1020/1200                       Loss D: -1435.3967, loss G: 309.0031, gp: 0.0538\n",
      "Epoch [0/1] Batch 1021/1200                       Loss D: -1432.4276, loss G: 307.8525, gp: 0.0428\n",
      "Epoch [0/1] Batch 1022/1200                       Loss D: -1440.0571, loss G: 316.3102, gp: 0.0382\n",
      "Epoch [0/1] Batch 1023/1200                       Loss D: -1440.3516, loss G: 313.8756, gp: 0.0899\n",
      "Epoch [0/1] Batch 1024/1200                       Loss D: -1439.9774, loss G: 311.8714, gp: 0.0857\n",
      "Epoch [0/1] Batch 1025/1200                       Loss D: -1444.5150, loss G: 312.7648, gp: 0.0517\n",
      "Epoch [0/1] Batch 1026/1200                       Loss D: -1448.4648, loss G: 312.8675, gp: 0.0410\n",
      "Epoch [0/1] Batch 1027/1200                       Loss D: -1447.3627, loss G: 314.8740, gp: 0.0646\n",
      "Epoch [0/1] Batch 1028/1200                       Loss D: -1454.0552, loss G: 317.5142, gp: 0.0590\n",
      "Epoch [0/1] Batch 1029/1200                       Loss D: -1458.8041, loss G: 318.1843, gp: 0.0797\n",
      "Epoch [0/1] Batch 1030/1200                       Loss D: -1462.2828, loss G: 323.2609, gp: 0.0833\n",
      "Epoch [0/1] Batch 1031/1200                       Loss D: -1466.0103, loss G: 322.7768, gp: 0.0298\n",
      "Epoch [0/1] Batch 1032/1200                       Loss D: -1468.7209, loss G: 322.7350, gp: 0.0620\n",
      "Epoch [0/1] Batch 1033/1200                       Loss D: -1468.4105, loss G: 319.9854, gp: 0.0401\n",
      "Epoch [0/1] Batch 1034/1200                       Loss D: -1472.1809, loss G: 324.0659, gp: 0.0681\n",
      "Epoch [0/1] Batch 1035/1200                       Loss D: -1486.2500, loss G: 329.5356, gp: 0.0172\n",
      "Epoch [0/1] Batch 1036/1200                       Loss D: -1484.6687, loss G: 328.1000, gp: 0.0510\n",
      "Epoch [0/1] Batch 1037/1200                       Loss D: -1486.0670, loss G: 328.6138, gp: 0.0292\n",
      "Epoch [0/1] Batch 1038/1200                       Loss D: -1487.4171, loss G: 328.9113, gp: 0.0323\n",
      "Epoch [0/1] Batch 1039/1200                       Loss D: -1490.6219, loss G: 334.3985, gp: 0.1085\n",
      "Epoch [0/1] Batch 1040/1200                       Loss D: -1492.7262, loss G: 330.5131, gp: 0.0599\n",
      "Epoch [0/1] Batch 1041/1200                       Loss D: -1497.4087, loss G: 331.7491, gp: 0.0356\n",
      "Epoch [0/1] Batch 1042/1200                       Loss D: -1498.8704, loss G: 328.8972, gp: 0.0582\n",
      "Epoch [0/1] Batch 1043/1200                       Loss D: -1501.6704, loss G: 334.3660, gp: 0.0710\n",
      "Epoch [0/1] Batch 1044/1200                       Loss D: -1509.3683, loss G: 333.3915, gp: 0.0358\n",
      "Epoch [0/1] Batch 1045/1200                       Loss D: -1504.8497, loss G: 334.2749, gp: 0.2200\n",
      "Epoch [0/1] Batch 1046/1200                       Loss D: -1504.9963, loss G: 333.3091, gp: 0.0361\n",
      "Epoch [0/1] Batch 1047/1200                       Loss D: -1518.0513, loss G: 342.3967, gp: 0.0377\n",
      "Epoch [0/1] Batch 1048/1200                       Loss D: -1519.7379, loss G: 341.6684, gp: 0.0982\n",
      "Epoch [0/1] Batch 1049/1200                       Loss D: -1521.1848, loss G: 341.4958, gp: 0.0541\n",
      "Epoch [0/1] Batch 1050/1200                       Loss D: -1525.7332, loss G: 340.1723, gp: 0.0505\n",
      "Epoch [0/1] Batch 1051/1200                       Loss D: -1518.3822, loss G: 334.5467, gp: 0.0925\n",
      "Epoch [0/1] Batch 1052/1200                       Loss D: -1527.3312, loss G: 339.5265, gp: 0.0225\n",
      "Epoch [0/1] Batch 1053/1200                       Loss D: -1529.9316, loss G: 338.2298, gp: 0.1206\n",
      "Epoch [0/1] Batch 1054/1200                       Loss D: -1528.9091, loss G: 341.4269, gp: 0.0693\n",
      "Epoch [0/1] Batch 1055/1200                       Loss D: -1528.3724, loss G: 340.9199, gp: 0.3277\n",
      "Epoch [0/1] Batch 1056/1200                       Loss D: -1537.8698, loss G: 342.4564, gp: 0.0314\n",
      "Epoch [0/1] Batch 1057/1200                       Loss D: -1540.5332, loss G: 341.6294, gp: 0.0377\n",
      "Epoch [0/1] Batch 1058/1200                       Loss D: -1549.1605, loss G: 345.1398, gp: 0.0662\n",
      "Epoch [0/1] Batch 1059/1200                       Loss D: -1551.9430, loss G: 350.0133, gp: 0.0524\n",
      "Epoch [0/1] Batch 1060/1200                       Loss D: -1557.7595, loss G: 350.6320, gp: 0.0275\n",
      "Epoch [0/1] Batch 1061/1200                       Loss D: -1554.8578, loss G: 351.9044, gp: 0.0391\n",
      "Epoch [0/1] Batch 1062/1200                       Loss D: -1561.8594, loss G: 352.7722, gp: 0.0542\n",
      "Epoch [0/1] Batch 1063/1200                       Loss D: -1571.0646, loss G: 353.8977, gp: 0.0753\n",
      "Epoch [0/1] Batch 1064/1200                       Loss D: -1574.4875, loss G: 358.1719, gp: 0.0188\n",
      "Epoch [0/1] Batch 1065/1200                       Loss D: -1568.1991, loss G: 353.5725, gp: 0.1182\n",
      "Epoch [0/1] Batch 1066/1200                       Loss D: -1578.8352, loss G: 357.2122, gp: 0.0364\n",
      "Epoch [0/1] Batch 1067/1200                       Loss D: -1574.6790, loss G: 355.8712, gp: 0.0277\n",
      "Epoch [0/1] Batch 1068/1200                       Loss D: -1576.7618, loss G: 358.8077, gp: 0.1099\n",
      "Epoch [0/1] Batch 1069/1200                       Loss D: -1586.9210, loss G: 361.8351, gp: 0.0462\n",
      "Epoch [0/1] Batch 1070/1200                       Loss D: -1589.1383, loss G: 360.0957, gp: 0.0483\n",
      "Epoch [0/1] Batch 1071/1200                       Loss D: -1590.1692, loss G: 361.4727, gp: 0.0522\n",
      "Epoch [0/1] Batch 1072/1200                       Loss D: -1592.8237, loss G: 364.9514, gp: 0.2672\n",
      "Epoch [0/1] Batch 1073/1200                       Loss D: -1600.6526, loss G: 365.0808, gp: 0.0691\n",
      "Epoch [0/1] Batch 1074/1200                       Loss D: -1602.2257, loss G: 365.3169, gp: 0.0365\n",
      "Epoch [0/1] Batch 1075/1200                       Loss D: -1605.1283, loss G: 367.4405, gp: 0.0444\n",
      "Epoch [0/1] Batch 1076/1200                       Loss D: -1604.8164, loss G: 365.1071, gp: 0.0256\n",
      "Epoch [0/1] Batch 1077/1200                       Loss D: -1609.3893, loss G: 365.9717, gp: 0.0490\n",
      "Epoch [0/1] Batch 1078/1200                       Loss D: -1612.1610, loss G: 366.0900, gp: 0.0252\n",
      "Epoch [0/1] Batch 1079/1200                       Loss D: -1611.2078, loss G: 366.4478, gp: 0.0360\n",
      "Epoch [0/1] Batch 1080/1200                       Loss D: -1618.7017, loss G: 368.9322, gp: 0.0294\n",
      "Epoch [0/1] Batch 1081/1200                       Loss D: -1623.3831, loss G: 369.8486, gp: 0.0267\n",
      "Epoch [0/1] Batch 1082/1200                       Loss D: -1621.8099, loss G: 368.6526, gp: 0.0411\n",
      "Epoch [0/1] Batch 1083/1200                       Loss D: -1623.0835, loss G: 369.0293, gp: 0.0625\n",
      "Epoch [0/1] Batch 1084/1200                       Loss D: -1624.8898, loss G: 368.0937, gp: 0.0233\n",
      "Epoch [0/1] Batch 1085/1200                       Loss D: -1627.1503, loss G: 368.5039, gp: 0.0207\n",
      "Epoch [0/1] Batch 1086/1200                       Loss D: -1627.5219, loss G: 366.1063, gp: 0.0333\n",
      "Epoch [0/1] Batch 1087/1200                       Loss D: -1633.4775, loss G: 370.0835, gp: 0.0235\n",
      "Epoch [0/1] Batch 1088/1200                       Loss D: -1631.8986, loss G: 368.1283, gp: 0.0396\n",
      "Epoch [0/1] Batch 1089/1200                       Loss D: -1636.5291, loss G: 365.4790, gp: 0.0298\n",
      "Epoch [0/1] Batch 1090/1200                       Loss D: -1633.6801, loss G: 362.7916, gp: 0.0282\n",
      "Epoch [0/1] Batch 1091/1200                       Loss D: -1642.6370, loss G: 367.4769, gp: 0.0198\n",
      "Epoch [0/1] Batch 1092/1200                       Loss D: -1636.4464, loss G: 367.2011, gp: 0.0202\n",
      "Epoch [0/1] Batch 1093/1200                       Loss D: -1652.6614, loss G: 371.7873, gp: 0.0528\n",
      "Epoch [0/1] Batch 1094/1200                       Loss D: -1648.9539, loss G: 372.4373, gp: 0.0675\n",
      "Epoch [0/1] Batch 1095/1200                       Loss D: -1651.6107, loss G: 374.1122, gp: 0.0302\n",
      "Epoch [0/1] Batch 1096/1200                       Loss D: -1656.1218, loss G: 374.6573, gp: 0.0202\n",
      "Epoch [0/1] Batch 1097/1200                       Loss D: -1651.3678, loss G: 369.6104, gp: 0.0732\n",
      "Epoch [0/1] Batch 1098/1200                       Loss D: -1658.1146, loss G: 374.0202, gp: 0.1402\n",
      "Epoch [0/1] Batch 1099/1200                       Loss D: -1663.3877, loss G: 372.9213, gp: 0.1761\n",
      "Epoch [0/1] Batch 1100/1200                       Loss D: -1665.2816, loss G: 373.4547, gp: 0.0454\n",
      "Epoch [0/1] Batch 1101/1200                       Loss D: -1664.0392, loss G: 373.8036, gp: 0.2848\n",
      "Epoch [0/1] Batch 1102/1200                       Loss D: -1672.1035, loss G: 374.7209, gp: 0.0261\n",
      "Epoch [0/1] Batch 1103/1200                       Loss D: -1671.6842, loss G: 375.6987, gp: 0.0888\n",
      "Epoch [0/1] Batch 1104/1200                       Loss D: -1678.4486, loss G: 375.5793, gp: 0.0315\n",
      "Epoch [0/1] Batch 1105/1200                       Loss D: -1687.4399, loss G: 383.8077, gp: 0.0360\n",
      "Epoch [0/1] Batch 1106/1200                       Loss D: -1688.9578, loss G: 388.8875, gp: 0.0938\n",
      "Epoch [0/1] Batch 1107/1200                       Loss D: -1692.6594, loss G: 381.3726, gp: 0.0215\n",
      "Epoch [0/1] Batch 1108/1200                       Loss D: -1689.4204, loss G: 381.0331, gp: 0.0498\n",
      "Epoch [0/1] Batch 1109/1200                       Loss D: -1692.7637, loss G: 382.1173, gp: 0.0268\n",
      "Epoch [0/1] Batch 1110/1200                       Loss D: -1697.6500, loss G: 386.9603, gp: 0.1218\n",
      "Epoch [0/1] Batch 1111/1200                       Loss D: -1709.5927, loss G: 395.7703, gp: 0.0382\n",
      "Epoch [0/1] Batch 1112/1200                       Loss D: -1712.2302, loss G: 392.6180, gp: 0.0127\n",
      "Epoch [0/1] Batch 1113/1200                       Loss D: -1701.2399, loss G: 387.4666, gp: 0.1518\n",
      "Epoch [0/1] Batch 1114/1200                       Loss D: -1716.0018, loss G: 392.6195, gp: 0.0952\n",
      "Epoch [0/1] Batch 1115/1200                       Loss D: -1718.9473, loss G: 395.1036, gp: 0.0882\n",
      "Epoch [0/1] Batch 1116/1200                       Loss D: -1726.7079, loss G: 400.7715, gp: 0.0165\n",
      "Epoch [0/1] Batch 1117/1200                       Loss D: -1728.1322, loss G: 401.0126, gp: 0.0648\n",
      "Epoch [0/1] Batch 1118/1200                       Loss D: -1732.3540, loss G: 402.4250, gp: 0.0351\n",
      "Epoch [0/1] Batch 1119/1200                       Loss D: -1729.2827, loss G: 399.9013, gp: 0.0910\n",
      "Epoch [0/1] Batch 1120/1200                       Loss D: -1729.2565, loss G: 396.8615, gp: 0.0740\n",
      "Epoch [0/1] Batch 1121/1200                       Loss D: -1736.7805, loss G: 399.3672, gp: 0.0354\n",
      "Epoch [0/1] Batch 1122/1200                       Loss D: -1743.5812, loss G: 401.8835, gp: 0.0162\n",
      "Epoch [0/1] Batch 1123/1200                       Loss D: -1740.0659, loss G: 398.0144, gp: 0.0915\n",
      "Epoch [0/1] Batch 1124/1200                       Loss D: -1742.5156, loss G: 401.2105, gp: 0.0741\n",
      "Epoch [0/1] Batch 1125/1200                       Loss D: -1745.9923, loss G: 401.8185, gp: 0.0174\n",
      "Epoch [0/1] Batch 1126/1200                       Loss D: -1754.4827, loss G: 406.2529, gp: 0.0505\n",
      "Epoch [0/1] Batch 1127/1200                       Loss D: -1757.2911, loss G: 403.2206, gp: 0.0255\n",
      "Epoch [0/1] Batch 1128/1200                       Loss D: -1766.0409, loss G: 409.4098, gp: 0.0146\n",
      "Epoch [0/1] Batch 1129/1200                       Loss D: -1760.3915, loss G: 405.0701, gp: 0.0194\n",
      "Epoch [0/1] Batch 1130/1200                       Loss D: -1764.5734, loss G: 404.5091, gp: 0.0311\n",
      "Epoch [0/1] Batch 1131/1200                       Loss D: -1763.5690, loss G: 405.2851, gp: 0.0378\n",
      "Epoch [0/1] Batch 1132/1200                       Loss D: -1764.3768, loss G: 408.4068, gp: 0.1512\n",
      "Epoch [0/1] Batch 1133/1200                       Loss D: -1764.4443, loss G: 400.0946, gp: 0.0497\n",
      "Epoch [0/1] Batch 1134/1200                       Loss D: -1770.4469, loss G: 403.5961, gp: 0.0501\n",
      "Epoch [0/1] Batch 1135/1200                       Loss D: -1771.4875, loss G: 403.0844, gp: 0.0974\n",
      "Epoch [0/1] Batch 1136/1200                       Loss D: -1772.9556, loss G: 406.3351, gp: 0.0313\n",
      "Epoch [0/1] Batch 1137/1200                       Loss D: -1776.3438, loss G: 406.4110, gp: 0.0624\n",
      "Epoch [0/1] Batch 1138/1200                       Loss D: -1782.4308, loss G: 407.2324, gp: 0.0259\n",
      "Epoch [0/1] Batch 1139/1200                       Loss D: -1787.6492, loss G: 409.5843, gp: 0.0591\n",
      "Epoch [0/1] Batch 1140/1200                       Loss D: -1789.1183, loss G: 412.0467, gp: 0.0450\n",
      "Epoch [0/1] Batch 1141/1200                       Loss D: -1788.0728, loss G: 410.2086, gp: 0.0721\n",
      "Epoch [0/1] Batch 1142/1200                       Loss D: -1792.9572, loss G: 413.9113, gp: 0.0114\n",
      "Epoch [0/1] Batch 1143/1200                       Loss D: -1796.9565, loss G: 406.8019, gp: 0.0354\n",
      "Epoch [0/1] Batch 1144/1200                       Loss D: -1796.1150, loss G: 408.8042, gp: 0.0403\n",
      "Epoch [0/1] Batch 1145/1200                       Loss D: -1803.7535, loss G: 417.6516, gp: 0.1551\n",
      "Epoch [0/1] Batch 1146/1200                       Loss D: -1801.9521, loss G: 413.2364, gp: 0.1018\n",
      "Epoch [0/1] Batch 1147/1200                       Loss D: -1816.1959, loss G: 421.5306, gp: 0.0297\n",
      "Epoch [0/1] Batch 1148/1200                       Loss D: -1819.3357, loss G: 419.7177, gp: 0.0442\n",
      "Epoch [0/1] Batch 1149/1200                       Loss D: -1816.0739, loss G: 418.0957, gp: 0.0861\n",
      "Epoch [0/1] Batch 1150/1200                       Loss D: -1823.6317, loss G: 419.4327, gp: 0.0528\n",
      "Epoch [0/1] Batch 1151/1200                       Loss D: -1825.8616, loss G: 420.7672, gp: 0.0322\n",
      "Epoch [0/1] Batch 1152/1200                       Loss D: -1818.8685, loss G: 425.8260, gp: 0.6709\n",
      "Epoch [0/1] Batch 1153/1200                       Loss D: -1820.3934, loss G: 418.9380, gp: 0.0840\n",
      "Epoch [0/1] Batch 1154/1200                       Loss D: -1832.5339, loss G: 423.8610, gp: 0.0454\n",
      "Epoch [0/1] Batch 1155/1200                       Loss D: -1832.9277, loss G: 417.3245, gp: 0.0409\n",
      "Epoch [0/1] Batch 1156/1200                       Loss D: -1837.3181, loss G: 422.9556, gp: 0.0986\n",
      "Epoch [0/1] Batch 1157/1200                       Loss D: -1842.2756, loss G: 423.6896, gp: 0.0259\n",
      "Epoch [0/1] Batch 1158/1200                       Loss D: -1837.9797, loss G: 422.9072, gp: 0.1405\n",
      "Epoch [0/1] Batch 1159/1200                       Loss D: -1839.8713, loss G: 423.6412, gp: 0.1173\n",
      "Epoch [0/1] Batch 1160/1200                       Loss D: -1849.5903, loss G: 425.0698, gp: 0.0248\n",
      "Epoch [0/1] Batch 1161/1200                       Loss D: -1848.0316, loss G: 426.0234, gp: 0.0347\n",
      "Epoch [0/1] Batch 1162/1200                       Loss D: -1857.4044, loss G: 426.4919, gp: 0.0165\n",
      "Epoch [0/1] Batch 1163/1200                       Loss D: -1856.5344, loss G: 426.9132, gp: 0.0400\n",
      "Epoch [0/1] Batch 1164/1200                       Loss D: -1854.5293, loss G: 427.7973, gp: 0.0721\n",
      "Epoch [0/1] Batch 1165/1200                       Loss D: -1864.0378, loss G: 432.9388, gp: 0.0738\n",
      "Epoch [0/1] Batch 1166/1200                       Loss D: -1860.9512, loss G: 429.9773, gp: 0.0789\n",
      "Epoch [0/1] Batch 1167/1200                       Loss D: -1871.3104, loss G: 434.2939, gp: 0.0321\n",
      "Epoch [0/1] Batch 1168/1200                       Loss D: -1875.6234, loss G: 436.6178, gp: 0.0446\n",
      "Epoch [0/1] Batch 1169/1200                       Loss D: -1871.1227, loss G: 428.7677, gp: 0.1652\n",
      "Epoch [0/1] Batch 1170/1200                       Loss D: -1884.8547, loss G: 439.0713, gp: 0.0459\n",
      "Epoch [0/1] Batch 1171/1200                       Loss D: -1890.8820, loss G: 441.1792, gp: 0.0626\n",
      "Epoch [0/1] Batch 1172/1200                       Loss D: -1887.2766, loss G: 439.2680, gp: 0.0576\n",
      "Epoch [0/1] Batch 1173/1200                       Loss D: -1894.6650, loss G: 436.5690, gp: 0.0336\n",
      "Epoch [0/1] Batch 1174/1200                       Loss D: -1893.5364, loss G: 441.8439, gp: 0.0322\n",
      "Epoch [0/1] Batch 1175/1200                       Loss D: -1896.1129, loss G: 440.8876, gp: 0.0377\n",
      "Epoch [0/1] Batch 1176/1200                       Loss D: -1899.5470, loss G: 444.9977, gp: 0.0595\n",
      "Epoch [0/1] Batch 1177/1200                       Loss D: -1904.4264, loss G: 444.9663, gp: 0.0782\n",
      "Epoch [0/1] Batch 1178/1200                       Loss D: -1915.1190, loss G: 453.8098, gp: 0.0363\n",
      "Epoch [0/1] Batch 1179/1200                       Loss D: -1910.4084, loss G: 445.8607, gp: 0.0552\n",
      "Epoch [0/1] Batch 1180/1200                       Loss D: -1920.1895, loss G: 449.8775, gp: 0.0393\n",
      "Epoch [0/1] Batch 1181/1200                       Loss D: -1913.0968, loss G: 446.7181, gp: 0.0604\n",
      "Epoch [0/1] Batch 1182/1200                       Loss D: -1915.1031, loss G: 444.2088, gp: 0.0481\n",
      "Epoch [0/1] Batch 1183/1200                       Loss D: -1919.6991, loss G: 445.5192, gp: 0.0431\n",
      "Epoch [0/1] Batch 1184/1200                       Loss D: -1916.3864, loss G: 440.4879, gp: 0.0505\n",
      "Epoch [0/1] Batch 1185/1200                       Loss D: -1924.2205, loss G: 445.6789, gp: 0.0323\n",
      "Epoch [0/1] Batch 1186/1200                       Loss D: -1926.8339, loss G: 443.3362, gp: 0.0222\n",
      "Epoch [0/1] Batch 1187/1200                       Loss D: -1923.7965, loss G: 438.2513, gp: 0.0307\n",
      "Epoch [0/1] Batch 1188/1200                       Loss D: -1923.8008, loss G: 438.4925, gp: 0.0455\n",
      "Epoch [0/1] Batch 1189/1200                       Loss D: -1923.8663, loss G: 441.8030, gp: 0.5020\n",
      "Epoch [0/1] Batch 1190/1200                       Loss D: -1920.9247, loss G: 437.0907, gp: 0.4323\n",
      "Epoch [0/1] Batch 1191/1200                       Loss D: -1926.3744, loss G: 436.1066, gp: 0.0660\n",
      "Epoch [0/1] Batch 1192/1200                       Loss D: -1923.9658, loss G: 434.6563, gp: 0.0639\n",
      "Epoch [0/1] Batch 1193/1200                       Loss D: -1925.3877, loss G: 428.7360, gp: 0.0250\n",
      "Epoch [0/1] Batch 1194/1200                       Loss D: -1929.8413, loss G: 433.4995, gp: 0.1077\n",
      "Epoch [0/1] Batch 1195/1200                       Loss D: -1939.2129, loss G: 442.8763, gp: 0.0576\n",
      "Epoch [0/1] Batch 1196/1200                       Loss D: -1943.7037, loss G: 439.3733, gp: 0.0244\n",
      "Epoch [0/1] Batch 1197/1200                       Loss D: -1947.4684, loss G: 439.9453, gp: 0.0540\n",
      "Epoch [0/1] Batch 1198/1200                       Loss D: -1940.1217, loss G: 432.6673, gp: 0.0584\n",
      "Epoch [0/1] Batch 1199/1200                       Loss D: -1946.3828, loss G: 436.0500, gp: 0.1500\n",
      "Epoch [0/1] Loss D: -669624.7869, loss G: 135106.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1] Test Accuracy: 14.4013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train the target cnn and discriminator\n",
    "target_cnn, discriminator = train_adapt_target(target_cnn, discriminator, source_cnn, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.401316691811823"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will now test the target cnn\n",
    "#we will test the target cnn on the test data\n",
    "#the classifier will be the same as trained\n",
    "#call the function to test the target cnn accuracy\n",
    "get_accuracy(target_cnn, classifier, test_loader)\n",
    "# print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.363333333333333"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the accuracy of source cnn on source data\n",
    "get_accuracy(source_cnn, classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14478/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.69441777533946"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get accuracy of source cnn on target data\n",
    "get_accuracy(source_cnn, classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079402cc50f681fca3bc4b588c8594ae5b0127c6215ec7c89d21fdfb87f97274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
