{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataset we will use is MNIST and USPS\n",
    "\n",
    "#In this notebook we will implement the domain adversarial training of neural network\n",
    "\n",
    "#the code is based on the paper \"Domain-Adversarial Training of Neural Networks\"\n",
    "#also the architecture is same as the paper \"Unsupervised Domain Adaptation by Backpropagation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use resnet50 as the base network\n",
    "# we will use gradient reversal layer to implement the domain adversarial training\n",
    "#we will implement gradient reversal layer at three different stages of the base\n",
    "# classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we will try to implement the gradient reversal layer at the end of the base classifier i.e. just before the fully connected layer and after the last feature extractor layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'officehome_classifier_resnet_stage_1'\n",
    "version = 'v0'\n",
    "\n",
    "#concat experiment name and version to get experiment id\n",
    "experiment_id = experiment_name + '_' + version\n",
    "model_path = 'saved_models/DANN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/officehome/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU name\n",
    "#\n",
    "GPU_NAME = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neceassary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, Function\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#import utils\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for visualizations\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "#import tenserboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable cudnn\n",
    "cudnn.benchmark = True\n",
    "# #cuda cache clear\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "#set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device\n",
    "device = torch.device(GPU_NAME if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the hyperparameters\n",
    "BATCH = 64\n",
    "batch_size = BATCH\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = 224\n",
    "CHANNELS_IMG = 3\n",
    "NUM_CLASSES = 65\n",
    "\n",
    "\n",
    "C = 1   #weight of the domain loss\n",
    "LAMBDA = 1 #scaling factor for the gradient reversal layer\n",
    "GAMMA = 10\n",
    "\n",
    "#parameters of ADAM optimizer\n",
    "LEARNING_RATE = 0.001\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "\n",
    "#parameters of SGD optimizer with momentum\n",
    "MOMENTUM = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to get one hot encoding of labels\n",
    "def one_hot_encoding(label, total_classes=NUM_CLASSES):\n",
    "    vect = torch.eye(total_classes)\n",
    "    return vect[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will define optimizer scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will build 3 different models for implementing the domain adversarial training\n",
    "#first be the feature extractor drived from resnet50\n",
    "#second be a classifier after the feature extractor layer of resnet50\n",
    "#third be the domain classifier with input as the feature vector from the feature extractor and output as the domain label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us first build the feature extractor\n",
    "#we input a resent50 model\n",
    "\n",
    "#creating the model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "#send weight sto gpu\n",
    "# weights = weights.to(device)\n",
    "#sending the model to GPU\n",
    "\n",
    "base_resnet = resnet50(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print model\n",
    "base_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will change the first convolution layer to accept single channel image\n",
    "#conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# #if CHANNELS_IMG == 1:\n",
    "# if CHANNELS_IMG == 1:\n",
    "#     base_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# #change the last fully connected layer to output classes in NUM_CLASSES\n",
    "# base_resnet.fc = nn.Linear(2048, NUM_CLASSES,  bias=True)\n",
    "\n",
    "#write a function to changethe model based on number of channels and number of classes\n",
    "def change_model(model, num_channels = CHANNELS_IMG, num_classes = NUM_CLASSES):\n",
    "    \n",
    "    model.fc = nn.Linear(2048, num_classes,  bias=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "base_resnet = change_model(base_resnet, num_channels = CHANNELS_IMG, num_classes = NUM_CLASSES)\n",
    "base_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will write class for the feature extractor network\n",
    "#we will pass the resnet50 model as the input to the class, and will use : nn.Sequential(*list(original_model.children())[:-2]) to get the feature extractor part of the model\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # self.base_model = base_model\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now create an instance of the feature extractor and print the model\n",
    "feature_extractor = FeatureExtractor(base_resnet).to(device)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print summary of the model\n",
    "#generate random pytorch tensor\n",
    "# x = torch.randn(BATCH, CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "#create x as (1, 1, 224, 224) random tensor\n",
    "# x = torch.rand(1, 1, 224, 224).to(device)\n",
    "#create numpy array of size (1, 1, 224, 224)\n",
    "# x = np.random.rand(1, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "#convert numpy array to tensor\n",
    "# x = torch.from_numpy(x).to(device)\n",
    "#create x as (1, 1, 224, 224) random tensor\n",
    "# x = torch.rand( 224, 224).to(device)\n",
    "# #add batch dimension\n",
    "# x = x.unsqueeze(0)\n",
    "# #add channel dimension\n",
    "# x = x.unsqueeze(0)\n",
    "# #print x shape\n",
    "# print(x.shape)\n",
    "# #pass it through the model\n",
    "# y = feature_extractor(x)\n",
    "# #print y shape\n",
    "# print(y.shape)\n",
    "\n",
    "\n",
    "# summary(feature_extractor, (CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create a random tensor and pass it through the feature extractor\n",
    "# x = torch.randn(1, 1, 224, 224).to(device)\n",
    "# out_feature_extractor = feature_extractor(x)\n",
    "# out_feature_extractor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #now we will create the classifier, it will be same as the part of the resnet50 model after the feature extractor i.e. the last two layers of the resnet50 model\n",
    "# #also the number of classes will be 10 as we have 10 classes in the MNIST dataset, the number of classes is stored in NUM_CLASSES\n",
    "\n",
    "# class ClassClassifier(nn.Module):\n",
    "#     def __init__(self, base_model, num_classes=NUM_CLASSES):\n",
    "#         super(ClassClassifier, self).__init__()\n",
    "#         self.num_classes = num_classes\n",
    "#         self.classifier = nn.Sequential(*list(base_model.children())[-2:])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         #print the shape of the input\n",
    "#         print(\"inside classifier: input shape\",x.shape)\n",
    "#         x = self.classifier(x)\n",
    "#         # x = x.view(-1, self.num_classes)\n",
    "#         return F.softmax(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will create the classifier, it will be same as the part of the resnet50 model after the feature extractor i.e. the last two layers of the resnet50 model\n",
    "#also the number of classes will be 10 as we have 10 classes in the MNIST dataset, the number of classes is stored in NUM_CLASSES\n",
    "\n",
    "class ClassClassifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ClassClassifier, self).__init__()\n",
    "        self.avgpool= nn.Sequential(*list(base_model.children())[-2:-1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(*list(base_model.children())[-1:])\n",
    "        #define a flatten layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print the shape of the input\n",
    "        # print(\"inside classifier: input shape\",x.shape)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassClassifier(\n",
       "  (avgpool): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=65, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of the classifier and print the model\n",
    "class_classifier = ClassClassifier(base_resnet).to(device)\n",
    "class_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary of the model\n",
    "# summary(class_classifier, (2048, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pass the out_feature_extractor through the classifier\n",
    "# out_classifier = classifier(out_feature_extractor)\n",
    "# #print the shape of the output\n",
    "# print(\"output shape\", out_classifier.shape)\n",
    "# # out_classifier.shape\n",
    "# #print the output of the classifier\n",
    "# print(\"output\", out_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summary of the classifier\n",
    "# #create \n",
    "# summary(classifier, (2048, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us write gradient reversal layer\n",
    "\n",
    "class GradientReversalLayer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.lambda_\n",
    "        return output, None\n",
    "    \n",
    "    def grad_reverse(x, lambda_):\n",
    "        return GradientReversalLayer.apply(x, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will create the domain classifier: named as DomainClassifier\n",
    "#it will have the rest of the resnet model after the feature extractor and the classifier and will have an additional layer at the end to output the domain label: 0 for source and 1 for target\n",
    "#it will have gradient reversal layer in between the feature extractor and the classifier, i.e the first layer of the domain classifier \n",
    "#it will be exactly same as ClassClassifier except for the last layer, which is not number of classes but 2 for domain labels and sigmoid activation function instead of softmax\n",
    "#it will also do same [-2:] to get the classifier part of the model, first layer be gradient reversal layer\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        #first layer of the domain classifier be the gradient reversal layer\n",
    "\n",
    "        self.avgpool = nn.Sequential(*list(base_model.children())[-2:-1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(2048, 512, bias=True))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(512, 62, bias=True))\n",
    "        self.fc3 = nn.Sequential(nn.Linear(62, 2, bias=True))\n",
    "        \n",
    "        #now add the last output layer\n",
    "        # self.domain_classifier.add_module('domain_classifier_output', nn.Linear(2048, 2))\n",
    "        #change the last layer to output 2 classes\n",
    "        # self.fc = nn.Linear(2048, 2 , bias=True)\n",
    "\n",
    "        #forward\n",
    "    def forward(self, x, lambda_ = LAMBDA):\n",
    "        x = GradientReversalLayer.grad_reverse(x, lambda_)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        #output the domain label\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = x.view(-1, 2)\n",
    "        return F.softmax(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DomainClassifier(\n",
       "  (avgpool): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=62, bias=True)\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=62, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of the domain classifier and print the model\n",
    "domain_classifier = DomainClassifier(base_resnet).to(device)\n",
    "domain_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print summary of the model\n",
    "# summary(domain_classifier, (2048, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create an instance of the domain classifier and print the model\n",
    "# domain_classifier = DomainClassifier(base_resnet).to(device)\n",
    "# #pass the output of feature extractor through the domain classifier\n",
    "# out_domain_classifier = domain_classifier(out_feature_extractor, 1)\n",
    "# #print the shape of the output\n",
    "# print(\"output shape\", out_domain_classifier.shape)\n",
    "# #print the output of the domain classifier\n",
    "# print(\"output\", out_domain_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the transform for the dataset\n",
    "transform_mnist_resnet = transforms.Compose(\n",
    "    [\n",
    "  \n",
    "    # if torch tensor then leave as it is, else convert to tensor\n",
    "    transforms.Lambda(lambda x: x if isinstance(x, torch.Tensor) else transforms.functional.to_tensor(x)),\n",
    "    #\n",
    "\n",
    "    #resize to 224x224\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "\n",
    "    #check if channels are 1, then convert to 3 channels\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "\n",
    "    transforms.Lambda(lambda x: preprocess(x)),\n",
    "\n",
    "   \n",
    "    \n",
    "    # normalize\n",
    "    transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training we will use MNIST dataset in pytorch library\n",
    "#for testing we will use USPS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data - Real_World\n",
    "#### test data - Clipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "#load the domain Real_World from the dataset\n",
    "\n",
    "train_data = datasets.ImageFolder(root=data_path + 'Real_World', transform=transform_mnist_resnet)\n",
    "#load train data\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load test data\n",
    "#Clipart dataset\n",
    "test_data = datasets.ImageFolder(root=data_path + 'Clipart', transform=transform_mnist_resnet)\n",
    "\n",
    "\n",
    "#load test data\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4357\n",
      "torch.Size([3, 224, 224])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#print the length of train and test data\n",
    "print(len(train_data))\n",
    "#print the shape of train data\n",
    "print(train_data[0][0].shape)\n",
    "#print label of train data\n",
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4365\n",
      "torch.Size([3, 224, 224])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#print length of test data\n",
    "print(len(test_data))\n",
    "#print shape of test data\n",
    "print(test_data[0][0].shape)\n",
    "#print label of test data\n",
    "print(test_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  4357\n",
      "Test size:  4365\n"
     ]
    }
   ],
   "source": [
    "#get the size of the train data and test data\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "#print\n",
    "print('Train size: ', train_size)\n",
    "print('Test size: ', test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #we will test the model on the first batch of the train data, to test the dimensions of the output of the model and the loss function\n",
    "# #we define a function to test the model\n",
    "# def test_model(feature_extractor, class_classifier, loss_function):\n",
    "#     #make the model sequential to pass the input through the feature extractor and then the output of feature extractor to the class classifier\n",
    "#     # model = nn.Sequential(feature_extractor, class_classifier, loss_function)\n",
    "\n",
    "#     #set model to eval mode\n",
    "#     feature_extractor.eval()\n",
    "#     class_classifier.eval()\n",
    "\n",
    "#     #initialize the loss and number of correct predictions\n",
    "#     loss = 0\n",
    "#     correct = 0\n",
    "\n",
    "#     #get the first batch of the train data\n",
    "#     #wee use for and then break to get the first batch\n",
    "#     for data, label in train_loader:\n",
    "\n",
    "#         #print data shape\n",
    "#         print(\"data shape\", data.shape)\n",
    "#         #print label shape\n",
    "#         print(\"label shape\", label.shape)\n",
    "#         #print data type\n",
    "#         print(\"data type\", data.dtype)\n",
    "#         #print label type\n",
    "#         print(\"label type\", label.dtype)\n",
    "#         #print first label\n",
    "#         print(\"first label\", label[0])\n",
    "#         #print all label\n",
    "#         print(\"all label\", label)\n",
    "\n",
    "\n",
    "#         #move data and label to device\n",
    "#         data, label = data.to(device), label.to(device)\n",
    "\n",
    "#         #we measure the accuracy here\n",
    "#         #get output from the model\n",
    "#         # output = model(data)\n",
    "#         #get the output from the feature extractor\n",
    "#         feature = feature_extractor(data)\n",
    "#         #print the shape of the feature\n",
    "#         print(\"feature shape: \", feature.shape)\n",
    "#         #print type of feature\n",
    "#         print(\"feature type: \", type(feature))\n",
    "#         #get the output from the class classifier\n",
    "#         output = class_classifier(feature)\n",
    "#         #print the shape of the output\n",
    "#         print(\"output shape: \", output.shape)\n",
    "#         #print type of output\n",
    "\n",
    "#         #print output shape\n",
    "#         print(\"output shape: \", output.shape)\n",
    "#         #print output type\n",
    "#         print(\"output type: \", type(output))\n",
    "\n",
    "#         #get the loss\n",
    "#         loss = loss_function(output, label)\n",
    "\n",
    "#         #print loss\n",
    "#         print(\"loss: \", loss)\n",
    "\n",
    "#         #get the prediction\n",
    "#         pred = output.argmax(dim=1, keepdim=True)\n",
    "#         #print pred\n",
    "#         print(\"pred: \", pred)\n",
    "#         #update the correct predictions\n",
    "#         correct = pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "#         #print correct\n",
    "#         print(\"correct: \", correct)\n",
    "#         #accuracy\n",
    "#         accuracy = correct / len(data)\n",
    "#         #print accuracy\n",
    "#         print(\"accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #break\n",
    "#         break\n",
    "\n",
    "# #now we test the above function and build model and loss function\n",
    "# #build the model\n",
    "# #the classifier is composed of two models - feature extractor and classifier in sequence: input first passes through feature extractor and then the output of feature extractor is passed to classifier\n",
    "# # and the output of classifier is the final output\n",
    "# classifier = ClassClassifier(base_resnet, NUM_CLASSES).to(device)\n",
    "# #build the feature extractor model from the class FeatureExtractor\n",
    "# feature_extractor = FeatureExtractor(base_resnet).to(device)\n",
    "# #now we make end to end model by combining the feature extractor and classifier\n",
    "# # model = nn.Sequential(feature_extractor, classifier).to(device)\n",
    "# #build the loss function\n",
    "# loss_function = nn.CrossEntropyLoss().to(device)\n",
    "# #test the model\n",
    "# test_model(feature_extractor, classifier, loss_function)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 12:43:10.285792: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "#initialize tensorboard writer\n",
    "#create writer for tensorboard\n",
    "writer = SummaryWriter(f'runs/'+experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a save model function\n",
    "#the arguments are the models, the optimizer, the epoch number, the model name\n",
    "def save_model(feature_extractor, class_classifier, optimizer, epoch):\n",
    "    #check if path exists, if not then create it\n",
    "    #recursively check if each root directory exists, if not then recursively create each\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "\n",
    "\n",
    "    #we will save the model by the name of the experiment id and the epoch number\n",
    "    torch.save(feature_extractor.state_dict(), f'{model_path}/{experiment_id}_feature_extractor_{epoch}.pth')\n",
    "    torch.save(class_classifier.state_dict(), f'{model_path}/{experiment_id}_class_classifier_{epoch}.pth')\n",
    "    torch.save(domain_classifier.state_dict(), f'{model_path}/{experiment_id}_domain_classifier_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining Class Classifier and Domain classifers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will write a routine to make the class classifier to have optimal performance on the source domain\n",
    "#this we call the pretrained model : as the class classifier is pretrained on the source domain\n",
    "# this kind of initialization and\n",
    "#better initialization would help the training to converge to a better solution\n",
    "\n",
    "#funtion to train the model: feature extractor and class classifier on the source domain\n",
    "#we make feature extractor non trainable\n",
    "#only parameters of class classifier are trainable\n",
    "\n",
    "# def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Domain Label : 0\n",
    "#### Target Domain Label : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function to test the model, i.e. the class classifier on the test data\n",
    "#the function takes as argument : the feature extractor, class classifier, domain classifier, test data loader \n",
    "def DANN_test(feature_extractor, class_classifier, domain_classifier, test_data_loader, dataset='TARGET'):\n",
    "    #set the model to evaluation mode\n",
    "    feature_extractor.eval()\n",
    "    class_classifier.eval()\n",
    "    domain_classifier.eval()\n",
    "\n",
    "    #set the total and correct for class classifier to 0\n",
    "    total_class_classifier = 0\n",
    "    correct_class_classifier = 0\n",
    "    #set the total and correct for domain classifier to 0\n",
    "    total_domain_classifier = 0\n",
    "    correct_domain_classifier = 0\n",
    "    \n",
    "    #set the writer\n",
    "    # writer.add_scalar\n",
    " \n",
    "    #loop through the test data loader\n",
    "    with torch.no_grad():\n",
    "        #loop through the test data loader\n",
    "        for data, labels in test_data_loader:\n",
    "            #get the data and labels\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            #if dataset is target, then we will add the domain labels as 1\n",
    "            if dataset == 'TARGET':\n",
    "                domain_labels = torch.ones(data.size(0)).long().to(device)\n",
    "            #if dataset is source, then we will add the domain labels as 0\n",
    "            elif dataset == 'SOURCE':\n",
    "                domain_labels = torch.zeros(data.size(0)).long().to(device)\n",
    "                #if dataset is not source or target, then we will raise an error\n",
    "            else:\n",
    "                raise ValueError('Dataset must be source or target')\n",
    "            #get the feature vector\n",
    "            feature_vector = feature_extractor(data)\n",
    "            #get the class classifier prediction\n",
    "            class_classifier_pred = class_classifier(feature_vector)\n",
    "            #get the domain classifier prediction\n",
    "            domain_classifier_pred = domain_classifier(feature_vector)\n",
    "   \n",
    "            #get the predicted class\n",
    "            _, predicted_class = torch.max(class_classifier_pred.data, 1)\n",
    "            #get the total and correct for class classifier\n",
    "            total_class_classifier += labels.size(0)\n",
    "            correct_class_classifier += (predicted_class == labels).sum().item()\n",
    "            #get the total and correct for domain classifier\n",
    "            _, predicted_domain = torch.max(domain_classifier_pred.data, 1)\n",
    "            total_domain_classifier += domain_labels.size(0)\n",
    "            correct_domain_classifier += (predicted_domain == domain_labels).sum().item()\n",
    "    #print the loss and accuracy\n",
    "    print(dataset+'_'+'Test Accuracy Class Classifier: {}/{} ({:.0f}%)'.format(correct_class_classifier, total_class_classifier,\n",
    "        100. * correct_class_classifier / total_class_classifier))\n",
    "    print(dataset+'_'+'Test Accuracy Domain Classifier: {}/{} ({:.0f}%)'.format(correct_domain_classifier, total_domain_classifier,\n",
    "        100. * correct_domain_classifier / total_domain_classifier))\n",
    "    \n",
    "    #write the loss and accuracy to the tensorboard\n",
    "    writer.add_scalar(dataset+'Test_Class_Accuracy', 100. * correct_class_classifier / total_class_classifier, global_step=0)\n",
    "    writer.add_scalar(dataset+'Test_Domain_Accuracy', 100. * correct_domain_classifier / total_domain_classifier, global_step=0)\n",
    "    #close the writer\n",
    "    writer.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5787/1062286346.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n",
      "/tmp/ipykernel_5787/2842852021.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET_Test Accuracy Class Classifier: 94/4365 (2%)\n",
      "TARGET_Test Accuracy Domain Classifier: 635/4365 (15%)\n"
     ]
    }
   ],
   "source": [
    "#let us first see the models performance on the test data before training: so we can compare the performance before and after training\n",
    "#test the model on the test data\n",
    "DANN_test(feature_extractor, class_classifier, domain_classifier, test_loader, dataset='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5787/1062286346.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n",
      "/tmp/ipykernel_5787/2842852021.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE_Test Accuracy Class Classifier: 56/4357 (1%)\n",
      "SOURCE_Test Accuracy Domain Classifier: 3428/4357 (79%)\n"
     ]
    }
   ],
   "source": [
    "#we will also test the model on the source data\n",
    "DANN_test(feature_extractor, class_classifier, domain_classifier, train_loader, dataset='SOURCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will write a function for training the models as per DANN paper\n",
    "#the arguments are: feature extractor, Class Classifier, Domain Classifier, source data loader, target data loader, number of epochs\n",
    "\n",
    "#we will simultaneously track the loss and accuracy of the model on source and target data\n",
    "#so we will store total corrects and total predictions for both source and target data\n",
    "\n",
    "\n",
    "def DANN_training(feature_extractor, class_classifier, domain_classifier, source_data_loader, target_data_loader, num_epochs=EPOCHS, device = device):\n",
    "    #for optimizer we will club all the parameters of all the models\n",
    "    #we use Adam optimizer\n",
    "    #the parameters of ADAM  are the standard ones for ADAM optimizer\n",
    "    optimizer = optim.Adam(list(feature_extractor.parameters()) + list(class_classifier.parameters()) + list(domain_classifier.parameters()), lr=LEARNING_RATE, betas=(BETA_1, BETA_2))\n",
    "    #we define the loss function for the domain classifier as cross entropy loss\n",
    "    loss_fn_domain = nn.CrossEntropyLoss().to(device)\n",
    "    #we define the loss function for the class classifier as cross entropy loss\n",
    "    loss_fn_class = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    #we will iterate through datasets in a unique way, as we need one batch of source data and one batch of target data at a time to process through the models and then update the parameters\n",
    "    #but we do not have same number of batches in source and target data\n",
    "    #so we will first assume that source data has more batches than target data\n",
    "    #write an assert statement to check if this is true\n",
    "    # assert len(source_data_loader) >= len(target_data_loader), \"Source data should have more batches than target data\"\n",
    "    #now we have source data to have more batches than target data, thus we will have 2 loops, one for source data and one for target data\n",
    "    #the outer would be for source data and inner for target data\n",
    "    #and we get the batch number of the source data from the outer loop and in the inner loop through the target data we will select the corresponding batch number of the target data \n",
    "    #, if the batch number is less than or equal to the minimum of the number of batches in source and target data,\n",
    "    # else when we have exhausted the target data, we will start again from the first batch of the target data\n",
    "    #we will do this untill we have exhausted the source data\n",
    "\n",
    "    #we will keep track of the batch number of the source data in the outer loop\n",
    "    batch_num_source = 0\n",
    "    #we will keep track of the batch number of the target data in the inner loop\n",
    "    batch_num_target = 0\n",
    "    #we will keep track of the minimum of the number of batches in source and target data\n",
    "    min_batches = min(len(source_data_loader), len(target_data_loader))\n",
    "    #we will keep track of the number of batches in source data\n",
    "    num_batches_source = len(source_data_loader)\n",
    "    #we will keep track of the number of batches in target data\n",
    "    num_batches_target = len(target_data_loader)\n",
    "\n",
    "    #set all the models to train mode\n",
    "    feature_extractor.train()\n",
    "    class_classifier.train()\n",
    "    domain_classifier.train()\n",
    "\n",
    "    #variable to store the statistics of model training, like : loss, accuracy, etc\n",
    "    epoch_tracker = 0   #it tracks the loop number\n",
    "    batch_tracker = 0   #it tracks the batch number * epoch number\n",
    "    #loss\n",
    "    total_loss = 0\n",
    "    total_class_loss = 0\n",
    "    total_domain_loss = 0\n",
    "    #accuracy\n",
    "    total_class_classifier = 0\n",
    "    correct_class_classifier = 0\n",
    "    total_domain_classifier = 0\n",
    "    correct_domain_classifier = 0\n",
    "\n",
    "    best_epoch_accuracy = 0     #this will store the best epoch accuracy\n",
    "    best_epoch = 0              #this will store the best epoch number\n",
    "\n",
    "    #loop through the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #if epoch is not 0, then we have values stored in epoch_total_class_classifier, epoch_total_class_correct, epoch_total_domain_classifier, epoch_total_domain_correct\n",
    "        #so we will calculate the accuracy and loss for the previous epoch\n",
    "        if epoch != 0:\n",
    "            #calculate the accuracy\n",
    "            epoch_accuracy_class_classifier = 100 * correct_class_classifier / total_class_classifier\n",
    "            epoch_accuracy_domain_classifier = 100 * correct_domain_classifier / total_domain_classifier\n",
    "            #calculate the loss, we divide total loss by batch_size, as we have added the loss for all the elements in the batch\n",
    "            epoch_loss_class_classifier = total_class_loss /  batch_min_size\n",
    "            epoch_loss_domain_classifier = total_domain_loss / batch_min_size\n",
    "            #write the loss and accuracy to tensorboard\n",
    "            writer.add_scalar('EPOCH_Train_Loss_class_classifier', epoch_loss_class_classifier, epoch_tracker)\n",
    "            writer.add_scalar('EPOCH_Train_Loss_domain_classifier', epoch_loss_domain_classifier, epoch_tracker)\n",
    "            writer.add_scalar('EPOCH_Train_Accuracy_class_classifier', epoch_accuracy_class_classifier, epoch_tracker)\n",
    "            writer.add_scalar('EPOCH_Train_Accuracy_domain_classifier', epoch_accuracy_domain_classifier, epoch_tracker)\n",
    "\n",
    "            #if the epoch_accuracy_domain_classifier is greater than best_epoch_accuracy, then we will update the best_epoch_accuracy and best_epoch\n",
    "            if epoch_accuracy_domain_classifier > best_epoch_accuracy:\n",
    "                best_epoch_accuracy = epoch_accuracy_domain_classifier\n",
    "                best_epoch = epoch\n",
    "                #we will save the model by the name of the experiment id and the epoch number\n",
    "                #call the save_model function\n",
    "                save_model(feature_extractor, class_classifier, domain_classifier, epoch)\n",
    "\n",
    "\n",
    "\n",
    "            #increment the epoch tracker\n",
    "            epoch_tracker += 1\n",
    "            \n",
    "          \n",
    "\n",
    "        #epoch variable to track statistics in an epoch\n",
    "        #loss\n",
    "        epoch_total_loss = 0\n",
    "        epoch_total_class_loss = 0\n",
    "        epoch_total_domain_loss = 0\n",
    "        #accuracy\n",
    "        epoch_total_class_classifier = 0\n",
    "        epoch_correct_class_classifier = 0\n",
    "        epoch_total_domain_classifier = 0\n",
    "        epoch_correct_domain_classifier = 0\n",
    "        \n",
    "\n",
    "        #start outer loop for source data\n",
    "        for batch_idx_source, (source_,target_) in enumerate(zip(source_data_loader, target_data_loader)):\n",
    "            #if the batch number of source data is less than the minimum of the number of batches in source and target data, then we will select the corresponding batch number of the target data\n",
    "            # #we name that as batch_of_target\n",
    "            # if batch_idx_source < min_batches:\n",
    "            #     batch_of_target = batch_idx_source\n",
    "\n",
    "            # #check if the batch_idx_source is equal to or greater than the minimum of the number of batches in source and target data\n",
    "            # else:\n",
    "            #     #if yes, then we will start again from the first batch of the target data\n",
    "            #     batch_of_target = batch_idx_source % min_batches\n",
    "\n",
    "        #start inner loop for target data\n",
    "        # for batch_idx_target, (target_data, target_labels) in enumerate(target_data_loader):\n",
    "            temp_total_class_classifier = 0\n",
    "            temp_correct_class_classifier = 0\n",
    "            temp_total_domain_classifier = 0\n",
    "            temp_correct_domain_classifier = 0\n",
    "            #now we will run the inner loop only when batch_idx_target is equal to the batch_of_target\n",
    "            #in all else cases we will continue\n",
    "            # if batch_idx_target != batch_of_target:\n",
    "            #     continue\n",
    "\n",
    "            #now we have one batch of source data and one batch of target data\n",
    "            #now we run as if we are training a normal model with source data and target data in a single batch\n",
    "            #send source data and target data to device\n",
    "            source_data, source_labels = source_\n",
    "            target_data, target_labels = target_\n",
    "            source_data, source_labels = source_data.to(device), source_labels.to(device)\n",
    "            target_data, target_labels = target_data.to(device), target_labels.to(device)\n",
    "\n",
    "            #now we will calculate the lambda for gradient reversal layer: for current loop\n",
    "            p = float(batch_idx_source + epoch *num_batches_source ) / num_epochs * num_batches_source\n",
    "            lambda_ = 2. / (1. + np.exp(- GAMMA * p)) - 1\n",
    "\n",
    "            #now we check that the number of images in source data and target data are same: i.e. batch size is same: this is particularly useful in case of last batch\n",
    "            batch_min_size = min(source_data.shape[0], target_data.shape[0])\n",
    "            #now we will select the same number of images from source and target data\n",
    "            source_data = source_data[:batch_min_size]\n",
    "            source_labels = source_labels[:batch_min_size]\n",
    "            target_data = target_data[:batch_min_size]\n",
    "            target_labels = target_labels[:batch_min_size]\n",
    "\n",
    "\n",
    "            #zero grdient optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #now we will create the domain labels for source and target data\n",
    "            #this is required for the domain classifier\n",
    "            #source domain label is 0\n",
    "            source_domain_labels = torch.zeros(source_data.shape[0]).long().to(device)\n",
    "            #target domain label is 1\n",
    "            target_domain_labels = torch.ones(target_data.shape[0]).long().to(device)\n",
    "\n",
    "            #now we will pass the source data and target data through the feature extractor\n",
    "            source_features = feature_extractor(source_data)\n",
    "            target_features = feature_extractor(target_data)\n",
    "\n",
    "            #now we will calculate class prediction of source data\n",
    "            source_class_pred = class_classifier(source_features)\n",
    "            #now we calculate the loss for the class classifier\n",
    "            class_classifier_loss = loss_fn_class(source_class_pred, source_labels)\n",
    "\n",
    "            #now we will calculate the domain prediction of source data and target data\n",
    "            source_domain_pred = domain_classifier(source_features, lambda_)\n",
    "            target_domain_pred = domain_classifier(target_features, lambda_)\n",
    "            #now we will calculate the loss for the domain classifier\n",
    "            domain_classifier_loss = loss_fn_domain(source_domain_pred, source_domain_labels) + loss_fn_domain(target_domain_pred, target_domain_labels)\n",
    "\n",
    "            #now we will calculate the total loss using the parameter: C\n",
    "            total_loss = class_classifier_loss + C * domain_classifier_loss\n",
    "\n",
    "            #now we will calculate the gradients\n",
    "            total_loss.backward()\n",
    "            #now we will update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            #now we will calculate the accuracy of the class classifier\n",
    "            _, predicted = torch.max(source_class_pred.data, 1)\n",
    "            temp_total_class_classifier += source_labels.size(0)\n",
    "            temp_correct_class_classifier += (predicted == source_labels).sum().item()\n",
    "            #add the total and correct to total, correct and epoch_total, epoch_correct\n",
    "            total_class_classifier += temp_total_class_classifier\n",
    "            correct_class_classifier += temp_correct_class_classifier\n",
    "            epoch_total_class_classifier += temp_total_class_classifier\n",
    "            epoch_correct_class_classifier += temp_correct_class_classifier\n",
    "\n",
    "            #now we will calculate the accuracy of the domain classifier\n",
    "            _, predicted = torch.max(source_domain_pred.data, 1)\n",
    "            temp_total_domain_classifier += source_domain_labels.size(0)\n",
    "            temp_correct_domain_classifier += (predicted == source_domain_labels).sum().item()\n",
    "            _, predicted = torch.max(target_domain_pred.data, 1)\n",
    "            temp_total_domain_classifier += target_domain_labels.size(0)\n",
    "            temp_correct_domain_classifier += (predicted == target_domain_labels).sum().item()\n",
    "            #add the total and correct to total, correct and epoch_total, epoch_correct\n",
    "            total_domain_classifier += temp_total_domain_classifier\n",
    "            correct_domain_classifier += temp_correct_domain_classifier\n",
    "            epoch_total_domain_classifier += temp_total_domain_classifier\n",
    "            epoch_correct_domain_classifier += temp_correct_domain_classifier\n",
    "\n",
    "            #now we will print the loss and accuracy based on the temp values\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tClass Loss: {:.6f}\\tDomain Loss: {:.6f}\\tClass Accuracy: {}/{} ({:.0f}%)\\tDomain Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch, batch_idx_source * len(source_data), len(source_data_loader.dataset),\n",
    "                100. * batch_idx_source / len(source_data_loader), class_classifier_loss.item(), domain_classifier_loss.item(),\n",
    "                temp_correct_class_classifier, temp_total_class_classifier,\n",
    "                100. * temp_correct_class_classifier / temp_total_class_classifier, temp_correct_domain_classifier, temp_total_domain_classifier,\n",
    "                100. * temp_correct_domain_classifier / temp_total_domain_classifier))\n",
    "            \n",
    "            #we will also write the loss and accuracy to the tensorboard based on temp values\n",
    "            writer.add_scalar('BATCH_Train_Class_Loss', class_classifier_loss.item(), global_step=batch_tracker)\n",
    "            writer.add_scalar('BATCH_Train_Domain_Loss', domain_classifier_loss.item(), global_step=batch_tracker)\n",
    "            writer.add_scalar('BATCH_Train_Class_Accuracy', 100. * temp_correct_class_classifier / temp_total_class_classifier, global_step=batch_tracker)\n",
    "            writer.add_scalar('BATCH_Train_Domain_Accuracy', 100. * temp_correct_domain_classifier / temp_total_domain_classifier, global_step=batch_tracker)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            batch_tracker += 1\n",
    "\n",
    "        #if epoch is 0 , then we will save the models and store best accuracy and epoch\n",
    "        if epoch == 0:\n",
    "            #save the model\n",
    "            #call the save model function\n",
    "            save_model(feature_extractor, class_classifier, domain_classifier, epoch)\n",
    "            #store the best accuracy and epoch\n",
    "            best_accuracy = 100. * epoch_correct_class_classifier / epoch_total_class_classifier\n",
    "            best_epoch = epoch\n",
    "    #now we will print the total accuracy for the training data\n",
    "    #now we will print the total accuracy for the training data\n",
    "    total_class_classifier_accuracy = 100. * correct_class_classifier / total_class_classifier\n",
    "    total_domain_classifier_accuracy = 100. * correct_domain_classifier / total_domain_classifier\n",
    "    print('Train Accuracy Class Classifier: {}/{} ({:.0f}%)'.format(correct_class_classifier, total_class_classifier, total_class_classifier_accuracy))\n",
    "    print('Train Accuracy Domain Classifier: {}/{} ({:.0f}%)'.format(correct_domain_classifier, total_domain_classifier, total_domain_classifier_accuracy))\n",
    "    #we will also write the loss and accuracy to the tensorboard based on total values\n",
    "    writer.add_scalar('TOTAL_Train_Class_Accuracy', total_class_classifier_accuracy, global_step=0)\n",
    "    writer.add_scalar('TOTAL_Train_Domain_Accuracy', total_domain_classifier_accuracy, global_step=0)\n",
    "    #save the model\n",
    "    #call the save model function\n",
    "    save_model(feature_extractor, class_classifier, domain_classifier, epoch= EPOCHS)\n",
    "    #print the best accuracy and epoch\n",
    "    print('Best Accuracy: {:.0f}% at Epoch: {}'.format(best_epoch_accuracy, best_epoch))\n",
    "\n",
    "\n",
    "    #close the writer\n",
    "    writer.close()\n",
    "\n",
    "    #return models\n",
    "    return feature_extractor, class_classifier, domain_classifier\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5787/1062286346.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n",
      "/tmp/ipykernel_5787/2842852021.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4357 (0%)]\tClass Loss: 4.174469\tDomain Loss: 1.386315\tClass Accuracy: 0/64 (0%)\tDomain Accuracy: 63/128 (49%)\n",
      "Train Epoch: 0 [64/4357 (1%)]\tClass Loss: 4.171318\tDomain Loss: 1.355121\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [128/4357 (3%)]\tClass Loss: 4.170028\tDomain Loss: 1.407735\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 65/128 (51%)\n",
      "Train Epoch: 0 [192/4357 (4%)]\tClass Loss: 4.169221\tDomain Loss: 1.454373\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 58/128 (45%)\n",
      "Train Epoch: 0 [256/4357 (6%)]\tClass Loss: 4.170135\tDomain Loss: 1.556932\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 60/128 (47%)\n",
      "Train Epoch: 0 [320/4357 (7%)]\tClass Loss: 4.166584\tDomain Loss: 1.596459\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 60/128 (47%)\n",
      "Train Epoch: 0 [384/4357 (9%)]\tClass Loss: 4.155640\tDomain Loss: 1.594908\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 55/128 (43%)\n",
      "Train Epoch: 0 [448/4357 (10%)]\tClass Loss: 4.157960\tDomain Loss: 1.448663\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 50/128 (39%)\n",
      "Train Epoch: 0 [512/4357 (12%)]\tClass Loss: 4.152270\tDomain Loss: 1.280717\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 71/128 (55%)\n",
      "Train Epoch: 0 [576/4357 (13%)]\tClass Loss: 4.163614\tDomain Loss: 1.427307\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 66/128 (52%)\n",
      "Train Epoch: 0 [640/4357 (14%)]\tClass Loss: 4.161080\tDomain Loss: 1.509307\tClass Accuracy: 1/64 (2%)\tDomain Accuracy: 65/128 (51%)\n",
      "Train Epoch: 0 [704/4357 (16%)]\tClass Loss: 4.151135\tDomain Loss: 1.549716\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 65/128 (51%)\n",
      "Train Epoch: 0 [768/4357 (17%)]\tClass Loss: 4.152075\tDomain Loss: 1.663755\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 45/128 (35%)\n",
      "Train Epoch: 0 [832/4357 (19%)]\tClass Loss: 4.142104\tDomain Loss: 1.772076\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 31/128 (24%)\n",
      "Train Epoch: 0 [896/4357 (20%)]\tClass Loss: 4.170494\tDomain Loss: 1.788339\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 21/128 (16%)\n",
      "Train Epoch: 0 [960/4357 (22%)]\tClass Loss: 4.159347\tDomain Loss: 1.812902\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 41/128 (32%)\n",
      "Train Epoch: 0 [1024/4357 (23%)]\tClass Loss: 4.155787\tDomain Loss: 1.720220\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1088/4357 (25%)]\tClass Loss: 4.142849\tDomain Loss: 1.683601\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1152/4357 (26%)]\tClass Loss: 4.149807\tDomain Loss: 1.654129\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1216/4357 (28%)]\tClass Loss: 4.165730\tDomain Loss: 1.675161\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1280/4357 (29%)]\tClass Loss: 4.169257\tDomain Loss: 1.657019\tClass Accuracy: 1/64 (2%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1344/4357 (30%)]\tClass Loss: 4.161301\tDomain Loss: 1.686860\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1408/4357 (32%)]\tClass Loss: 4.125050\tDomain Loss: 1.683597\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1472/4357 (33%)]\tClass Loss: 4.128295\tDomain Loss: 1.760113\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1536/4357 (35%)]\tClass Loss: 4.154983\tDomain Loss: 1.751264\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [1600/4357 (36%)]\tClass Loss: 4.174089\tDomain Loss: 1.874418\tClass Accuracy: 0/64 (0%)\tDomain Accuracy: 56/128 (44%)\n",
      "Train Epoch: 0 [1664/4357 (38%)]\tClass Loss: 4.161523\tDomain Loss: 1.971032\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 32/128 (25%)\n",
      "Train Epoch: 0 [1728/4357 (39%)]\tClass Loss: 4.133374\tDomain Loss: 2.026236\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 20/128 (16%)\n",
      "Train Epoch: 0 [1792/4357 (41%)]\tClass Loss: 4.166060\tDomain Loss: 2.041051\tClass Accuracy: 1/64 (2%)\tDomain Accuracy: 18/128 (14%)\n",
      "Train Epoch: 0 [1856/4357 (42%)]\tClass Loss: 4.178108\tDomain Loss: 2.024169\tClass Accuracy: 0/64 (0%)\tDomain Accuracy: 23/128 (18%)\n",
      "Train Epoch: 0 [1920/4357 (43%)]\tClass Loss: 4.167571\tDomain Loss: 1.989945\tClass Accuracy: 1/64 (2%)\tDomain Accuracy: 32/128 (25%)\n",
      "Train Epoch: 0 [1984/4357 (45%)]\tClass Loss: 4.143044\tDomain Loss: 1.928588\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 54/128 (42%)\n",
      "Train Epoch: 0 [2048/4357 (46%)]\tClass Loss: 4.112309\tDomain Loss: 1.890167\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 53/128 (41%)\n",
      "Train Epoch: 0 [2112/4357 (48%)]\tClass Loss: 4.124804\tDomain Loss: 1.641009\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2176/4357 (49%)]\tClass Loss: 4.150351\tDomain Loss: 1.801356\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 58/128 (45%)\n",
      "Train Epoch: 0 [2240/4357 (51%)]\tClass Loss: 4.163294\tDomain Loss: 1.779324\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 62/128 (48%)\n",
      "Train Epoch: 0 [2304/4357 (52%)]\tClass Loss: 4.149901\tDomain Loss: 1.782271\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2368/4357 (54%)]\tClass Loss: 4.123778\tDomain Loss: 1.767796\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2432/4357 (55%)]\tClass Loss: 4.140061\tDomain Loss: 1.792951\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2496/4357 (57%)]\tClass Loss: 4.150385\tDomain Loss: 1.810968\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2560/4357 (58%)]\tClass Loss: 4.105102\tDomain Loss: 1.823437\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2624/4357 (59%)]\tClass Loss: 4.110659\tDomain Loss: 1.819537\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2688/4357 (61%)]\tClass Loss: 4.130484\tDomain Loss: 1.796014\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2752/4357 (62%)]\tClass Loss: 4.148171\tDomain Loss: 1.626426\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2816/4357 (64%)]\tClass Loss: 4.136416\tDomain Loss: 1.627263\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2880/4357 (65%)]\tClass Loss: 4.138397\tDomain Loss: 1.626820\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [2944/4357 (67%)]\tClass Loss: 4.114553\tDomain Loss: 1.626599\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3008/4357 (68%)]\tClass Loss: 4.129732\tDomain Loss: 1.626735\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3072/4357 (70%)]\tClass Loss: 4.150909\tDomain Loss: 1.626781\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3136/4357 (71%)]\tClass Loss: 4.100471\tDomain Loss: 1.626665\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3200/4357 (72%)]\tClass Loss: 4.143160\tDomain Loss: 1.626531\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3264/4357 (74%)]\tClass Loss: 4.153788\tDomain Loss: 1.626926\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3328/4357 (75%)]\tClass Loss: 4.104728\tDomain Loss: 1.629190\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3392/4357 (77%)]\tClass Loss: 4.137202\tDomain Loss: 1.632059\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3456/4357 (78%)]\tClass Loss: 4.146058\tDomain Loss: 1.629080\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3520/4357 (80%)]\tClass Loss: 4.144361\tDomain Loss: 1.635942\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3584/4357 (81%)]\tClass Loss: 4.153340\tDomain Loss: 1.630458\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3648/4357 (83%)]\tClass Loss: 4.101600\tDomain Loss: 1.641519\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3712/4357 (84%)]\tClass Loss: 4.115863\tDomain Loss: 1.645950\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3776/4357 (86%)]\tClass Loss: 4.162740\tDomain Loss: 1.652999\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3840/4357 (87%)]\tClass Loss: 4.122332\tDomain Loss: 1.668127\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3904/4357 (88%)]\tClass Loss: 4.117317\tDomain Loss: 1.663020\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [3968/4357 (90%)]\tClass Loss: 4.162828\tDomain Loss: 1.680516\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [4032/4357 (91%)]\tClass Loss: 4.135624\tDomain Loss: 1.698146\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [4096/4357 (93%)]\tClass Loss: 4.130672\tDomain Loss: 1.714074\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [4160/4357 (94%)]\tClass Loss: 4.158502\tDomain Loss: 1.704535\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [4224/4357 (96%)]\tClass Loss: 4.133089\tDomain Loss: 1.682506\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [4288/4357 (97%)]\tClass Loss: 4.124599\tDomain Loss: 1.705234\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 0 [340/4357 (99%)]\tClass Loss: 4.192913\tDomain Loss: 1.628142\tClass Accuracy: 0/5 (0%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 1 [0/4357 (0%)]\tClass Loss: 4.150702\tDomain Loss: 1.634664\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [64/4357 (1%)]\tClass Loss: 4.138939\tDomain Loss: 1.697994\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [128/4357 (3%)]\tClass Loss: 4.175137\tDomain Loss: 1.552771\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [192/4357 (4%)]\tClass Loss: 4.174718\tDomain Loss: 1.576262\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [256/4357 (6%)]\tClass Loss: 4.127187\tDomain Loss: 1.662488\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [320/4357 (7%)]\tClass Loss: 4.125978\tDomain Loss: 1.662174\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [384/4357 (9%)]\tClass Loss: 4.066358\tDomain Loss: 1.667459\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [448/4357 (10%)]\tClass Loss: 4.123651\tDomain Loss: 1.674046\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [512/4357 (12%)]\tClass Loss: 4.100618\tDomain Loss: 1.675535\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [576/4357 (13%)]\tClass Loss: 4.119661\tDomain Loss: 1.672156\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [640/4357 (14%)]\tClass Loss: 4.151092\tDomain Loss: 1.664789\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [704/4357 (16%)]\tClass Loss: 4.135494\tDomain Loss: 1.665160\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [768/4357 (17%)]\tClass Loss: 4.090680\tDomain Loss: 1.660754\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [832/4357 (19%)]\tClass Loss: 4.109908\tDomain Loss: 1.661656\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [896/4357 (20%)]\tClass Loss: 4.081695\tDomain Loss: 1.658349\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [960/4357 (22%)]\tClass Loss: 4.038348\tDomain Loss: 1.648922\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1024/4357 (23%)]\tClass Loss: 4.134996\tDomain Loss: 1.647256\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1088/4357 (25%)]\tClass Loss: 4.136935\tDomain Loss: 1.649760\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1152/4357 (26%)]\tClass Loss: 4.122479\tDomain Loss: 1.644723\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1216/4357 (28%)]\tClass Loss: 4.110538\tDomain Loss: 1.646238\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1280/4357 (29%)]\tClass Loss: 4.092005\tDomain Loss: 1.645100\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1344/4357 (30%)]\tClass Loss: 4.149309\tDomain Loss: 1.645176\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1408/4357 (32%)]\tClass Loss: 4.144469\tDomain Loss: 1.633695\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1472/4357 (33%)]\tClass Loss: 4.106890\tDomain Loss: 1.642606\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1536/4357 (35%)]\tClass Loss: 4.125578\tDomain Loss: 1.626661\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1600/4357 (36%)]\tClass Loss: 4.116716\tDomain Loss: 1.626535\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1664/4357 (38%)]\tClass Loss: 4.118847\tDomain Loss: 1.626525\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1728/4357 (39%)]\tClass Loss: 4.098062\tDomain Loss: 1.626527\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1792/4357 (41%)]\tClass Loss: 4.071294\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1856/4357 (42%)]\tClass Loss: 4.179824\tDomain Loss: 1.626524\tClass Accuracy: 0/64 (0%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1920/4357 (43%)]\tClass Loss: 4.135900\tDomain Loss: 1.626524\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [1984/4357 (45%)]\tClass Loss: 4.136539\tDomain Loss: 1.626524\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2048/4357 (46%)]\tClass Loss: 4.113330\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2112/4357 (48%)]\tClass Loss: 4.069495\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2176/4357 (49%)]\tClass Loss: 4.130553\tDomain Loss: 1.626524\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2240/4357 (51%)]\tClass Loss: 4.157329\tDomain Loss: 1.626524\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2304/4357 (52%)]\tClass Loss: 4.105454\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2368/4357 (54%)]\tClass Loss: 4.107275\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2432/4357 (55%)]\tClass Loss: 4.097723\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2496/4357 (57%)]\tClass Loss: 4.032458\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2560/4357 (58%)]\tClass Loss: 4.098285\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2624/4357 (59%)]\tClass Loss: 4.048472\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2688/4357 (61%)]\tClass Loss: 4.131862\tDomain Loss: 1.626524\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2752/4357 (62%)]\tClass Loss: 4.074830\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2816/4357 (64%)]\tClass Loss: 4.078722\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2880/4357 (65%)]\tClass Loss: 4.087837\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [2944/4357 (67%)]\tClass Loss: 4.156845\tDomain Loss: 1.626524\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3008/4357 (68%)]\tClass Loss: 4.105753\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3072/4357 (70%)]\tClass Loss: 4.065650\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3136/4357 (71%)]\tClass Loss: 4.107232\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3200/4357 (72%)]\tClass Loss: 4.107616\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3264/4357 (74%)]\tClass Loss: 4.068044\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3328/4357 (75%)]\tClass Loss: 4.123169\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3392/4357 (77%)]\tClass Loss: 4.082779\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3456/4357 (78%)]\tClass Loss: 4.160199\tDomain Loss: 1.626524\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3520/4357 (80%)]\tClass Loss: 4.109655\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3584/4357 (81%)]\tClass Loss: 4.117112\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3648/4357 (83%)]\tClass Loss: 4.029488\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3712/4357 (84%)]\tClass Loss: 4.054665\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3776/4357 (86%)]\tClass Loss: 4.121377\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3840/4357 (87%)]\tClass Loss: 4.053208\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3904/4357 (88%)]\tClass Loss: 4.164310\tDomain Loss: 1.626524\tClass Accuracy: 2/64 (3%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [3968/4357 (90%)]\tClass Loss: 4.089694\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [4032/4357 (91%)]\tClass Loss: 4.034075\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [4096/4357 (93%)]\tClass Loss: 4.091557\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [4160/4357 (94%)]\tClass Loss: 4.077869\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [4224/4357 (96%)]\tClass Loss: 4.120088\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [4288/4357 (97%)]\tClass Loss: 4.123272\tDomain Loss: 1.626524\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 1 [340/4357 (99%)]\tClass Loss: 4.196582\tDomain Loss: 1.626523\tClass Accuracy: 0/5 (0%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 2 [0/4357 (0%)]\tClass Loss: 4.047744\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [64/4357 (1%)]\tClass Loss: 4.049740\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [128/4357 (3%)]\tClass Loss: 4.092213\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [192/4357 (4%)]\tClass Loss: 4.058401\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [256/4357 (6%)]\tClass Loss: 4.080975\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [320/4357 (7%)]\tClass Loss: 4.051793\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [384/4357 (9%)]\tClass Loss: 4.002042\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [448/4357 (10%)]\tClass Loss: 4.064411\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [512/4357 (12%)]\tClass Loss: 3.985728\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [576/4357 (13%)]\tClass Loss: 4.052936\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [640/4357 (14%)]\tClass Loss: 4.064601\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [704/4357 (16%)]\tClass Loss: 4.067009\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [768/4357 (17%)]\tClass Loss: 4.050212\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [832/4357 (19%)]\tClass Loss: 4.128412\tDomain Loss: 1.626524\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [896/4357 (20%)]\tClass Loss: 4.116985\tDomain Loss: 1.626524\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [960/4357 (22%)]\tClass Loss: 4.111273\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1024/4357 (23%)]\tClass Loss: 4.093987\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1088/4357 (25%)]\tClass Loss: 4.026244\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1152/4357 (26%)]\tClass Loss: 4.066578\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1216/4357 (28%)]\tClass Loss: 4.083976\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1280/4357 (29%)]\tClass Loss: 4.065226\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1344/4357 (30%)]\tClass Loss: 4.051916\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1408/4357 (32%)]\tClass Loss: 4.068532\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1472/4357 (33%)]\tClass Loss: 4.084572\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1536/4357 (35%)]\tClass Loss: 4.066740\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1600/4357 (36%)]\tClass Loss: 4.061230\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1664/4357 (38%)]\tClass Loss: 4.000797\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1728/4357 (39%)]\tClass Loss: 4.002132\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1792/4357 (41%)]\tClass Loss: 4.106756\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1856/4357 (42%)]\tClass Loss: 3.970502\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1920/4357 (43%)]\tClass Loss: 4.109591\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [1984/4357 (45%)]\tClass Loss: 3.984288\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2048/4357 (46%)]\tClass Loss: 4.011107\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2112/4357 (48%)]\tClass Loss: 4.114159\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2176/4357 (49%)]\tClass Loss: 4.015734\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2240/4357 (51%)]\tClass Loss: 4.106255\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2304/4357 (52%)]\tClass Loss: 4.069499\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2368/4357 (54%)]\tClass Loss: 4.129194\tDomain Loss: 1.626524\tClass Accuracy: 4/64 (6%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2432/4357 (55%)]\tClass Loss: 4.064332\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2496/4357 (57%)]\tClass Loss: 4.057504\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2560/4357 (58%)]\tClass Loss: 4.015298\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2624/4357 (59%)]\tClass Loss: 4.040325\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2688/4357 (61%)]\tClass Loss: 4.028970\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2752/4357 (62%)]\tClass Loss: 4.040359\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2816/4357 (64%)]\tClass Loss: 4.042122\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2880/4357 (65%)]\tClass Loss: 4.036021\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [2944/4357 (67%)]\tClass Loss: 4.047407\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3008/4357 (68%)]\tClass Loss: 4.004551\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3072/4357 (70%)]\tClass Loss: 4.095659\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3136/4357 (71%)]\tClass Loss: 4.095994\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3200/4357 (72%)]\tClass Loss: 4.020458\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3264/4357 (74%)]\tClass Loss: 4.048075\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3328/4357 (75%)]\tClass Loss: 3.996665\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3392/4357 (77%)]\tClass Loss: 4.041549\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3456/4357 (78%)]\tClass Loss: 4.142128\tDomain Loss: 1.626524\tClass Accuracy: 3/64 (5%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3520/4357 (80%)]\tClass Loss: 4.032413\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3584/4357 (81%)]\tClass Loss: 4.027324\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3648/4357 (83%)]\tClass Loss: 4.021466\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3712/4357 (84%)]\tClass Loss: 4.094027\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3776/4357 (86%)]\tClass Loss: 4.014324\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3840/4357 (87%)]\tClass Loss: 3.987073\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3904/4357 (88%)]\tClass Loss: 4.093358\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [3968/4357 (90%)]\tClass Loss: 4.018187\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [4032/4357 (91%)]\tClass Loss: 4.056217\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [4096/4357 (93%)]\tClass Loss: 4.054117\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [4160/4357 (94%)]\tClass Loss: 4.051322\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [4224/4357 (96%)]\tClass Loss: 4.050657\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [4288/4357 (97%)]\tClass Loss: 4.032556\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 2 [340/4357 (99%)]\tClass Loss: 3.996298\tDomain Loss: 1.626523\tClass Accuracy: 1/5 (20%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 3 [0/4357 (0%)]\tClass Loss: 3.953510\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [64/4357 (1%)]\tClass Loss: 3.886686\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [128/4357 (3%)]\tClass Loss: 4.048234\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [192/4357 (4%)]\tClass Loss: 3.920489\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [256/4357 (6%)]\tClass Loss: 3.988714\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [320/4357 (7%)]\tClass Loss: 4.007418\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [384/4357 (9%)]\tClass Loss: 4.005108\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [448/4357 (10%)]\tClass Loss: 4.091064\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [512/4357 (12%)]\tClass Loss: 3.949501\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [576/4357 (13%)]\tClass Loss: 4.027823\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [640/4357 (14%)]\tClass Loss: 4.030437\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [704/4357 (16%)]\tClass Loss: 4.012433\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [768/4357 (17%)]\tClass Loss: 3.972817\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [832/4357 (19%)]\tClass Loss: 4.034898\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [896/4357 (20%)]\tClass Loss: 4.113091\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [960/4357 (22%)]\tClass Loss: 4.087836\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1024/4357 (23%)]\tClass Loss: 3.982408\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1088/4357 (25%)]\tClass Loss: 4.086868\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1152/4357 (26%)]\tClass Loss: 4.001954\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1216/4357 (28%)]\tClass Loss: 3.997063\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1280/4357 (29%)]\tClass Loss: 4.062288\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1344/4357 (30%)]\tClass Loss: 4.037049\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1408/4357 (32%)]\tClass Loss: 4.045335\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1472/4357 (33%)]\tClass Loss: 3.945124\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1536/4357 (35%)]\tClass Loss: 4.056656\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1600/4357 (36%)]\tClass Loss: 3.998621\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1664/4357 (38%)]\tClass Loss: 4.036674\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1728/4357 (39%)]\tClass Loss: 4.052036\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1792/4357 (41%)]\tClass Loss: 3.977221\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1856/4357 (42%)]\tClass Loss: 4.014253\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1920/4357 (43%)]\tClass Loss: 3.998028\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [1984/4357 (45%)]\tClass Loss: 4.008817\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2048/4357 (46%)]\tClass Loss: 3.956976\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2112/4357 (48%)]\tClass Loss: 4.005239\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2176/4357 (49%)]\tClass Loss: 3.990237\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2240/4357 (51%)]\tClass Loss: 4.083958\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2304/4357 (52%)]\tClass Loss: 3.994168\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2368/4357 (54%)]\tClass Loss: 3.996279\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2432/4357 (55%)]\tClass Loss: 4.012784\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2496/4357 (57%)]\tClass Loss: 4.041389\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2560/4357 (58%)]\tClass Loss: 4.053852\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2624/4357 (59%)]\tClass Loss: 4.049017\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2688/4357 (61%)]\tClass Loss: 4.025135\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2752/4357 (62%)]\tClass Loss: 4.050028\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2816/4357 (64%)]\tClass Loss: 3.979936\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2880/4357 (65%)]\tClass Loss: 3.982741\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [2944/4357 (67%)]\tClass Loss: 3.990187\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3008/4357 (68%)]\tClass Loss: 4.002625\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3072/4357 (70%)]\tClass Loss: 3.982214\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3136/4357 (71%)]\tClass Loss: 4.067819\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3200/4357 (72%)]\tClass Loss: 4.001994\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3264/4357 (74%)]\tClass Loss: 3.963178\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3328/4357 (75%)]\tClass Loss: 4.026282\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3392/4357 (77%)]\tClass Loss: 3.806989\tDomain Loss: 1.626524\tClass Accuracy: 26/64 (41%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3456/4357 (78%)]\tClass Loss: 3.981721\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3520/4357 (80%)]\tClass Loss: 3.984296\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3584/4357 (81%)]\tClass Loss: 4.099380\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3648/4357 (83%)]\tClass Loss: 4.083666\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3712/4357 (84%)]\tClass Loss: 3.924319\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3776/4357 (86%)]\tClass Loss: 4.022149\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3840/4357 (87%)]\tClass Loss: 4.007703\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3904/4357 (88%)]\tClass Loss: 4.029064\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [3968/4357 (90%)]\tClass Loss: 4.086929\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [4032/4357 (91%)]\tClass Loss: 4.049984\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [4096/4357 (93%)]\tClass Loss: 4.073185\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [4160/4357 (94%)]\tClass Loss: 3.970935\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [4224/4357 (96%)]\tClass Loss: 4.027553\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [4288/4357 (97%)]\tClass Loss: 4.004642\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 3 [340/4357 (99%)]\tClass Loss: 4.193662\tDomain Loss: 1.626523\tClass Accuracy: 0/5 (0%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 4 [0/4357 (0%)]\tClass Loss: 3.974432\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [64/4357 (1%)]\tClass Loss: 3.966286\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [128/4357 (3%)]\tClass Loss: 3.936103\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [192/4357 (4%)]\tClass Loss: 4.036691\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [256/4357 (6%)]\tClass Loss: 4.015805\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [320/4357 (7%)]\tClass Loss: 4.054066\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [384/4357 (9%)]\tClass Loss: 4.030080\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [448/4357 (10%)]\tClass Loss: 4.070741\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [512/4357 (12%)]\tClass Loss: 4.056652\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [576/4357 (13%)]\tClass Loss: 4.066355\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [640/4357 (14%)]\tClass Loss: 3.966943\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [704/4357 (16%)]\tClass Loss: 3.976322\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [768/4357 (17%)]\tClass Loss: 4.114386\tDomain Loss: 1.626524\tClass Accuracy: 5/64 (8%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [832/4357 (19%)]\tClass Loss: 4.045397\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [896/4357 (20%)]\tClass Loss: 3.970256\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [960/4357 (22%)]\tClass Loss: 4.001076\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1024/4357 (23%)]\tClass Loss: 4.022694\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1088/4357 (25%)]\tClass Loss: 4.021829\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1152/4357 (26%)]\tClass Loss: 3.944547\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1216/4357 (28%)]\tClass Loss: 4.043102\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1280/4357 (29%)]\tClass Loss: 4.062053\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1344/4357 (30%)]\tClass Loss: 4.009704\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1408/4357 (32%)]\tClass Loss: 3.962935\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1472/4357 (33%)]\tClass Loss: 3.991172\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1536/4357 (35%)]\tClass Loss: 4.029838\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1600/4357 (36%)]\tClass Loss: 4.066918\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1664/4357 (38%)]\tClass Loss: 4.053879\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1728/4357 (39%)]\tClass Loss: 4.037878\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1792/4357 (41%)]\tClass Loss: 4.007185\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1856/4357 (42%)]\tClass Loss: 3.901083\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1920/4357 (43%)]\tClass Loss: 4.032937\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [1984/4357 (45%)]\tClass Loss: 3.981202\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2048/4357 (46%)]\tClass Loss: 3.986597\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2112/4357 (48%)]\tClass Loss: 4.065301\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2176/4357 (49%)]\tClass Loss: 3.990452\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2240/4357 (51%)]\tClass Loss: 4.003228\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2304/4357 (52%)]\tClass Loss: 3.988593\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2368/4357 (54%)]\tClass Loss: 4.018102\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2432/4357 (55%)]\tClass Loss: 4.006327\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2496/4357 (57%)]\tClass Loss: 3.954061\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2560/4357 (58%)]\tClass Loss: 3.929084\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2624/4357 (59%)]\tClass Loss: 4.049566\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2688/4357 (61%)]\tClass Loss: 3.996769\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2752/4357 (62%)]\tClass Loss: 3.911157\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2816/4357 (64%)]\tClass Loss: 3.992575\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2880/4357 (65%)]\tClass Loss: 3.929220\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [2944/4357 (67%)]\tClass Loss: 3.973245\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3008/4357 (68%)]\tClass Loss: 3.974163\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3072/4357 (70%)]\tClass Loss: 3.940161\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3136/4357 (71%)]\tClass Loss: 3.983343\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3200/4357 (72%)]\tClass Loss: 3.916395\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3264/4357 (74%)]\tClass Loss: 4.082398\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3328/4357 (75%)]\tClass Loss: 3.984708\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3392/4357 (77%)]\tClass Loss: 4.024655\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3456/4357 (78%)]\tClass Loss: 3.999863\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3520/4357 (80%)]\tClass Loss: 4.018923\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3584/4357 (81%)]\tClass Loss: 4.028918\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3648/4357 (83%)]\tClass Loss: 4.010846\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3712/4357 (84%)]\tClass Loss: 4.009204\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3776/4357 (86%)]\tClass Loss: 3.980378\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3840/4357 (87%)]\tClass Loss: 4.007781\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3904/4357 (88%)]\tClass Loss: 4.001404\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [3968/4357 (90%)]\tClass Loss: 4.047123\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [4032/4357 (91%)]\tClass Loss: 4.042693\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [4096/4357 (93%)]\tClass Loss: 3.992401\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [4160/4357 (94%)]\tClass Loss: 3.995526\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [4224/4357 (96%)]\tClass Loss: 4.023810\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [4288/4357 (97%)]\tClass Loss: 4.090783\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 4 [340/4357 (99%)]\tClass Loss: 4.183615\tDomain Loss: 1.626523\tClass Accuracy: 0/5 (0%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 5 [0/4357 (0%)]\tClass Loss: 3.989766\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [64/4357 (1%)]\tClass Loss: 4.013896\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [128/4357 (3%)]\tClass Loss: 4.020446\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [192/4357 (4%)]\tClass Loss: 4.038431\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [256/4357 (6%)]\tClass Loss: 3.959230\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [320/4357 (7%)]\tClass Loss: 4.049236\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [384/4357 (9%)]\tClass Loss: 3.968478\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [448/4357 (10%)]\tClass Loss: 4.007851\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [512/4357 (12%)]\tClass Loss: 3.939485\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [576/4357 (13%)]\tClass Loss: 3.973426\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [640/4357 (14%)]\tClass Loss: 4.039064\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [704/4357 (16%)]\tClass Loss: 4.020395\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [768/4357 (17%)]\tClass Loss: 3.961558\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [832/4357 (19%)]\tClass Loss: 4.003905\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [896/4357 (20%)]\tClass Loss: 3.987326\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [960/4357 (22%)]\tClass Loss: 3.860879\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1024/4357 (23%)]\tClass Loss: 4.078826\tDomain Loss: 1.626524\tClass Accuracy: 7/64 (11%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1088/4357 (25%)]\tClass Loss: 4.045755\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1152/4357 (26%)]\tClass Loss: 3.974375\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1216/4357 (28%)]\tClass Loss: 4.032300\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1280/4357 (29%)]\tClass Loss: 3.978230\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1344/4357 (30%)]\tClass Loss: 4.030571\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1408/4357 (32%)]\tClass Loss: 3.993923\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1472/4357 (33%)]\tClass Loss: 3.972492\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1536/4357 (35%)]\tClass Loss: 3.983796\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1600/4357 (36%)]\tClass Loss: 4.037986\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1664/4357 (38%)]\tClass Loss: 3.950009\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1728/4357 (39%)]\tClass Loss: 3.939558\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1792/4357 (41%)]\tClass Loss: 3.973356\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1856/4357 (42%)]\tClass Loss: 4.018378\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1920/4357 (43%)]\tClass Loss: 3.996971\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [1984/4357 (45%)]\tClass Loss: 3.922930\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2048/4357 (46%)]\tClass Loss: 3.919869\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2112/4357 (48%)]\tClass Loss: 4.021994\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2176/4357 (49%)]\tClass Loss: 3.989656\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2240/4357 (51%)]\tClass Loss: 4.055983\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2304/4357 (52%)]\tClass Loss: 3.957862\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2368/4357 (54%)]\tClass Loss: 4.032052\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2432/4357 (55%)]\tClass Loss: 3.969043\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2496/4357 (57%)]\tClass Loss: 4.030402\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2560/4357 (58%)]\tClass Loss: 4.019845\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2624/4357 (59%)]\tClass Loss: 3.871430\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2688/4357 (61%)]\tClass Loss: 4.035358\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2752/4357 (62%)]\tClass Loss: 3.913866\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2816/4357 (64%)]\tClass Loss: 3.902554\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2880/4357 (65%)]\tClass Loss: 3.953335\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [2944/4357 (67%)]\tClass Loss: 3.966961\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3008/4357 (68%)]\tClass Loss: 3.913247\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3072/4357 (70%)]\tClass Loss: 3.935596\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3136/4357 (71%)]\tClass Loss: 4.054667\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3200/4357 (72%)]\tClass Loss: 3.983969\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3264/4357 (74%)]\tClass Loss: 3.899926\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3328/4357 (75%)]\tClass Loss: 4.015094\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3392/4357 (77%)]\tClass Loss: 3.874600\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3456/4357 (78%)]\tClass Loss: 4.078014\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3520/4357 (80%)]\tClass Loss: 3.944823\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3584/4357 (81%)]\tClass Loss: 3.958958\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3648/4357 (83%)]\tClass Loss: 3.979074\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3712/4357 (84%)]\tClass Loss: 3.997790\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3776/4357 (86%)]\tClass Loss: 3.958943\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3840/4357 (87%)]\tClass Loss: 3.912790\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3904/4357 (88%)]\tClass Loss: 3.983859\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [3968/4357 (90%)]\tClass Loss: 3.992764\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [4032/4357 (91%)]\tClass Loss: 4.038476\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [4096/4357 (93%)]\tClass Loss: 3.944631\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [4160/4357 (94%)]\tClass Loss: 3.979710\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [4224/4357 (96%)]\tClass Loss: 3.959575\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [4288/4357 (97%)]\tClass Loss: 4.058485\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 5 [340/4357 (99%)]\tClass Loss: 3.997253\tDomain Loss: 1.626523\tClass Accuracy: 1/5 (20%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 6 [0/4357 (0%)]\tClass Loss: 4.009157\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [64/4357 (1%)]\tClass Loss: 3.915106\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [128/4357 (3%)]\tClass Loss: 4.006089\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [192/4357 (4%)]\tClass Loss: 3.895933\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [256/4357 (6%)]\tClass Loss: 4.025228\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [320/4357 (7%)]\tClass Loss: 3.953446\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [384/4357 (9%)]\tClass Loss: 3.978513\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [448/4357 (10%)]\tClass Loss: 4.024875\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [512/4357 (12%)]\tClass Loss: 3.987491\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [576/4357 (13%)]\tClass Loss: 3.907612\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [640/4357 (14%)]\tClass Loss: 4.007665\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [704/4357 (16%)]\tClass Loss: 4.026371\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [768/4357 (17%)]\tClass Loss: 4.035893\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [832/4357 (19%)]\tClass Loss: 3.977097\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [896/4357 (20%)]\tClass Loss: 3.929780\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [960/4357 (22%)]\tClass Loss: 3.990223\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1024/4357 (23%)]\tClass Loss: 4.000853\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1088/4357 (25%)]\tClass Loss: 3.890584\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1152/4357 (26%)]\tClass Loss: 3.994833\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1216/4357 (28%)]\tClass Loss: 3.972842\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1280/4357 (29%)]\tClass Loss: 3.888270\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1344/4357 (30%)]\tClass Loss: 3.966321\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1408/4357 (32%)]\tClass Loss: 4.021097\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1472/4357 (33%)]\tClass Loss: 3.955554\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1536/4357 (35%)]\tClass Loss: 3.924581\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1600/4357 (36%)]\tClass Loss: 3.919997\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1664/4357 (38%)]\tClass Loss: 3.972241\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1728/4357 (39%)]\tClass Loss: 3.968456\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1792/4357 (41%)]\tClass Loss: 3.864858\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1856/4357 (42%)]\tClass Loss: 3.922945\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1920/4357 (43%)]\tClass Loss: 3.998810\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [1984/4357 (45%)]\tClass Loss: 4.000431\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2048/4357 (46%)]\tClass Loss: 3.943298\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2112/4357 (48%)]\tClass Loss: 3.912195\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2176/4357 (49%)]\tClass Loss: 3.992941\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2240/4357 (51%)]\tClass Loss: 3.959460\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2304/4357 (52%)]\tClass Loss: 4.075038\tDomain Loss: 1.626524\tClass Accuracy: 8/64 (12%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2368/4357 (54%)]\tClass Loss: 3.995563\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2432/4357 (55%)]\tClass Loss: 3.953269\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2496/4357 (57%)]\tClass Loss: 3.968905\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2560/4357 (58%)]\tClass Loss: 3.872283\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2624/4357 (59%)]\tClass Loss: 3.962164\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2688/4357 (61%)]\tClass Loss: 3.966471\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2752/4357 (62%)]\tClass Loss: 3.894927\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2816/4357 (64%)]\tClass Loss: 3.924227\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2880/4357 (65%)]\tClass Loss: 3.942037\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [2944/4357 (67%)]\tClass Loss: 4.006922\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3008/4357 (68%)]\tClass Loss: 3.991808\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3072/4357 (70%)]\tClass Loss: 3.875830\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3136/4357 (71%)]\tClass Loss: 3.945751\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3200/4357 (72%)]\tClass Loss: 3.959489\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3264/4357 (74%)]\tClass Loss: 4.025630\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3328/4357 (75%)]\tClass Loss: 3.963332\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3392/4357 (77%)]\tClass Loss: 3.942025\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3456/4357 (78%)]\tClass Loss: 3.978928\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3520/4357 (80%)]\tClass Loss: 3.973205\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3584/4357 (81%)]\tClass Loss: 3.847894\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3648/4357 (83%)]\tClass Loss: 4.045357\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3712/4357 (84%)]\tClass Loss: 3.976257\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3776/4357 (86%)]\tClass Loss: 3.968578\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3840/4357 (87%)]\tClass Loss: 3.924556\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3904/4357 (88%)]\tClass Loss: 3.912073\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [3968/4357 (90%)]\tClass Loss: 3.931992\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [4032/4357 (91%)]\tClass Loss: 3.983342\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [4096/4357 (93%)]\tClass Loss: 3.998334\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [4160/4357 (94%)]\tClass Loss: 3.854379\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [4224/4357 (96%)]\tClass Loss: 3.952455\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [4288/4357 (97%)]\tClass Loss: 3.937075\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 6 [340/4357 (99%)]\tClass Loss: 4.039942\tDomain Loss: 1.626523\tClass Accuracy: 1/5 (20%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 7 [0/4357 (0%)]\tClass Loss: 3.892204\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [64/4357 (1%)]\tClass Loss: 4.038490\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [128/4357 (3%)]\tClass Loss: 3.962595\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [192/4357 (4%)]\tClass Loss: 4.016158\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [256/4357 (6%)]\tClass Loss: 3.885263\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [320/4357 (7%)]\tClass Loss: 3.964549\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [384/4357 (9%)]\tClass Loss: 3.881343\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [448/4357 (10%)]\tClass Loss: 3.969114\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [512/4357 (12%)]\tClass Loss: 3.936795\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [576/4357 (13%)]\tClass Loss: 3.948668\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [640/4357 (14%)]\tClass Loss: 3.877125\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [704/4357 (16%)]\tClass Loss: 4.057568\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [768/4357 (17%)]\tClass Loss: 3.989271\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [832/4357 (19%)]\tClass Loss: 3.981809\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [896/4357 (20%)]\tClass Loss: 4.099331\tDomain Loss: 1.626524\tClass Accuracy: 6/64 (9%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [960/4357 (22%)]\tClass Loss: 3.919420\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1024/4357 (23%)]\tClass Loss: 3.961994\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1088/4357 (25%)]\tClass Loss: 3.959356\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1152/4357 (26%)]\tClass Loss: 3.923645\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1216/4357 (28%)]\tClass Loss: 3.928018\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1280/4357 (29%)]\tClass Loss: 3.947656\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1344/4357 (30%)]\tClass Loss: 3.967647\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1408/4357 (32%)]\tClass Loss: 3.861782\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1472/4357 (33%)]\tClass Loss: 3.982924\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1536/4357 (35%)]\tClass Loss: 3.950247\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1600/4357 (36%)]\tClass Loss: 3.984758\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1664/4357 (38%)]\tClass Loss: 3.854602\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1728/4357 (39%)]\tClass Loss: 3.977964\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1792/4357 (41%)]\tClass Loss: 3.895919\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1856/4357 (42%)]\tClass Loss: 4.017408\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1920/4357 (43%)]\tClass Loss: 3.956516\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [1984/4357 (45%)]\tClass Loss: 3.943360\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2048/4357 (46%)]\tClass Loss: 3.943326\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2112/4357 (48%)]\tClass Loss: 4.013716\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2176/4357 (49%)]\tClass Loss: 4.009707\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2240/4357 (51%)]\tClass Loss: 3.943000\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2304/4357 (52%)]\tClass Loss: 3.889340\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2368/4357 (54%)]\tClass Loss: 3.966382\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2432/4357 (55%)]\tClass Loss: 3.931782\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2496/4357 (57%)]\tClass Loss: 3.965436\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2560/4357 (58%)]\tClass Loss: 4.039760\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2624/4357 (59%)]\tClass Loss: 3.997332\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2688/4357 (61%)]\tClass Loss: 3.915180\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2752/4357 (62%)]\tClass Loss: 3.848444\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2816/4357 (64%)]\tClass Loss: 4.007018\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2880/4357 (65%)]\tClass Loss: 3.967911\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [2944/4357 (67%)]\tClass Loss: 4.048439\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3008/4357 (68%)]\tClass Loss: 3.904843\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3072/4357 (70%)]\tClass Loss: 3.874036\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3136/4357 (71%)]\tClass Loss: 3.945722\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3200/4357 (72%)]\tClass Loss: 3.891145\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3264/4357 (74%)]\tClass Loss: 3.951433\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3328/4357 (75%)]\tClass Loss: 3.935052\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3392/4357 (77%)]\tClass Loss: 3.934319\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3456/4357 (78%)]\tClass Loss: 3.997327\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3520/4357 (80%)]\tClass Loss: 3.900498\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3584/4357 (81%)]\tClass Loss: 4.024092\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3648/4357 (83%)]\tClass Loss: 3.916564\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3712/4357 (84%)]\tClass Loss: 3.912982\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3776/4357 (86%)]\tClass Loss: 3.949851\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3840/4357 (87%)]\tClass Loss: 3.960534\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3904/4357 (88%)]\tClass Loss: 3.899345\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [3968/4357 (90%)]\tClass Loss: 3.891897\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [4032/4357 (91%)]\tClass Loss: 3.904192\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [4096/4357 (93%)]\tClass Loss: 3.980033\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [4160/4357 (94%)]\tClass Loss: 3.920296\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [4224/4357 (96%)]\tClass Loss: 4.001300\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [4288/4357 (97%)]\tClass Loss: 4.014750\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 7 [340/4357 (99%)]\tClass Loss: 3.648088\tDomain Loss: 1.626523\tClass Accuracy: 3/5 (60%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 8 [0/4357 (0%)]\tClass Loss: 3.842434\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [64/4357 (1%)]\tClass Loss: 3.913046\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [128/4357 (3%)]\tClass Loss: 3.986564\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [192/4357 (4%)]\tClass Loss: 3.789141\tDomain Loss: 1.626524\tClass Accuracy: 27/64 (42%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [256/4357 (6%)]\tClass Loss: 3.911723\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [320/4357 (7%)]\tClass Loss: 4.044725\tDomain Loss: 1.626524\tClass Accuracy: 10/64 (16%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [384/4357 (9%)]\tClass Loss: 3.928974\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [448/4357 (10%)]\tClass Loss: 3.887542\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [512/4357 (12%)]\tClass Loss: 3.975068\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [576/4357 (13%)]\tClass Loss: 3.925870\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [640/4357 (14%)]\tClass Loss: 3.928921\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [704/4357 (16%)]\tClass Loss: 3.865017\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [768/4357 (17%)]\tClass Loss: 3.869322\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [832/4357 (19%)]\tClass Loss: 3.904700\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [896/4357 (20%)]\tClass Loss: 3.947377\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [960/4357 (22%)]\tClass Loss: 3.890468\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1024/4357 (23%)]\tClass Loss: 3.944073\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1088/4357 (25%)]\tClass Loss: 3.919551\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1152/4357 (26%)]\tClass Loss: 3.967779\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1216/4357 (28%)]\tClass Loss: 3.925287\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1280/4357 (29%)]\tClass Loss: 3.930783\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1344/4357 (30%)]\tClass Loss: 3.905682\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1408/4357 (32%)]\tClass Loss: 3.817367\tDomain Loss: 1.626524\tClass Accuracy: 25/64 (39%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1472/4357 (33%)]\tClass Loss: 3.860848\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1536/4357 (35%)]\tClass Loss: 3.926785\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1600/4357 (36%)]\tClass Loss: 3.935045\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1664/4357 (38%)]\tClass Loss: 3.913708\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1728/4357 (39%)]\tClass Loss: 4.002520\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1792/4357 (41%)]\tClass Loss: 4.030506\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1856/4357 (42%)]\tClass Loss: 3.890465\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1920/4357 (43%)]\tClass Loss: 3.950624\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [1984/4357 (45%)]\tClass Loss: 3.999335\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2048/4357 (46%)]\tClass Loss: 3.921177\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2112/4357 (48%)]\tClass Loss: 3.966793\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2176/4357 (49%)]\tClass Loss: 3.956690\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2240/4357 (51%)]\tClass Loss: 3.931581\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2304/4357 (52%)]\tClass Loss: 3.950073\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2368/4357 (54%)]\tClass Loss: 3.821920\tDomain Loss: 1.626524\tClass Accuracy: 25/64 (39%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2432/4357 (55%)]\tClass Loss: 3.862720\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2496/4357 (57%)]\tClass Loss: 4.051385\tDomain Loss: 1.626524\tClass Accuracy: 9/64 (14%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2560/4357 (58%)]\tClass Loss: 3.837356\tDomain Loss: 1.626524\tClass Accuracy: 24/64 (38%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2624/4357 (59%)]\tClass Loss: 3.852572\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2688/4357 (61%)]\tClass Loss: 3.942496\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2752/4357 (62%)]\tClass Loss: 3.950217\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2816/4357 (64%)]\tClass Loss: 3.921213\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2880/4357 (65%)]\tClass Loss: 3.985053\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [2944/4357 (67%)]\tClass Loss: 4.040726\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3008/4357 (68%)]\tClass Loss: 3.934544\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3072/4357 (70%)]\tClass Loss: 3.957780\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3136/4357 (71%)]\tClass Loss: 4.008455\tDomain Loss: 1.626524\tClass Accuracy: 12/64 (19%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3200/4357 (72%)]\tClass Loss: 3.953748\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3264/4357 (74%)]\tClass Loss: 3.880084\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3328/4357 (75%)]\tClass Loss: 3.919643\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3392/4357 (77%)]\tClass Loss: 3.966151\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3456/4357 (78%)]\tClass Loss: 3.940560\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3520/4357 (80%)]\tClass Loss: 3.931662\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3584/4357 (81%)]\tClass Loss: 3.947924\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3648/4357 (83%)]\tClass Loss: 3.908681\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3712/4357 (84%)]\tClass Loss: 3.904978\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3776/4357 (86%)]\tClass Loss: 3.872785\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3840/4357 (87%)]\tClass Loss: 3.971980\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3904/4357 (88%)]\tClass Loss: 3.881060\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [3968/4357 (90%)]\tClass Loss: 4.014606\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [4032/4357 (91%)]\tClass Loss: 3.925577\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [4096/4357 (93%)]\tClass Loss: 3.992060\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [4160/4357 (94%)]\tClass Loss: 3.838364\tDomain Loss: 1.626524\tClass Accuracy: 24/64 (38%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [4224/4357 (96%)]\tClass Loss: 3.950420\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [4288/4357 (97%)]\tClass Loss: 3.890959\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 8 [340/4357 (99%)]\tClass Loss: 4.198407\tDomain Loss: 1.626523\tClass Accuracy: 0/5 (0%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Epoch: 9 [0/4357 (0%)]\tClass Loss: 3.875502\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [64/4357 (1%)]\tClass Loss: 3.932101\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [128/4357 (3%)]\tClass Loss: 3.940727\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [192/4357 (4%)]\tClass Loss: 3.862249\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [256/4357 (6%)]\tClass Loss: 3.982644\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [320/4357 (7%)]\tClass Loss: 3.957661\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [384/4357 (9%)]\tClass Loss: 3.928551\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [448/4357 (10%)]\tClass Loss: 3.926458\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [512/4357 (12%)]\tClass Loss: 3.972705\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [576/4357 (13%)]\tClass Loss: 3.969792\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [640/4357 (14%)]\tClass Loss: 3.979312\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [704/4357 (16%)]\tClass Loss: 3.894336\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [768/4357 (17%)]\tClass Loss: 3.936972\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [832/4357 (19%)]\tClass Loss: 3.930712\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [896/4357 (20%)]\tClass Loss: 3.864807\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [960/4357 (22%)]\tClass Loss: 3.933868\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1024/4357 (23%)]\tClass Loss: 3.922119\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1088/4357 (25%)]\tClass Loss: 3.821510\tDomain Loss: 1.626524\tClass Accuracy: 24/64 (38%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1152/4357 (26%)]\tClass Loss: 3.838357\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1216/4357 (28%)]\tClass Loss: 3.928122\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1280/4357 (29%)]\tClass Loss: 3.896055\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1344/4357 (30%)]\tClass Loss: 3.809665\tDomain Loss: 1.626524\tClass Accuracy: 26/64 (41%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1408/4357 (32%)]\tClass Loss: 3.966962\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1472/4357 (33%)]\tClass Loss: 3.894087\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1536/4357 (35%)]\tClass Loss: 3.973052\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1600/4357 (36%)]\tClass Loss: 3.922607\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1664/4357 (38%)]\tClass Loss: 3.935602\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1728/4357 (39%)]\tClass Loss: 3.898429\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1792/4357 (41%)]\tClass Loss: 3.929900\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1856/4357 (42%)]\tClass Loss: 3.865532\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1920/4357 (43%)]\tClass Loss: 3.904253\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [1984/4357 (45%)]\tClass Loss: 3.946320\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2048/4357 (46%)]\tClass Loss: 3.904568\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2112/4357 (48%)]\tClass Loss: 3.866977\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2176/4357 (49%)]\tClass Loss: 3.975184\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2240/4357 (51%)]\tClass Loss: 3.881178\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2304/4357 (52%)]\tClass Loss: 3.941326\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2368/4357 (54%)]\tClass Loss: 3.938432\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2432/4357 (55%)]\tClass Loss: 3.897212\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2496/4357 (57%)]\tClass Loss: 4.024652\tDomain Loss: 1.626524\tClass Accuracy: 11/64 (17%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2560/4357 (58%)]\tClass Loss: 3.973708\tDomain Loss: 1.626524\tClass Accuracy: 15/64 (23%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2624/4357 (59%)]\tClass Loss: 3.916927\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2688/4357 (61%)]\tClass Loss: 3.883098\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2752/4357 (62%)]\tClass Loss: 3.860742\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2816/4357 (64%)]\tClass Loss: 3.947431\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2880/4357 (65%)]\tClass Loss: 3.955727\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [2944/4357 (67%)]\tClass Loss: 3.990736\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3008/4357 (68%)]\tClass Loss: 3.892540\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3072/4357 (70%)]\tClass Loss: 3.929128\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3136/4357 (71%)]\tClass Loss: 3.932496\tDomain Loss: 1.626524\tClass Accuracy: 17/64 (27%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3200/4357 (72%)]\tClass Loss: 3.912939\tDomain Loss: 1.626524\tClass Accuracy: 19/64 (30%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3264/4357 (74%)]\tClass Loss: 3.894910\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3328/4357 (75%)]\tClass Loss: 3.969757\tDomain Loss: 1.626524\tClass Accuracy: 14/64 (22%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3392/4357 (77%)]\tClass Loss: 3.928501\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3456/4357 (78%)]\tClass Loss: 3.796394\tDomain Loss: 1.626524\tClass Accuracy: 24/64 (38%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3520/4357 (80%)]\tClass Loss: 3.902853\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3584/4357 (81%)]\tClass Loss: 3.951675\tDomain Loss: 1.626524\tClass Accuracy: 16/64 (25%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3648/4357 (83%)]\tClass Loss: 3.900850\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3712/4357 (84%)]\tClass Loss: 3.912346\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3776/4357 (86%)]\tClass Loss: 3.993746\tDomain Loss: 1.626524\tClass Accuracy: 13/64 (20%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3840/4357 (87%)]\tClass Loss: 3.861159\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3904/4357 (88%)]\tClass Loss: 3.883407\tDomain Loss: 1.626524\tClass Accuracy: 21/64 (33%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [3968/4357 (90%)]\tClass Loss: 3.855814\tDomain Loss: 1.626524\tClass Accuracy: 23/64 (36%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [4032/4357 (91%)]\tClass Loss: 3.871442\tDomain Loss: 1.626524\tClass Accuracy: 22/64 (34%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [4096/4357 (93%)]\tClass Loss: 3.920612\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [4160/4357 (94%)]\tClass Loss: 3.847497\tDomain Loss: 1.626524\tClass Accuracy: 24/64 (38%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [4224/4357 (96%)]\tClass Loss: 3.895035\tDomain Loss: 1.626524\tClass Accuracy: 20/64 (31%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [4288/4357 (97%)]\tClass Loss: 3.931257\tDomain Loss: 1.626524\tClass Accuracy: 18/64 (28%)\tDomain Accuracy: 64/128 (50%)\n",
      "Train Epoch: 9 [340/4357 (99%)]\tClass Loss: 4.182424\tDomain Loss: 1.626523\tClass Accuracy: 0/5 (0%)\tDomain Accuracy: 5/10 (50%)\n",
      "Train Accuracy Class Classifier: 8555/43570 (20%)\n",
      "Train Accuracy Domain Classifier: 43194/87140 (50%)\n",
      "Best Accuracy: 50% at Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#test the function on the models and data\n",
    "feature_extractor, class_classifier, domain_classifier = DANN_training(feature_extractor, class_classifier, domain_classifier,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5787/1062286346.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n",
      "/tmp/ipykernel_5787/2842852021.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET_Test Accuracy Class Classifier: 428/4365 (10%)\n",
      "TARGET_Test Accuracy Domain Classifier: 4365/4365 (100%)\n"
     ]
    }
   ],
   "source": [
    "#test DANN on the test data\n",
    "DANN_test(feature_extractor, class_classifier, domain_classifier, test_loader, dataset='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5787/1062286346.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n",
      "/tmp/ipykernel_5787/2842852021.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE_Test Accuracy Class Classifier: 1340/4357 (31%)\n",
      "SOURCE_Test Accuracy Domain Classifier: 0/4357 (0%)\n"
     ]
    }
   ],
   "source": [
    "#test  DANN predictions on train data\n",
    "DANN_test(feature_extractor, class_classifier, domain_classifier, train_loader, dataset='SOURCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.___torch_mangle_442.FeatureExtractor,\n",
      "      %x : Float(1, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cuda:0)):\n",
      "  %feature_extractor : __torch__.torch.nn.modules.container.___torch_mangle_441.Sequential = prim::GetAttr[name=\"feature_extractor\"](%self.1)\n",
      "  %2966 : float = prim::Constant[value=0.10000000000000001](), scope: __module.feature_extractor/__module.feature_extractor.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %2967 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.feature_extractor/__module.feature_extractor.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %2968 : NoneType = prim::Constant(), scope: __module.feature_extractor/__module.feature_extractor.0\n",
      "  %2969 : int = prim::Constant[value=2](), scope: __module.feature_extractor/__module.feature_extractor.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %2970 : int = prim::Constant[value=3](), scope: __module.feature_extractor/__module.feature_extractor.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %2971 : int = prim::Constant[value=1](), scope: __module.feature_extractor/__module.feature_extractor.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %2972 : bool = prim::Constant[value=0](), scope: __module.feature_extractor/__module.feature_extractor.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %2973 : int = prim::Constant[value=0](), scope: __module.feature_extractor/__module.feature_extractor.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %2974 : bool = prim::Constant[value=1](), scope: __module.feature_extractor/__module.feature_extractor.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %_7 : __torch__.torch.nn.modules.container.___torch_mangle_440.Sequential = prim::GetAttr[name=\"7\"](%feature_extractor)\n",
      "  %_6 : __torch__.torch.nn.modules.container.___torch_mangle_412.Sequential = prim::GetAttr[name=\"6\"](%feature_extractor)\n",
      "  %_5.1 : __torch__.torch.nn.modules.container.___torch_mangle_360.Sequential = prim::GetAttr[name=\"5\"](%feature_extractor)\n",
      "  %_4.1 : __torch__.torch.nn.modules.container.___torch_mangle_324.Sequential = prim::GetAttr[name=\"4\"](%feature_extractor)\n",
      "  %_3.1 : __torch__.torch.nn.modules.pooling.___torch_mangle_296.MaxPool2d = prim::GetAttr[name=\"3\"](%feature_extractor)\n",
      "  %_2.1 : __torch__.torch.nn.modules.activation.___torch_mangle_295.ReLU = prim::GetAttr[name=\"2\"](%feature_extractor)\n",
      "  %_1.1 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_294.BatchNorm2d = prim::GetAttr[name=\"1\"](%feature_extractor)\n",
      "  %_0.1 : __torch__.torch.nn.modules.conv.___torch_mangle_293.Conv2d = prim::GetAttr[name=\"0\"](%feature_extractor)\n",
      "  %weight.213 : Tensor = prim::GetAttr[name=\"weight\"](%_0.1)\n",
      "  %2984 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.0\n",
      "  %2985 : int[] = prim::ListConstruct(%2970, %2970), scope: __module.feature_extractor/__module.feature_extractor.0\n",
      "  %2986 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.0\n",
      "  %2987 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.0\n",
      "  %input.1 : Float(1, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%x, %weight.213, %2968, %2984, %2985, %2986, %2972, %2987, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.107 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.1)\n",
      "  %running_mean.107 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.1)\n",
      "  %bias.107 : Tensor = prim::GetAttr[name=\"bias\"](%_1.1)\n",
      "  %weight.215 : Tensor = prim::GetAttr[name=\"weight\"](%_1.1)\n",
      "  %input.3 : Float(1, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.1, %weight.215, %bias.107, %running_mean.107, %running_var.107, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.5 : Float(1, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.3), scope: __module.feature_extractor/__module.feature_extractor.2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %2995 : int[] = prim::ListConstruct(%2970, %2970), scope: __module.feature_extractor/__module.feature_extractor.3\n",
      "  %2996 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.3\n",
      "  %2997 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.3\n",
      "  %2998 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.3\n",
      "  %input.7 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%input.5, %2995, %2996, %2997, %2998, %2972), scope: __module.feature_extractor/__module.feature_extractor.3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:782:0\n",
      "  %_2.3 : __torch__.torchvision.models.resnet.___torch_mangle_323.Bottleneck = prim::GetAttr[name=\"2\"](%_4.1)\n",
      "  %_1.5 : __torch__.torchvision.models.resnet.___torch_mangle_315.Bottleneck = prim::GetAttr[name=\"1\"](%_4.1)\n",
      "  %_0.5 : __torch__.torchvision.models.resnet.___torch_mangle_307.Bottleneck = prim::GetAttr[name=\"0\"](%_4.1)\n",
      "  %downsample.1 : __torch__.torch.nn.modules.container.___torch_mangle_306.Sequential = prim::GetAttr[name=\"downsample\"](%_0.5)\n",
      "  %bn3.1 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_302.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_0.5)\n",
      "  %conv3.1 : __torch__.torch.nn.modules.conv.___torch_mangle_301.Conv2d = prim::GetAttr[name=\"conv3\"](%_0.5)\n",
      "  %bn2.1 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_300.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0.5)\n",
      "  %conv2.1 : __torch__.torch.nn.modules.conv.___torch_mangle_299.Conv2d = prim::GetAttr[name=\"conv2\"](%_0.5)\n",
      "  %relu.1 : __torch__.torch.nn.modules.activation.___torch_mangle_303.ReLU = prim::GetAttr[name=\"relu\"](%_0.5)\n",
      "  %bn1.1 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_298.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0.5)\n",
      "  %conv1.1 : __torch__.torch.nn.modules.conv.___torch_mangle_297.Conv2d = prim::GetAttr[name=\"conv1\"](%_0.5)\n",
      "  %weight.217 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.1)\n",
      "  %3012 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv1\n",
      "  %3013 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv1\n",
      "  %3014 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv1\n",
      "  %3015 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv1\n",
      "  %input.9 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.7, %weight.217, %2968, %3012, %3013, %3014, %2972, %3015, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.109 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.1)\n",
      "  %running_mean.109 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.1)\n",
      "  %bias.109 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.1)\n",
      "  %weight.219 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.1)\n",
      "  %input.11 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.9, %weight.219, %bias.109, %running_mean.109, %running_var.109, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.13 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.11), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.221 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.1)\n",
      "  %3024 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv2\n",
      "  %3025 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv2\n",
      "  %3026 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv2\n",
      "  %3027 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv2\n",
      "  %input.15 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.13, %weight.221, %2968, %3024, %3025, %3026, %2972, %3027, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.111 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.1)\n",
      "  %running_mean.111 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.1)\n",
      "  %bias.111 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.1)\n",
      "  %weight.223 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.1)\n",
      "  %input.17 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.15, %weight.223, %bias.111, %running_mean.111, %running_var.111, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.19 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.17), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.225 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.1)\n",
      "  %3036 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv3\n",
      "  %3037 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv3\n",
      "  %3038 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv3\n",
      "  %3039 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv3\n",
      "  %input.21 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.19, %weight.225, %2968, %3036, %3037, %3038, %2972, %3039, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.113 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.1)\n",
      "  %running_mean.113 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.1)\n",
      "  %bias.113 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.1)\n",
      "  %weight.227 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.1)\n",
      "  %out.1 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.21, %weight.227, %bias.113, %running_mean.113, %running_var.113, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %_1.3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_305.BatchNorm2d = prim::GetAttr[name=\"1\"](%downsample.1)\n",
      "  %_0.3 : __torch__.torch.nn.modules.conv.___torch_mangle_304.Conv2d = prim::GetAttr[name=\"0\"](%downsample.1)\n",
      "  %weight.229 : Tensor = prim::GetAttr[name=\"weight\"](%_0.3)\n",
      "  %3049 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.downsample/__module.feature_extractor.4.0.downsample.0\n",
      "  %3050 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.downsample/__module.feature_extractor.4.0.downsample.0\n",
      "  %3051 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.downsample/__module.feature_extractor.4.0.downsample.0\n",
      "  %3052 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.downsample/__module.feature_extractor.4.0.downsample.0\n",
      "  %input.23 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.7, %weight.229, %2968, %3049, %3050, %3051, %2972, %3052, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.downsample/__module.feature_extractor.4.0.downsample.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.115 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.3)\n",
      "  %running_mean.115 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.3)\n",
      "  %bias.115 : Tensor = prim::GetAttr[name=\"bias\"](%_1.3)\n",
      "  %weight.231 : Tensor = prim::GetAttr[name=\"weight\"](%_1.3)\n",
      "  %identity.1 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.23, %weight.231, %bias.115, %running_mean.115, %running_var.115, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.downsample/__module.feature_extractor.4.0.downsample.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.25 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.1, %identity.1, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.27 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.25), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.0/__module.feature_extractor.4.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_313.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_1.5)\n",
      "  %conv3.3 : __torch__.torch.nn.modules.conv.___torch_mangle_312.Conv2d = prim::GetAttr[name=\"conv3\"](%_1.5)\n",
      "  %bn2.3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_311.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1.5)\n",
      "  %conv2.3 : __torch__.torch.nn.modules.conv.___torch_mangle_310.Conv2d = prim::GetAttr[name=\"conv2\"](%_1.5)\n",
      "  %relu.3 : __torch__.torch.nn.modules.activation.___torch_mangle_314.ReLU = prim::GetAttr[name=\"relu\"](%_1.5)\n",
      "  %bn1.3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_309.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1.5)\n",
      "  %conv1.3 : __torch__.torch.nn.modules.conv.___torch_mangle_308.Conv2d = prim::GetAttr[name=\"conv1\"](%_1.5)\n",
      "  %weight.233 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.3)\n",
      "  %3069 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv1\n",
      "  %3070 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv1\n",
      "  %3071 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv1\n",
      "  %3072 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv1\n",
      "  %input.29 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.27, %weight.233, %2968, %3069, %3070, %3071, %2972, %3072, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.117 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.3)\n",
      "  %running_mean.117 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.3)\n",
      "  %bias.117 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.3)\n",
      "  %weight.235 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.3)\n",
      "  %input.31 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.29, %weight.235, %bias.117, %running_mean.117, %running_var.117, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.33 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.31), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.237 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.3)\n",
      "  %3081 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv2\n",
      "  %3082 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv2\n",
      "  %3083 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv2\n",
      "  %3084 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv2\n",
      "  %input.35 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.33, %weight.237, %2968, %3081, %3082, %3083, %2972, %3084, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.119 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.3)\n",
      "  %running_mean.119 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.3)\n",
      "  %bias.119 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.3)\n",
      "  %weight.239 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.3)\n",
      "  %input.37 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.35, %weight.239, %bias.119, %running_mean.119, %running_var.119, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.39 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.37), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.241 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.3)\n",
      "  %3093 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv3\n",
      "  %3094 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv3\n",
      "  %3095 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv3\n",
      "  %3096 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv3\n",
      "  %input.41 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.39, %weight.241, %2968, %3093, %3094, %3095, %2972, %3096, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.121 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.3)\n",
      "  %running_mean.121 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.3)\n",
      "  %bias.121 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.3)\n",
      "  %weight.243 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.3)\n",
      "  %out.3 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.41, %weight.243, %bias.121, %running_mean.121, %running_var.121, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.43 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.3, %input.27, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.45 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.43), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.1/__module.feature_extractor.4.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.5 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_321.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_2.3)\n",
      "  %conv3.5 : __torch__.torch.nn.modules.conv.___torch_mangle_320.Conv2d = prim::GetAttr[name=\"conv3\"](%_2.3)\n",
      "  %bn2.5 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_319.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_2.3)\n",
      "  %conv2.5 : __torch__.torch.nn.modules.conv.___torch_mangle_318.Conv2d = prim::GetAttr[name=\"conv2\"](%_2.3)\n",
      "  %relu.5 : __torch__.torch.nn.modules.activation.___torch_mangle_322.ReLU = prim::GetAttr[name=\"relu\"](%_2.3)\n",
      "  %bn1.5 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_317.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_2.3)\n",
      "  %conv1.5 : __torch__.torch.nn.modules.conv.___torch_mangle_316.Conv2d = prim::GetAttr[name=\"conv1\"](%_2.3)\n",
      "  %weight.245 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.5)\n",
      "  %3113 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv1\n",
      "  %3114 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv1\n",
      "  %3115 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv1\n",
      "  %3116 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv1\n",
      "  %input.47 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.45, %weight.245, %2968, %3113, %3114, %3115, %2972, %3116, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.123 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.5)\n",
      "  %running_mean.123 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.5)\n",
      "  %bias.123 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.5)\n",
      "  %weight.247 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.5)\n",
      "  %input.49 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.47, %weight.247, %bias.123, %running_mean.123, %running_var.123, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.51 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.49), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.249 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.5)\n",
      "  %3125 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv2\n",
      "  %3126 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv2\n",
      "  %3127 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv2\n",
      "  %3128 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv2\n",
      "  %input.53 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.51, %weight.249, %2968, %3125, %3126, %3127, %2972, %3128, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.125 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.5)\n",
      "  %running_mean.125 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.5)\n",
      "  %bias.125 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.5)\n",
      "  %weight.251 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.5)\n",
      "  %input.55 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.53, %weight.251, %bias.125, %running_mean.125, %running_var.125, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.57 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.55), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.253 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.5)\n",
      "  %3137 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv3\n",
      "  %3138 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv3\n",
      "  %3139 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv3\n",
      "  %3140 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv3\n",
      "  %input.59 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.57, %weight.253, %2968, %3137, %3138, %3139, %2972, %3140, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.127 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.5)\n",
      "  %running_mean.127 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.5)\n",
      "  %bias.127 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.5)\n",
      "  %weight.255 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.5)\n",
      "  %out.5 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.59, %weight.255, %bias.127, %running_mean.127, %running_var.127, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.61 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.5, %input.45, %2971), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.63 : Float(1, 256, 56, 56, strides=[802816, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.61), scope: __module.feature_extractor/__module.feature_extractor.4/__module.feature_extractor.4.2/__module.feature_extractor.4.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %_3.3 : __torch__.torchvision.models.resnet.___torch_mangle_359.Bottleneck = prim::GetAttr[name=\"3\"](%_5.1)\n",
      "  %_2.5 : __torch__.torchvision.models.resnet.___torch_mangle_351.Bottleneck = prim::GetAttr[name=\"2\"](%_5.1)\n",
      "  %_1.9 : __torch__.torchvision.models.resnet.___torch_mangle_343.Bottleneck = prim::GetAttr[name=\"1\"](%_5.1)\n",
      "  %_0.9 : __torch__.torchvision.models.resnet.___torch_mangle_335.Bottleneck = prim::GetAttr[name=\"0\"](%_5.1)\n",
      "  %downsample.3 : __torch__.torch.nn.modules.container.___torch_mangle_334.Sequential = prim::GetAttr[name=\"downsample\"](%_0.9)\n",
      "  %bn3.7 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_330.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_0.9)\n",
      "  %conv3.7 : __torch__.torch.nn.modules.conv.___torch_mangle_329.Conv2d = prim::GetAttr[name=\"conv3\"](%_0.9)\n",
      "  %bn2.7 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_328.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0.9)\n",
      "  %conv2.7 : __torch__.torch.nn.modules.conv.___torch_mangle_327.Conv2d = prim::GetAttr[name=\"conv2\"](%_0.9)\n",
      "  %relu.7 : __torch__.torch.nn.modules.activation.___torch_mangle_331.ReLU = prim::GetAttr[name=\"relu\"](%_0.9)\n",
      "  %bn1.7 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_326.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0.9)\n",
      "  %conv1.7 : __torch__.torch.nn.modules.conv.___torch_mangle_325.Conv2d = prim::GetAttr[name=\"conv1\"](%_0.9)\n",
      "  %weight.257 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.7)\n",
      "  %3162 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv1\n",
      "  %3163 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv1\n",
      "  %3164 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv1\n",
      "  %3165 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv1\n",
      "  %input.65 : Float(1, 128, 56, 56, strides=[401408, 3136, 56, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.63, %weight.257, %2968, %3162, %3163, %3164, %2972, %3165, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.129 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.7)\n",
      "  %running_mean.129 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.7)\n",
      "  %bias.129 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.7)\n",
      "  %weight.259 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.7)\n",
      "  %input.67 : Float(1, 128, 56, 56, strides=[401408, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.65, %weight.259, %bias.129, %running_mean.129, %running_var.129, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.69 : Float(1, 128, 56, 56, strides=[401408, 3136, 56, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.67), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.261 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.7)\n",
      "  %3174 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv2\n",
      "  %3175 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv2\n",
      "  %3176 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv2\n",
      "  %3177 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv2\n",
      "  %input.71 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.69, %weight.261, %2968, %3174, %3175, %3176, %2972, %3177, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.131 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.7)\n",
      "  %running_mean.131 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.7)\n",
      "  %bias.131 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.7)\n",
      "  %weight.263 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.7)\n",
      "  %input.73 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.71, %weight.263, %bias.131, %running_mean.131, %running_var.131, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.75 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.73), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.265 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.7)\n",
      "  %3186 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv3\n",
      "  %3187 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv3\n",
      "  %3188 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv3\n",
      "  %3189 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv3\n",
      "  %input.77 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.75, %weight.265, %2968, %3186, %3187, %3188, %2972, %3189, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.133 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.7)\n",
      "  %running_mean.133 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.7)\n",
      "  %bias.133 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.7)\n",
      "  %weight.267 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.7)\n",
      "  %out.7 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.77, %weight.267, %bias.133, %running_mean.133, %running_var.133, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %_1.7 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_333.BatchNorm2d = prim::GetAttr[name=\"1\"](%downsample.3)\n",
      "  %_0.7 : __torch__.torch.nn.modules.conv.___torch_mangle_332.Conv2d = prim::GetAttr[name=\"0\"](%downsample.3)\n",
      "  %weight.269 : Tensor = prim::GetAttr[name=\"weight\"](%_0.7)\n",
      "  %3199 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.downsample/__module.feature_extractor.5.0.downsample.0\n",
      "  %3200 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.downsample/__module.feature_extractor.5.0.downsample.0\n",
      "  %3201 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.downsample/__module.feature_extractor.5.0.downsample.0\n",
      "  %3202 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.downsample/__module.feature_extractor.5.0.downsample.0\n",
      "  %input.79 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.63, %weight.269, %2968, %3199, %3200, %3201, %2972, %3202, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.downsample/__module.feature_extractor.5.0.downsample.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.135 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.7)\n",
      "  %running_mean.135 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.7)\n",
      "  %bias.135 : Tensor = prim::GetAttr[name=\"bias\"](%_1.7)\n",
      "  %weight.271 : Tensor = prim::GetAttr[name=\"weight\"](%_1.7)\n",
      "  %identity.3 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.79, %weight.271, %bias.135, %running_mean.135, %running_var.135, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.downsample/__module.feature_extractor.5.0.downsample.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.81 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.7, %identity.3, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.83 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.81), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.0/__module.feature_extractor.5.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.9 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_341.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_1.9)\n",
      "  %conv3.9 : __torch__.torch.nn.modules.conv.___torch_mangle_340.Conv2d = prim::GetAttr[name=\"conv3\"](%_1.9)\n",
      "  %bn2.9 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_339.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1.9)\n",
      "  %conv2.9 : __torch__.torch.nn.modules.conv.___torch_mangle_338.Conv2d = prim::GetAttr[name=\"conv2\"](%_1.9)\n",
      "  %relu.9 : __torch__.torch.nn.modules.activation.___torch_mangle_342.ReLU = prim::GetAttr[name=\"relu\"](%_1.9)\n",
      "  %bn1.9 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_337.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1.9)\n",
      "  %conv1.9 : __torch__.torch.nn.modules.conv.___torch_mangle_336.Conv2d = prim::GetAttr[name=\"conv1\"](%_1.9)\n",
      "  %weight.273 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.9)\n",
      "  %3219 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv1\n",
      "  %3220 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv1\n",
      "  %3221 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv1\n",
      "  %3222 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv1\n",
      "  %input.85 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.83, %weight.273, %2968, %3219, %3220, %3221, %2972, %3222, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.137 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.9)\n",
      "  %running_mean.137 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.9)\n",
      "  %bias.137 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.9)\n",
      "  %weight.275 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.9)\n",
      "  %input.87 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.85, %weight.275, %bias.137, %running_mean.137, %running_var.137, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.89 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.87), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.277 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.9)\n",
      "  %3231 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv2\n",
      "  %3232 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv2\n",
      "  %3233 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv2\n",
      "  %3234 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv2\n",
      "  %input.91 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.89, %weight.277, %2968, %3231, %3232, %3233, %2972, %3234, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.139 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.9)\n",
      "  %running_mean.139 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.9)\n",
      "  %bias.139 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.9)\n",
      "  %weight.279 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.9)\n",
      "  %input.93 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.91, %weight.279, %bias.139, %running_mean.139, %running_var.139, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.95 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.93), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.281 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.9)\n",
      "  %3243 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv3\n",
      "  %3244 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv3\n",
      "  %3245 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv3\n",
      "  %3246 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv3\n",
      "  %input.97 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.95, %weight.281, %2968, %3243, %3244, %3245, %2972, %3246, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.141 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.9)\n",
      "  %running_mean.141 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.9)\n",
      "  %bias.141 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.9)\n",
      "  %weight.283 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.9)\n",
      "  %out.9 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.97, %weight.283, %bias.141, %running_mean.141, %running_var.141, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.99 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.9, %input.83, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.101 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.99), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.1/__module.feature_extractor.5.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.11 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_349.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_2.5)\n",
      "  %conv3.11 : __torch__.torch.nn.modules.conv.___torch_mangle_348.Conv2d = prim::GetAttr[name=\"conv3\"](%_2.5)\n",
      "  %bn2.11 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_347.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_2.5)\n",
      "  %conv2.11 : __torch__.torch.nn.modules.conv.___torch_mangle_346.Conv2d = prim::GetAttr[name=\"conv2\"](%_2.5)\n",
      "  %relu.11 : __torch__.torch.nn.modules.activation.___torch_mangle_350.ReLU = prim::GetAttr[name=\"relu\"](%_2.5)\n",
      "  %bn1.11 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_345.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_2.5)\n",
      "  %conv1.11 : __torch__.torch.nn.modules.conv.___torch_mangle_344.Conv2d = prim::GetAttr[name=\"conv1\"](%_2.5)\n",
      "  %weight.285 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.11)\n",
      "  %3263 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv1\n",
      "  %3264 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv1\n",
      "  %3265 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv1\n",
      "  %3266 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv1\n",
      "  %input.103 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.101, %weight.285, %2968, %3263, %3264, %3265, %2972, %3266, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.143 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.11)\n",
      "  %running_mean.143 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.11)\n",
      "  %bias.143 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.11)\n",
      "  %weight.287 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.11)\n",
      "  %input.105 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.103, %weight.287, %bias.143, %running_mean.143, %running_var.143, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.107 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.105), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.289 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.11)\n",
      "  %3275 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv2\n",
      "  %3276 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv2\n",
      "  %3277 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv2\n",
      "  %3278 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv2\n",
      "  %input.109 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.107, %weight.289, %2968, %3275, %3276, %3277, %2972, %3278, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.145 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.11)\n",
      "  %running_mean.145 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.11)\n",
      "  %bias.145 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.11)\n",
      "  %weight.291 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.11)\n",
      "  %input.111 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.109, %weight.291, %bias.145, %running_mean.145, %running_var.145, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.113 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.111), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.293 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.11)\n",
      "  %3287 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv3\n",
      "  %3288 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv3\n",
      "  %3289 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv3\n",
      "  %3290 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv3\n",
      "  %input.115 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.113, %weight.293, %2968, %3287, %3288, %3289, %2972, %3290, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.147 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.11)\n",
      "  %running_mean.147 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.11)\n",
      "  %bias.147 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.11)\n",
      "  %weight.295 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.11)\n",
      "  %out.11 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.115, %weight.295, %bias.147, %running_mean.147, %running_var.147, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.117 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.11, %input.101, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.119 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.117), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.2/__module.feature_extractor.5.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.13 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_357.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_3.3)\n",
      "  %conv3.13 : __torch__.torch.nn.modules.conv.___torch_mangle_356.Conv2d = prim::GetAttr[name=\"conv3\"](%_3.3)\n",
      "  %bn2.13 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_355.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_3.3)\n",
      "  %conv2.13 : __torch__.torch.nn.modules.conv.___torch_mangle_354.Conv2d = prim::GetAttr[name=\"conv2\"](%_3.3)\n",
      "  %relu.13 : __torch__.torch.nn.modules.activation.___torch_mangle_358.ReLU = prim::GetAttr[name=\"relu\"](%_3.3)\n",
      "  %bn1.13 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_353.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_3.3)\n",
      "  %conv1.13 : __torch__.torch.nn.modules.conv.___torch_mangle_352.Conv2d = prim::GetAttr[name=\"conv1\"](%_3.3)\n",
      "  %weight.297 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.13)\n",
      "  %3307 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv1\n",
      "  %3308 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv1\n",
      "  %3309 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv1\n",
      "  %3310 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv1\n",
      "  %input.121 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.119, %weight.297, %2968, %3307, %3308, %3309, %2972, %3310, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.149 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.13)\n",
      "  %running_mean.149 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.13)\n",
      "  %bias.149 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.13)\n",
      "  %weight.299 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.13)\n",
      "  %input.123 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.121, %weight.299, %bias.149, %running_mean.149, %running_var.149, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.125 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.123), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.301 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.13)\n",
      "  %3319 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv2\n",
      "  %3320 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv2\n",
      "  %3321 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv2\n",
      "  %3322 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv2\n",
      "  %input.127 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.125, %weight.301, %2968, %3319, %3320, %3321, %2972, %3322, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.151 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.13)\n",
      "  %running_mean.151 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.13)\n",
      "  %bias.151 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.13)\n",
      "  %weight.303 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.13)\n",
      "  %input.129 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.127, %weight.303, %bias.151, %running_mean.151, %running_var.151, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.131 : Float(1, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.129), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.305 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.13)\n",
      "  %3331 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv3\n",
      "  %3332 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv3\n",
      "  %3333 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv3\n",
      "  %3334 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv3\n",
      "  %input.133 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.131, %weight.305, %2968, %3331, %3332, %3333, %2972, %3334, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.153 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.13)\n",
      "  %running_mean.153 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.13)\n",
      "  %bias.153 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.13)\n",
      "  %weight.307 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.13)\n",
      "  %out.13 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.133, %weight.307, %bias.153, %running_mean.153, %running_var.153, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.135 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.13, %input.119, %2971), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.137 : Float(1, 512, 28, 28, strides=[401408, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.135), scope: __module.feature_extractor/__module.feature_extractor.5/__module.feature_extractor.5.3/__module.feature_extractor.5.3.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %_5 : __torch__.torchvision.models.resnet.___torch_mangle_411.Bottleneck = prim::GetAttr[name=\"5\"](%_6)\n",
      "  %_4 : __torch__.torchvision.models.resnet.___torch_mangle_403.Bottleneck = prim::GetAttr[name=\"4\"](%_6)\n",
      "  %_3 : __torch__.torchvision.models.resnet.___torch_mangle_395.Bottleneck = prim::GetAttr[name=\"3\"](%_6)\n",
      "  %_2.7 : __torch__.torchvision.models.resnet.___torch_mangle_387.Bottleneck = prim::GetAttr[name=\"2\"](%_6)\n",
      "  %_1.13 : __torch__.torchvision.models.resnet.___torch_mangle_379.Bottleneck = prim::GetAttr[name=\"1\"](%_6)\n",
      "  %_0.13 : __torch__.torchvision.models.resnet.___torch_mangle_371.Bottleneck = prim::GetAttr[name=\"0\"](%_6)\n",
      "  %downsample.5 : __torch__.torch.nn.modules.container.___torch_mangle_370.Sequential = prim::GetAttr[name=\"downsample\"](%_0.13)\n",
      "  %bn3.15 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_366.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_0.13)\n",
      "  %conv3.15 : __torch__.torch.nn.modules.conv.___torch_mangle_365.Conv2d = prim::GetAttr[name=\"conv3\"](%_0.13)\n",
      "  %bn2.15 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_364.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0.13)\n",
      "  %conv2.15 : __torch__.torch.nn.modules.conv.___torch_mangle_363.Conv2d = prim::GetAttr[name=\"conv2\"](%_0.13)\n",
      "  %relu.15 : __torch__.torch.nn.modules.activation.___torch_mangle_367.ReLU = prim::GetAttr[name=\"relu\"](%_0.13)\n",
      "  %bn1.15 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_362.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0.13)\n",
      "  %conv1.15 : __torch__.torch.nn.modules.conv.___torch_mangle_361.Conv2d = prim::GetAttr[name=\"conv1\"](%_0.13)\n",
      "  %weight.309 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.15)\n",
      "  %3358 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv1\n",
      "  %3359 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv1\n",
      "  %3360 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv1\n",
      "  %3361 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv1\n",
      "  %input.139 : Float(1, 256, 28, 28, strides=[200704, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.137, %weight.309, %2968, %3358, %3359, %3360, %2972, %3361, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.155 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.15)\n",
      "  %running_mean.155 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.15)\n",
      "  %bias.155 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.15)\n",
      "  %weight.311 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.15)\n",
      "  %input.141 : Float(1, 256, 28, 28, strides=[200704, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.139, %weight.311, %bias.155, %running_mean.155, %running_var.155, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.143 : Float(1, 256, 28, 28, strides=[200704, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.141), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.313 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.15)\n",
      "  %3370 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv2\n",
      "  %3371 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv2\n",
      "  %3372 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv2\n",
      "  %3373 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv2\n",
      "  %input.145 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.143, %weight.313, %2968, %3370, %3371, %3372, %2972, %3373, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.157 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.15)\n",
      "  %running_mean.157 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.15)\n",
      "  %bias.157 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.15)\n",
      "  %weight.315 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.15)\n",
      "  %input.147 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.145, %weight.315, %bias.157, %running_mean.157, %running_var.157, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.149 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.147), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.317 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.15)\n",
      "  %3382 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv3\n",
      "  %3383 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv3\n",
      "  %3384 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv3\n",
      "  %3385 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv3\n",
      "  %input.151 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.149, %weight.317, %2968, %3382, %3383, %3384, %2972, %3385, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.159 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.15)\n",
      "  %running_mean.159 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.15)\n",
      "  %bias.159 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.15)\n",
      "  %weight.319 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.15)\n",
      "  %out.15 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.151, %weight.319, %bias.159, %running_mean.159, %running_var.159, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %_1.11 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_369.BatchNorm2d = prim::GetAttr[name=\"1\"](%downsample.5)\n",
      "  %_0.11 : __torch__.torch.nn.modules.conv.___torch_mangle_368.Conv2d = prim::GetAttr[name=\"0\"](%downsample.5)\n",
      "  %weight.321 : Tensor = prim::GetAttr[name=\"weight\"](%_0.11)\n",
      "  %3395 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.downsample/__module.feature_extractor.6.0.downsample.0\n",
      "  %3396 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.downsample/__module.feature_extractor.6.0.downsample.0\n",
      "  %3397 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.downsample/__module.feature_extractor.6.0.downsample.0\n",
      "  %3398 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.downsample/__module.feature_extractor.6.0.downsample.0\n",
      "  %input.153 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.137, %weight.321, %2968, %3395, %3396, %3397, %2972, %3398, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.downsample/__module.feature_extractor.6.0.downsample.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.161 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.11)\n",
      "  %running_mean.161 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.11)\n",
      "  %bias.161 : Tensor = prim::GetAttr[name=\"bias\"](%_1.11)\n",
      "  %weight.323 : Tensor = prim::GetAttr[name=\"weight\"](%_1.11)\n",
      "  %identity.5 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.153, %weight.323, %bias.161, %running_mean.161, %running_var.161, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.downsample/__module.feature_extractor.6.0.downsample.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.155 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.15, %identity.5, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.157 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.155), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.0/__module.feature_extractor.6.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.17 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_377.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_1.13)\n",
      "  %conv3.17 : __torch__.torch.nn.modules.conv.___torch_mangle_376.Conv2d = prim::GetAttr[name=\"conv3\"](%_1.13)\n",
      "  %bn2.17 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_375.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1.13)\n",
      "  %conv2.17 : __torch__.torch.nn.modules.conv.___torch_mangle_374.Conv2d = prim::GetAttr[name=\"conv2\"](%_1.13)\n",
      "  %relu.17 : __torch__.torch.nn.modules.activation.___torch_mangle_378.ReLU = prim::GetAttr[name=\"relu\"](%_1.13)\n",
      "  %bn1.17 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_373.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1.13)\n",
      "  %conv1.17 : __torch__.torch.nn.modules.conv.___torch_mangle_372.Conv2d = prim::GetAttr[name=\"conv1\"](%_1.13)\n",
      "  %weight.325 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.17)\n",
      "  %3415 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv1\n",
      "  %3416 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv1\n",
      "  %3417 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv1\n",
      "  %3418 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv1\n",
      "  %input.159 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.157, %weight.325, %2968, %3415, %3416, %3417, %2972, %3418, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.163 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.17)\n",
      "  %running_mean.163 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.17)\n",
      "  %bias.163 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.17)\n",
      "  %weight.327 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.17)\n",
      "  %input.161 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.159, %weight.327, %bias.163, %running_mean.163, %running_var.163, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.163 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.161), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.329 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.17)\n",
      "  %3427 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv2\n",
      "  %3428 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv2\n",
      "  %3429 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv2\n",
      "  %3430 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv2\n",
      "  %input.165 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.163, %weight.329, %2968, %3427, %3428, %3429, %2972, %3430, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.165 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.17)\n",
      "  %running_mean.165 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.17)\n",
      "  %bias.165 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.17)\n",
      "  %weight.331 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.17)\n",
      "  %input.167 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.165, %weight.331, %bias.165, %running_mean.165, %running_var.165, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.169 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.167), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.333 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.17)\n",
      "  %3439 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv3\n",
      "  %3440 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv3\n",
      "  %3441 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv3\n",
      "  %3442 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv3\n",
      "  %input.171 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.169, %weight.333, %2968, %3439, %3440, %3441, %2972, %3442, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.167 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.17)\n",
      "  %running_mean.167 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.17)\n",
      "  %bias.167 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.17)\n",
      "  %weight.335 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.17)\n",
      "  %out.17 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.171, %weight.335, %bias.167, %running_mean.167, %running_var.167, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.173 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.17, %input.157, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.175 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.173), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.1/__module.feature_extractor.6.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.19 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_385.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_2.7)\n",
      "  %conv3.19 : __torch__.torch.nn.modules.conv.___torch_mangle_384.Conv2d = prim::GetAttr[name=\"conv3\"](%_2.7)\n",
      "  %bn2.19 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_383.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_2.7)\n",
      "  %conv2.19 : __torch__.torch.nn.modules.conv.___torch_mangle_382.Conv2d = prim::GetAttr[name=\"conv2\"](%_2.7)\n",
      "  %relu.19 : __torch__.torch.nn.modules.activation.___torch_mangle_386.ReLU = prim::GetAttr[name=\"relu\"](%_2.7)\n",
      "  %bn1.19 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_381.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_2.7)\n",
      "  %conv1.19 : __torch__.torch.nn.modules.conv.___torch_mangle_380.Conv2d = prim::GetAttr[name=\"conv1\"](%_2.7)\n",
      "  %weight.337 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.19)\n",
      "  %3459 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv1\n",
      "  %3460 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv1\n",
      "  %3461 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv1\n",
      "  %3462 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv1\n",
      "  %input.177 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.175, %weight.337, %2968, %3459, %3460, %3461, %2972, %3462, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.169 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.19)\n",
      "  %running_mean.169 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.19)\n",
      "  %bias.169 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.19)\n",
      "  %weight.339 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.19)\n",
      "  %input.179 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.177, %weight.339, %bias.169, %running_mean.169, %running_var.169, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.181 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.179), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.341 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.19)\n",
      "  %3471 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv2\n",
      "  %3472 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv2\n",
      "  %3473 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv2\n",
      "  %3474 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv2\n",
      "  %input.183 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.181, %weight.341, %2968, %3471, %3472, %3473, %2972, %3474, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.171 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.19)\n",
      "  %running_mean.171 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.19)\n",
      "  %bias.171 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.19)\n",
      "  %weight.343 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.19)\n",
      "  %input.185 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.183, %weight.343, %bias.171, %running_mean.171, %running_var.171, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.187 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.185), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.345 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.19)\n",
      "  %3483 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv3\n",
      "  %3484 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv3\n",
      "  %3485 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv3\n",
      "  %3486 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv3\n",
      "  %input.189 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.187, %weight.345, %2968, %3483, %3484, %3485, %2972, %3486, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.173 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.19)\n",
      "  %running_mean.173 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.19)\n",
      "  %bias.173 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.19)\n",
      "  %weight.347 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.19)\n",
      "  %out.19 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.189, %weight.347, %bias.173, %running_mean.173, %running_var.173, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.191 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.19, %input.175, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.193 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.191), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.2/__module.feature_extractor.6.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.21 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_393.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_3)\n",
      "  %conv3.21 : __torch__.torch.nn.modules.conv.___torch_mangle_392.Conv2d = prim::GetAttr[name=\"conv3\"](%_3)\n",
      "  %bn2.21 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_3)\n",
      "  %conv2.21 : __torch__.torch.nn.modules.conv.___torch_mangle_390.Conv2d = prim::GetAttr[name=\"conv2\"](%_3)\n",
      "  %relu.21 : __torch__.torch.nn.modules.activation.___torch_mangle_394.ReLU = prim::GetAttr[name=\"relu\"](%_3)\n",
      "  %bn1.21 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_389.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_3)\n",
      "  %conv1.21 : __torch__.torch.nn.modules.conv.___torch_mangle_388.Conv2d = prim::GetAttr[name=\"conv1\"](%_3)\n",
      "  %weight.349 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.21)\n",
      "  %3503 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv1\n",
      "  %3504 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv1\n",
      "  %3505 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv1\n",
      "  %3506 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv1\n",
      "  %input.195 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.193, %weight.349, %2968, %3503, %3504, %3505, %2972, %3506, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.175 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.21)\n",
      "  %running_mean.175 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.21)\n",
      "  %bias.175 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.21)\n",
      "  %weight.351 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.21)\n",
      "  %input.197 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.195, %weight.351, %bias.175, %running_mean.175, %running_var.175, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.199 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.197), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.353 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.21)\n",
      "  %3515 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv2\n",
      "  %3516 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv2\n",
      "  %3517 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv2\n",
      "  %3518 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv2\n",
      "  %input.201 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.199, %weight.353, %2968, %3515, %3516, %3517, %2972, %3518, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.177 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.21)\n",
      "  %running_mean.177 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.21)\n",
      "  %bias.177 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.21)\n",
      "  %weight.355 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.21)\n",
      "  %input.203 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.201, %weight.355, %bias.177, %running_mean.177, %running_var.177, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.205 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.203), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.357 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.21)\n",
      "  %3527 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv3\n",
      "  %3528 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv3\n",
      "  %3529 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv3\n",
      "  %3530 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv3\n",
      "  %input.207 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.205, %weight.357, %2968, %3527, %3528, %3529, %2972, %3530, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.179 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.21)\n",
      "  %running_mean.179 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.21)\n",
      "  %bias.179 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.21)\n",
      "  %weight.359 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.21)\n",
      "  %out.21 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.207, %weight.359, %bias.179, %running_mean.179, %running_var.179, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.209 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.21, %input.193, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.211 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.209), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.3/__module.feature_extractor.6.3.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.23 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_401.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_4)\n",
      "  %conv3.23 : __torch__.torch.nn.modules.conv.___torch_mangle_400.Conv2d = prim::GetAttr[name=\"conv3\"](%_4)\n",
      "  %bn2.23 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_399.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_4)\n",
      "  %conv2.23 : __torch__.torch.nn.modules.conv.___torch_mangle_398.Conv2d = prim::GetAttr[name=\"conv2\"](%_4)\n",
      "  %relu.23 : __torch__.torch.nn.modules.activation.___torch_mangle_402.ReLU = prim::GetAttr[name=\"relu\"](%_4)\n",
      "  %bn1.23 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_397.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_4)\n",
      "  %conv1.23 : __torch__.torch.nn.modules.conv.___torch_mangle_396.Conv2d = prim::GetAttr[name=\"conv1\"](%_4)\n",
      "  %weight.361 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.23)\n",
      "  %3547 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv1\n",
      "  %3548 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv1\n",
      "  %3549 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv1\n",
      "  %3550 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv1\n",
      "  %input.213 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.211, %weight.361, %2968, %3547, %3548, %3549, %2972, %3550, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.181 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.23)\n",
      "  %running_mean.181 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.23)\n",
      "  %bias.181 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.23)\n",
      "  %weight.363 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.23)\n",
      "  %input.215 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.213, %weight.363, %bias.181, %running_mean.181, %running_var.181, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.217 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.215), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.365 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.23)\n",
      "  %3559 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv2\n",
      "  %3560 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv2\n",
      "  %3561 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv2\n",
      "  %3562 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv2\n",
      "  %input.219 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.217, %weight.365, %2968, %3559, %3560, %3561, %2972, %3562, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.183 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.23)\n",
      "  %running_mean.183 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.23)\n",
      "  %bias.183 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.23)\n",
      "  %weight.367 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.23)\n",
      "  %input.221 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.219, %weight.367, %bias.183, %running_mean.183, %running_var.183, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.223 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.221), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.369 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.23)\n",
      "  %3571 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv3\n",
      "  %3572 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv3\n",
      "  %3573 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv3\n",
      "  %3574 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv3\n",
      "  %input.225 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.223, %weight.369, %2968, %3571, %3572, %3573, %2972, %3574, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.185 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.23)\n",
      "  %running_mean.185 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.23)\n",
      "  %bias.185 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.23)\n",
      "  %weight.371 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.23)\n",
      "  %out.23 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.225, %weight.371, %bias.185, %running_mean.185, %running_var.185, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.227 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.23, %input.211, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.229 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.227), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.4/__module.feature_extractor.6.4.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.25 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_409.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_5)\n",
      "  %conv3.25 : __torch__.torch.nn.modules.conv.___torch_mangle_408.Conv2d = prim::GetAttr[name=\"conv3\"](%_5)\n",
      "  %bn2.25 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_407.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_5)\n",
      "  %conv2.25 : __torch__.torch.nn.modules.conv.___torch_mangle_406.Conv2d = prim::GetAttr[name=\"conv2\"](%_5)\n",
      "  %relu.25 : __torch__.torch.nn.modules.activation.___torch_mangle_410.ReLU = prim::GetAttr[name=\"relu\"](%_5)\n",
      "  %bn1.25 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_405.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_5)\n",
      "  %conv1.25 : __torch__.torch.nn.modules.conv.___torch_mangle_404.Conv2d = prim::GetAttr[name=\"conv1\"](%_5)\n",
      "  %weight.373 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.25)\n",
      "  %3591 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv1\n",
      "  %3592 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv1\n",
      "  %3593 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv1\n",
      "  %3594 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv1\n",
      "  %input.231 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.229, %weight.373, %2968, %3591, %3592, %3593, %2972, %3594, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.187 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.25)\n",
      "  %running_mean.187 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.25)\n",
      "  %bias.187 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.25)\n",
      "  %weight.375 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.25)\n",
      "  %input.233 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.231, %weight.375, %bias.187, %running_mean.187, %running_var.187, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.235 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.233), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.377 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.25)\n",
      "  %3603 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv2\n",
      "  %3604 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv2\n",
      "  %3605 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv2\n",
      "  %3606 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv2\n",
      "  %input.237 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.235, %weight.377, %2968, %3603, %3604, %3605, %2972, %3606, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.189 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.25)\n",
      "  %running_mean.189 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.25)\n",
      "  %bias.189 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.25)\n",
      "  %weight.379 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.25)\n",
      "  %input.239 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.237, %weight.379, %bias.189, %running_mean.189, %running_var.189, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.241 : Float(1, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.239), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.381 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.25)\n",
      "  %3615 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv3\n",
      "  %3616 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv3\n",
      "  %3617 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv3\n",
      "  %3618 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv3\n",
      "  %input.243 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.241, %weight.381, %2968, %3615, %3616, %3617, %2972, %3618, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.191 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.25)\n",
      "  %running_mean.191 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.25)\n",
      "  %bias.191 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.25)\n",
      "  %weight.383 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.25)\n",
      "  %out.25 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.243, %weight.383, %bias.191, %running_mean.191, %running_var.191, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.245 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.25, %input.229, %2971), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.247 : Float(1, 1024, 14, 14, strides=[200704, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.245), scope: __module.feature_extractor/__module.feature_extractor.6/__module.feature_extractor.6.5/__module.feature_extractor.6.5.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %_2 : __torch__.torchvision.models.resnet.___torch_mangle_439.Bottleneck = prim::GetAttr[name=\"2\"](%_7)\n",
      "  %_1 : __torch__.torchvision.models.resnet.___torch_mangle_431.Bottleneck = prim::GetAttr[name=\"1\"](%_7)\n",
      "  %_0 : __torch__.torchvision.models.resnet.___torch_mangle_423.Bottleneck = prim::GetAttr[name=\"0\"](%_7)\n",
      "  %downsample : __torch__.torch.nn.modules.container.___torch_mangle_422.Sequential = prim::GetAttr[name=\"downsample\"](%_0)\n",
      "  %bn3.27 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_418.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_0)\n",
      "  %conv3.27 : __torch__.torch.nn.modules.conv.___torch_mangle_417.Conv2d = prim::GetAttr[name=\"conv3\"](%_0)\n",
      "  %bn2.27 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_416.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_0)\n",
      "  %conv2.27 : __torch__.torch.nn.modules.conv.___torch_mangle_415.Conv2d = prim::GetAttr[name=\"conv2\"](%_0)\n",
      "  %relu.27 : __torch__.torch.nn.modules.activation.___torch_mangle_419.ReLU = prim::GetAttr[name=\"relu\"](%_0)\n",
      "  %bn1.27 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_414.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_0)\n",
      "  %conv1.27 : __torch__.torch.nn.modules.conv.___torch_mangle_413.Conv2d = prim::GetAttr[name=\"conv1\"](%_0)\n",
      "  %weight.385 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.27)\n",
      "  %3639 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv1\n",
      "  %3640 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv1\n",
      "  %3641 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv1\n",
      "  %3642 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv1\n",
      "  %input.249 : Float(1, 512, 14, 14, strides=[100352, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.247, %weight.385, %2968, %3639, %3640, %3641, %2972, %3642, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.193 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.27)\n",
      "  %running_mean.193 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.27)\n",
      "  %bias.193 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.27)\n",
      "  %weight.387 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.27)\n",
      "  %input.251 : Float(1, 512, 14, 14, strides=[100352, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.249, %weight.387, %bias.193, %running_mean.193, %running_var.193, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.253 : Float(1, 512, 14, 14, strides=[100352, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.251), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.389 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.27)\n",
      "  %3651 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv2\n",
      "  %3652 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv2\n",
      "  %3653 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv2\n",
      "  %3654 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv2\n",
      "  %input.255 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.253, %weight.389, %2968, %3651, %3652, %3653, %2972, %3654, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.195 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.27)\n",
      "  %running_mean.195 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.27)\n",
      "  %bias.195 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.27)\n",
      "  %weight.391 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.27)\n",
      "  %input.257 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.255, %weight.391, %bias.195, %running_mean.195, %running_var.195, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.259 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.257), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.393 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.27)\n",
      "  %3663 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv3\n",
      "  %3664 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv3\n",
      "  %3665 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv3\n",
      "  %3666 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv3\n",
      "  %input.261 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.259, %weight.393, %2968, %3663, %3664, %3665, %2972, %3666, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.197 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.27)\n",
      "  %running_mean.197 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.27)\n",
      "  %bias.197 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.27)\n",
      "  %weight.395 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.27)\n",
      "  %out.27 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.261, %weight.395, %bias.197, %running_mean.197, %running_var.197, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %_1.15 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_421.BatchNorm2d = prim::GetAttr[name=\"1\"](%downsample)\n",
      "  %_0.15 : __torch__.torch.nn.modules.conv.___torch_mangle_420.Conv2d = prim::GetAttr[name=\"0\"](%downsample)\n",
      "  %weight.397 : Tensor = prim::GetAttr[name=\"weight\"](%_0.15)\n",
      "  %3676 : int[] = prim::ListConstruct(%2969, %2969), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.downsample/__module.feature_extractor.7.0.downsample.0\n",
      "  %3677 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.downsample/__module.feature_extractor.7.0.downsample.0\n",
      "  %3678 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.downsample/__module.feature_extractor.7.0.downsample.0\n",
      "  %3679 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.downsample/__module.feature_extractor.7.0.downsample.0\n",
      "  %input.263 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.247, %weight.397, %2968, %3676, %3677, %3678, %2972, %3679, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.downsample/__module.feature_extractor.7.0.downsample.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.199 : Tensor = prim::GetAttr[name=\"running_var\"](%_1.15)\n",
      "  %running_mean.199 : Tensor = prim::GetAttr[name=\"running_mean\"](%_1.15)\n",
      "  %bias.199 : Tensor = prim::GetAttr[name=\"bias\"](%_1.15)\n",
      "  %weight.399 : Tensor = prim::GetAttr[name=\"weight\"](%_1.15)\n",
      "  %identity : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.263, %weight.399, %bias.199, %running_mean.199, %running_var.199, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.downsample/__module.feature_extractor.7.0.downsample.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.265 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.27, %identity, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.267 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.265), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.0/__module.feature_extractor.7.0.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3.29 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_429.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_1)\n",
      "  %conv3.29 : __torch__.torch.nn.modules.conv.___torch_mangle_428.Conv2d = prim::GetAttr[name=\"conv3\"](%_1)\n",
      "  %bn2.29 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_427.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_1)\n",
      "  %conv2.29 : __torch__.torch.nn.modules.conv.___torch_mangle_426.Conv2d = prim::GetAttr[name=\"conv2\"](%_1)\n",
      "  %relu.29 : __torch__.torch.nn.modules.activation.___torch_mangle_430.ReLU = prim::GetAttr[name=\"relu\"](%_1)\n",
      "  %bn1.29 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_425.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_1)\n",
      "  %conv1.29 : __torch__.torch.nn.modules.conv.___torch_mangle_424.Conv2d = prim::GetAttr[name=\"conv1\"](%_1)\n",
      "  %weight.401 : Tensor = prim::GetAttr[name=\"weight\"](%conv1.29)\n",
      "  %3696 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv1\n",
      "  %3697 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv1\n",
      "  %3698 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv1\n",
      "  %3699 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv1\n",
      "  %input.269 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.267, %weight.401, %2968, %3696, %3697, %3698, %2972, %3699, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.201 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1.29)\n",
      "  %running_mean.201 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1.29)\n",
      "  %bias.201 : Tensor = prim::GetAttr[name=\"bias\"](%bn1.29)\n",
      "  %weight.403 : Tensor = prim::GetAttr[name=\"weight\"](%bn1.29)\n",
      "  %input.271 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.269, %weight.403, %bias.201, %running_mean.201, %running_var.201, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.273 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.271), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.405 : Tensor = prim::GetAttr[name=\"weight\"](%conv2.29)\n",
      "  %3708 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv2\n",
      "  %3709 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv2\n",
      "  %3710 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv2\n",
      "  %3711 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv2\n",
      "  %input.275 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.273, %weight.405, %2968, %3708, %3709, %3710, %2972, %3711, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.203 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2.29)\n",
      "  %running_mean.203 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2.29)\n",
      "  %bias.203 : Tensor = prim::GetAttr[name=\"bias\"](%bn2.29)\n",
      "  %weight.407 : Tensor = prim::GetAttr[name=\"weight\"](%bn2.29)\n",
      "  %input.277 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.275, %weight.407, %bias.203, %running_mean.203, %running_var.203, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.279 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.277), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.409 : Tensor = prim::GetAttr[name=\"weight\"](%conv3.29)\n",
      "  %3720 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv3\n",
      "  %3721 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv3\n",
      "  %3722 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv3\n",
      "  %3723 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv3\n",
      "  %input.281 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.279, %weight.409, %2968, %3720, %3721, %3722, %2972, %3723, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.205 : Tensor = prim::GetAttr[name=\"running_var\"](%bn3.29)\n",
      "  %running_mean.205 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3.29)\n",
      "  %bias.205 : Tensor = prim::GetAttr[name=\"bias\"](%bn3.29)\n",
      "  %weight.411 : Tensor = prim::GetAttr[name=\"weight\"](%bn3.29)\n",
      "  %out.29 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.281, %weight.411, %bias.205, %running_mean.205, %running_var.205, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.283 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::add_(%out.29, %input.267, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %input.285 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.283), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.1/__module.feature_extractor.7.1.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %bn3 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_437.BatchNorm2d = prim::GetAttr[name=\"bn3\"](%_2)\n",
      "  %conv3 : __torch__.torch.nn.modules.conv.___torch_mangle_436.Conv2d = prim::GetAttr[name=\"conv3\"](%_2)\n",
      "  %bn2 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_435.BatchNorm2d = prim::GetAttr[name=\"bn2\"](%_2)\n",
      "  %conv2 : __torch__.torch.nn.modules.conv.___torch_mangle_434.Conv2d = prim::GetAttr[name=\"conv2\"](%_2)\n",
      "  %relu : __torch__.torch.nn.modules.activation.___torch_mangle_438.ReLU = prim::GetAttr[name=\"relu\"](%_2)\n",
      "  %bn1 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_433.BatchNorm2d = prim::GetAttr[name=\"bn1\"](%_2)\n",
      "  %conv1 : __torch__.torch.nn.modules.conv.___torch_mangle_432.Conv2d = prim::GetAttr[name=\"conv1\"](%_2)\n",
      "  %weight.413 : Tensor = prim::GetAttr[name=\"weight\"](%conv1)\n",
      "  %3740 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv1\n",
      "  %3741 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv1\n",
      "  %3742 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv1\n",
      "  %3743 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv1\n",
      "  %input.287 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.285, %weight.413, %2968, %3740, %3741, %3742, %2972, %3743, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.207 : Tensor = prim::GetAttr[name=\"running_var\"](%bn1)\n",
      "  %running_mean.207 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn1)\n",
      "  %bias.207 : Tensor = prim::GetAttr[name=\"bias\"](%bn1)\n",
      "  %weight.415 : Tensor = prim::GetAttr[name=\"weight\"](%bn1)\n",
      "  %input.289 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.287, %weight.415, %bias.207, %running_mean.207, %running_var.207, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.bn1 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.291 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.289), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.417 : Tensor = prim::GetAttr[name=\"weight\"](%conv2)\n",
      "  %3752 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv2\n",
      "  %3753 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv2\n",
      "  %3754 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv2\n",
      "  %3755 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv2\n",
      "  %input.293 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.291, %weight.417, %2968, %3752, %3753, %3754, %2972, %3755, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var.209 : Tensor = prim::GetAttr[name=\"running_var\"](%bn2)\n",
      "  %running_mean.209 : Tensor = prim::GetAttr[name=\"running_mean\"](%bn2)\n",
      "  %bias.209 : Tensor = prim::GetAttr[name=\"bias\"](%bn2)\n",
      "  %weight.419 : Tensor = prim::GetAttr[name=\"weight\"](%bn2)\n",
      "  %input.295 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.293, %weight.419, %bias.209, %running_mean.209, %running_var.209, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.bn2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input.297 : Float(1, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input.295), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  %weight.421 : Tensor = prim::GetAttr[name=\"weight\"](%conv3)\n",
      "  %3764 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv3\n",
      "  %3765 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv3\n",
      "  %3766 : int[] = prim::ListConstruct(%2971, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv3\n",
      "  %3767 : int[] = prim::ListConstruct(%2973, %2973), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv3\n",
      "  %input.299 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%input.297, %weight.421, %2968, %3764, %3765, %3766, %2972, %3767, %2971, %2974, %2972, %2974, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.conv3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %running_var : Tensor = prim::GetAttr[name=\"running_var\"](%bn3)\n",
      "  %running_mean : Tensor = prim::GetAttr[name=\"running_mean\"](%bn3)\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%bn3)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%bn3)\n",
      "  %out : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::batch_norm(%input.299, %weight, %bias, %running_mean, %running_var, %2972, %2966, %2967, %2974), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.bn3 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2438:0\n",
      "  %input : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::add_(%out, %input.285, %2971), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/resnet.py:160:0\n",
      "  %3775 : Float(1, 2048, 7, 7, strides=[100352, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::relu_(%input), scope: __module.feature_extractor/__module.feature_extractor.7/__module.feature_extractor.7.2/__module.feature_extractor.7.2.relu # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1455:0\n",
      "  return (%3775)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ##create a writer and pot all the model sto tensorboard\n",
    "# writer_feature_extractor = SummaryWriter('runs/plot_oh_DANN_models_stage_1_feature_extractor')\n",
    "# #plot the models\n",
    "# #create a dummy input\n",
    "# dummy_input = torch.rand(CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE).unsqueeze(0).to(device)\n",
    "# # writer.add_graph(feature_extractor, dummy_input)\n",
    "# #add graph with name: feature_extractor as we will have multiple graphs\n",
    "# writer_feature_extractor.add_graph(feature_extractor, dummy_input, verbose=True)\n",
    "# # writer.add_graph(class_classifier,(2048, 1, 1))\n",
    "# # writer.add_graph(domain_classifier,(2048, 1, 1))\n",
    "# #close\n",
    "# writer_feature_extractor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.ClassClassifier,\n",
      "      %x : Float(1, 2048, 1, 1, strides=[2048, 1, 1, 1], requires_grad=0, device=cuda:0)):\n",
      "  %fc : __torch__.torch.nn.modules.container.___torch_mangle_594.Sequential = prim::GetAttr[name=\"fc\"](%self.1)\n",
      "  %flatten : __torch__.torch.nn.modules.flatten.Flatten = prim::GetAttr[name=\"flatten\"](%self.1)\n",
      "  %avgpool : __torch__.torch.nn.modules.container.___torch_mangle_593.Sequential = prim::GetAttr[name=\"avgpool\"](%self.1)\n",
      "  %64 : int = prim::Constant[value=1](), scope: __module.avgpool/__module.avgpool.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1214:0\n",
      "  %_0.1 : __torch__.torch.nn.modules.pooling.AdaptiveAvgPool2d = prim::GetAttr[name=\"0\"](%avgpool)\n",
      "  %66 : int[] = prim::ListConstruct(%64, %64), scope: __module.avgpool/__module.avgpool.0\n",
      "  %input.1 : Float(1, 2048, 1, 1, strides=[2048, 1, 1, 1], requires_grad=0, device=cuda:0) = aten::adaptive_avg_pool2d(%x, %66), scope: __module.avgpool/__module.avgpool.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1214:0\n",
      "  %68 : int = prim::Constant[value=-1](), scope: __module.flatten # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/flatten.py:45:0\n",
      "  %69 : int = prim::Constant[value=1](), scope: __module.flatten # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/flatten.py:45:0\n",
      "  %input.3 : Float(1, 2048, strides=[2048, 1], requires_grad=0, device=cuda:0) = aten::flatten(%input.1, %69, %68), scope: __module.flatten # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/flatten.py:45:0\n",
      "  %_0 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"0\"](%fc)\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%_0)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%_0)\n",
      "  %input : Float(1, 65, strides=[65, 1], requires_grad=1, device=cuda:0) = aten::linear(%input.3, %weight, %bias), scope: __module.fc/__module.fc.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %41 : int = prim::Constant[value=1]() # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1834:0\n",
      "  %42 : NoneType = prim::Constant()\n",
      "  %43 : Float(1, 65, strides=[65, 1], requires_grad=1, device=cuda:0) = aten::softmax(%input, %41, %42) # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1834:0\n",
      "  return (%43)\n",
      "\n",
      "graph(%self.1 : __torch__.DomainClassifier,\n",
      "      %x : Float(1, 2048, 1, 1, strides=[2048, 1, 1, 1], requires_grad=0, device=cuda:0)):\n",
      "  %fc3 : __torch__.torch.nn.modules.container.___torch_mangle_609.Sequential = prim::GetAttr[name=\"fc3\"](%self.1)\n",
      "  %fc2 : __torch__.torch.nn.modules.container.___torch_mangle_607.Sequential = prim::GetAttr[name=\"fc2\"](%self.1)\n",
      "  %fc1 : __torch__.torch.nn.modules.container.___torch_mangle_605.Sequential = prim::GetAttr[name=\"fc1\"](%self.1)\n",
      "  %flatten : __torch__.torch.nn.modules.flatten.___torch_mangle_603.Flatten = prim::GetAttr[name=\"flatten\"](%self.1)\n",
      "  %avgpool : __torch__.torch.nn.modules.container.___torch_mangle_602.Sequential = prim::GetAttr[name=\"avgpool\"](%self.1)\n",
      "  %input.1 : Float(1, 2048, 1, 1, strides=[2048, 1, 1, 1], requires_grad=0, device=cuda:0) = ^GradientReversalLayer(1)(%x) # /tmp/ipykernel_2368/562394563.py:15:0\n",
      "  %102 : int = prim::Constant[value=1](), scope: __module.avgpool/__module.avgpool.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1214:0\n",
      "  %_0.1 : __torch__.torch.nn.modules.pooling.___torch_mangle_601.AdaptiveAvgPool2d = prim::GetAttr[name=\"0\"](%avgpool)\n",
      "  %104 : int[] = prim::ListConstruct(%102, %102), scope: __module.avgpool/__module.avgpool.0\n",
      "  %input.3 : Float(1, 2048, 1, 1, strides=[2048, 1, 1, 1], requires_grad=0, device=cuda:0) = aten::adaptive_avg_pool2d(%input.1, %104), scope: __module.avgpool/__module.avgpool.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1214:0\n",
      "  %106 : int = prim::Constant[value=-1](), scope: __module.flatten # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/flatten.py:45:0\n",
      "  %107 : int = prim::Constant[value=1](), scope: __module.flatten # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/flatten.py:45:0\n",
      "  %input.5 : Float(1, 2048, strides=[2048, 1], requires_grad=0, device=cuda:0) = aten::flatten(%input.3, %107, %106), scope: __module.flatten # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/flatten.py:45:0\n",
      "  %_0.3 : __torch__.torch.nn.modules.linear.___torch_mangle_604.Linear = prim::GetAttr[name=\"0\"](%fc1)\n",
      "  %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%_0.3)\n",
      "  %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%_0.3)\n",
      "  %input.7 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = aten::linear(%input.5, %weight.1, %bias.1), scope: __module.fc1/__module.fc1.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %_0.5 : __torch__.torch.nn.modules.linear.___torch_mangle_606.Linear = prim::GetAttr[name=\"0\"](%fc2)\n",
      "  %bias.3 : Tensor = prim::GetAttr[name=\"bias\"](%_0.5)\n",
      "  %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%_0.5)\n",
      "  %input.9 : Float(1, 62, strides=[62, 1], requires_grad=1, device=cuda:0) = aten::linear(%input.7, %weight.3, %bias.3), scope: __module.fc2/__module.fc2.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %_0 : __torch__.torch.nn.modules.linear.___torch_mangle_608.Linear = prim::GetAttr[name=\"0\"](%fc3)\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%_0)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%_0)\n",
      "  %input : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = aten::linear(%input.9, %weight, %bias), scope: __module.fc3/__module.fc3.0 # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %61 : int = prim::Constant[value=1]() # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1834:0\n",
      "  %62 : NoneType = prim::Constant()\n",
      "  %63 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cuda:0) = aten::softmax(%input, %61, %62) # /home/lisa/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1834:0\n",
      "  return (%63)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2368/1062286346.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n",
      "/tmp/ipykernel_2368/2842852021.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# #create a dummy input for the class classifier\n",
    "# dummy_input = torch.rand(2048, 1, 1).unsqueeze(0).to(device)\n",
    "# #plot the model bycrear a writer and plot the model\n",
    "# writer_class_classifier = SummaryWriter('runs/plot_oh_DANN_models_stage_1_class_classifier')\n",
    "# writer_class_classifier.add_graph(class_classifier, dummy_input, verbose=True)\n",
    "# #close\n",
    "# writer_class_classifier.close()\n",
    "\n",
    "# #create a dummy input for the domain classifier of shape : torch.Size([1, 512, 28, 28])\n",
    "# writer_domain_classifier = SummaryWriter('runs/plot_oh_DANN_models_stage_1_domain_classifier')\n",
    "# writer_domain_classifier.add_graph(domain_classifier, dummy_input, verbose=True)\n",
    "# #close\n",
    "# writer_domain_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079402cc50f681fca3bc4b588c8594ae5b0127c6215ec7c89d21fdfb87f97274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
