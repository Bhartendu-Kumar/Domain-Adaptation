{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this script we will implement the adversarial discriminative domain adaptation algorithm\n",
    "#the algorithm is described in the paper \"Adversarial Discriminative Domain Adaptation\" by Ganin et al.\n",
    "#we will use MNIST as the source domain and USPS as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the whole training procedure has 3 steps:\n",
    "#1. train the a (feature extractor CNN + Classifier ) on the source domain\n",
    "#2. train a GAN on the features extracted from the source domain and the target domain to minimize the discrepancy between the two domains\n",
    "#3. test the classifier on the target domain\n",
    "\n",
    "#1. in step 1 both the feature extractor CNN and the classifier are trainable\n",
    "#2. in step 2 only the GAN Discriminator is trainable\n",
    "#3. in step 3 none are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use wasserstein loss\n",
    "#we will use gradient penalty for the discriminator\n",
    "#we eill use Resnet50 as base model for the feature extractor CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'mnist_adda'\n",
    "version = 'v3'\n",
    "\n",
    "#concat experiment name and version to get experiment id\n",
    "experiment_id = experiment_name + '_' + version\n",
    "\n",
    "model_path = 'saved_models/ADDA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU name\n",
    "#\n",
    "GPU_NAME = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neceassary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, Function\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#import utils\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for visualizations\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 08:40:14.562656: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "#import tenserboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#initialize tensorboard writer\n",
    "#create writer for tensorboard\n",
    "writer = SummaryWriter(f'runs/'+experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable cudnn\n",
    "cudnn.benchmark = True\n",
    "#cuda cache clear\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device\n",
    "device = torch.device(GPU_NAME if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the hyperparameters\n",
    "BATCH = 50\n",
    "\n",
    "EPOCHS = 5\n",
    "NUM_EPOCHS_PRETRAINING = 5\n",
    "\n",
    "#WHGAN parameters\n",
    "NUM_EPOCHS_GAN = 100\n",
    "CRITIC_ITERATIONS = 5\n",
    "LEARNING_RATE_GAN = 1e-4\n",
    "LAMBDA_GP = 10\n",
    "\n",
    "\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "CHANNELS_IMG = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "#parameters of ADAM optimizer\n",
    "LEARNING_RATE = 0.001\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "\n",
    "#parameters of SGD optimizer with momentum\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function for making all parameters of a model non trainable or trainable based on require_grad\n",
    "def freeze_unfreeze_model(model, require_grad = True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = require_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "def save_model(model):\n",
    "    #check if model path exists\n",
    "    if not os.path.exists(model_path, name_to_save):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    #we will save the model by the name of the experiment id \n",
    "    torch.save(model.state_dict(),  f'{model_path}/{experiment_id}+{name_to_save}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will have 3 different models : \n",
    "# 1. Feature Extractor CNN or Encoder, Source Feature Extractor CNN or Source Encoder AND Target Feature Extractor CNN or Target Encoder\n",
    "# 2. Classifier\n",
    "# 3. Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us first build the feature extractor\n",
    "#we input a resent50 model\n",
    "\n",
    "#creating the model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "#send weight sto gpu\n",
    "# weights = weights.to(device)\n",
    "#sending the model to GPU\n",
    "\n",
    "base_resnet = resnet50(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print model\n",
    "base_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will change the first convolution layer to accept single channel image\n",
    "#conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# #if CHANNELS_IMG == 1:\n",
    "# if CHANNELS_IMG == 1:\n",
    "#     base_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# #change the last fully connected layer to output classes in NUM_CLASSES\n",
    "# base_resnet.fc = nn.Linear(2048, NUM_CLASSES,  bias=True)\n",
    "\n",
    "#write a function to changethe model based on number of channels and number of classes\n",
    "def change_model(model, num_classes = NUM_CLASSES):\n",
    "    \n",
    "    model.fc = nn.Linear(2048, num_classes,  bias=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "base_resnet = change_model(base_resnet, num_classes = NUM_CLASSES)\n",
    "base_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will write class for the feature extractor network\n",
    "#we will pass the resnet50 model as the input to the class, and will use : nn.Sequential(*list(original_model.children())[:-2]) to get the feature extractor part of the model\n",
    "\n",
    "class ENCODER_CNN(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ENCODER_CNN, self).__init__()\n",
    "        # self.base_model = base_model\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENCODER_CNN(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now create an instance of the feature extractor and print the model\n",
    "source_cnn = ENCODER_CNN(base_resnet).to(device)\n",
    "source_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model summary\n",
    "# summary(source_cnn, (CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1024, 14, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tthe format for output of the feature extractor\n",
    "#create a random vector of size (BATCH, CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE)\n",
    "x = torch.randn(BATCH, CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "#pass the random vector through the feature extractor\n",
    "x = source_cnn(x)\n",
    "#check the output shape\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 14, 14])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will store the output shape of the feature extractor\n",
    "output_shape = x.shape\n",
    "#make the first dimension as 1 and then remove it\n",
    "output_shape = output_shape[1:]\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#now we will create the classifier, it will be same as the part of the resnet50 model after the feature extractor i.e. the last two layers of the resnet50 model\n",
    "#also the number of classes will be 10 as we have 10 classes in the MNIST dataset, the number of classes is stored in NUM_CLASSES\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.avgpool= nn.Sequential(*list(base_model.children())[-3:-1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(*list(base_model.children())[-1:])\n",
    "        #define a flatten layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print the shape of the input\n",
    "        # print(\"inside classifier: input shape\",x.shape)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (avgpool): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Classifier(base_resnet).to(device)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print summary of the classifier\n",
    "#input shape is the output shape of the feature extractor\n",
    "# summary(classifier, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will create the domain classifier: named as DomainClassifier\n",
    "#it will have the rest of the resnet model after the feature extractor and the classifier and will have an additional layer at the end to output the domain label: 0 for source and 1 for target\n",
    "#it will have gradient reversal layer in between the feature extractor and the classifier, i.e the first layer of the domain classifier \n",
    "#it will be exactly same as ClassClassifier except for the last layer, which is not number of classes but 2 for domain labels and sigmoid activation function instead of softmax\n",
    "#it will also do same [-2:] to get the classifier part of the model, first layer be gradient reversal layer\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #first layer of the domain classifier be the gradient reversal layer\n",
    "\n",
    "        self.avgpool = nn.Sequential(*list(base_model.children())[-3:-1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(nn.Linear(2048, 1, bias=True))\n",
    "       \n",
    "        \n",
    "        #now add the last output layer\n",
    "        # self.domain_classifier.add_module('domain_classifier_output', nn.Linear(2048, 2))\n",
    "        #change the last layer to output 2 classes\n",
    "        # self.fc = nn.Linear(2048, 2 , bias=True)\n",
    "\n",
    "        #forward\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        #output the domain label\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # x = x.view(-1, 2)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (avgpool): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of the domain classifier\n",
    "discriminator = Discriminator(base_resnet).to(device)\n",
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print summary of the domain classifier\n",
    "# summary(discriminator, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##create a writer and pot all the model sto tensorboard\n",
    "# writer_sourcecnn = SummaryWriter('runs/plot_oh_ADDA_models_sourcecnn')\n",
    "# #plot the models\n",
    "# #create a dummy input\n",
    "# dummy_input = torch.rand(CHANNELS_IMG, IMAGE_SIZE, IMAGE_SIZE).unsqueeze(0).to(device)\n",
    "# writer_sourcecnn.add_graph(source_cnn, dummy_input)\n",
    "# # writer.add_graph(class_classifier,(2048, 1, 1))\n",
    "# # writer.add_graph(domain_classifier,(2048, 1, 1))\n",
    "# #close\n",
    "# writer_sourcecnn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #write classifier\n",
    "\n",
    "# writer_classifier = SummaryWriter('runs/plot_oh_ADDA_models_classifier')\n",
    "# #plot the models\n",
    "# #create a dummy input\n",
    "# dummy_input = torch.rand(1, 1024, 14, 14).to(device)\n",
    "# writer_classifier.add_graph(classifier, dummy_input)\n",
    "# # writer.add_graph(class_classifier,(2048, 1, 1))\n",
    "# # writer.add_graph(domain_classifier,(2048, 1, 1))\n",
    "# #close\n",
    "# writer_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #discriminator\n",
    "# writer_discriminator = SummaryWriter('runs/plot_oh_ADDA_models_discriminator')\n",
    "# #plot the models\n",
    "# #create a dummy input: torch.Size([50, 1024, 14, 14])\n",
    "# dummy_input = torch.rand(1, 1024, 14, 14).to(device)\n",
    "# writer_discriminator.add_graph(discriminator, dummy_input)\n",
    "# # writer.add_graph(class_classifier,(2048, 1, 1))\n",
    "# # writer.add_graph(domain_classifier,(2048, 1, 1))\n",
    "# #close\n",
    "# writer_discriminator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the transform for the dataset\n",
    "transform_mnist_resnet = transforms.Compose(\n",
    "    [\n",
    "  \n",
    "    # if torch tensor then leave as it is, else convert to tensor\n",
    "    transforms.Lambda(lambda x: x if isinstance(x, torch.Tensor) else transforms.functional.to_tensor(x)),\n",
    "    #\n",
    "\n",
    "    #resize to 224x224\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "\n",
    "    #check if channels are 1, then convert to 3 channels\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "\n",
    "    transforms.Lambda(lambda x: preprocess(x)),\n",
    "\n",
    "    #if channels are 3, then make them 1\n",
    "    # transforms.Lambda(lambda x: x[0].unsqueeze(0) if x.shape[0] == 3 else x),\n",
    "    \n",
    "    # normalize\n",
    "    transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training we will use MNIST dataset in pytorch library\n",
    "#for testing we will use USPS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data - MNIST\n",
    "#### test data - USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "train_data = datasets.MNIST(root='./data/', download=True, transform=transform_mnist_resnet) \n",
    "#load train data\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load test data\n",
    "#USPS dataset\n",
    "test_data = datasets.USPS(root='./data/', download=True, transform=transform_mnist_resnet)\n",
    "#load test data\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#print the length of train and test data\n",
    "print(len(train_data))\n",
    "#print the shape of train data\n",
    "print(train_data[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7291\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#print length of test data\n",
    "print(len(test_data))\n",
    "#print shape of test data\n",
    "print(test_data[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "#print number of batches in train and test data\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training by Adversarial Discriminative Domain Adaptation (ADDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Pre- training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " #function to return gradient  norm\n",
    "#write a function to calculate the gradient penalty\n",
    "def gradient_norm(dnn, current_batch):\n",
    "\n",
    "    BATCH_SIZE, C, H, W = current_batch.shape\n",
    "    #print batch size, c,h,w\n",
    "    # print(\"batch size, c, h, w\", BATCH_SIZE, C, H, W)\n",
    "    if BATCH_SIZE%2==1:\n",
    "        #remove the last element\n",
    "        current_batch = current_batch[:-1]\n",
    "    #if batch size is 0 , then just return\n",
    "    if BATCH_SIZE==0:\n",
    "        return 0\n",
    "    \n",
    "    half_batch = int(BATCH_SIZE / 2)\n",
    "    # current_batch = current_batch.to(device)\n",
    "    # current_batch = Variable(current_batch, requires_grad=True)\n",
    "    #we select the first half of the batch\n",
    "    first_half = current_batch[:half_batch]\n",
    "    #we select the second half of the batch\n",
    "    second_half = current_batch[half_batch:]\n",
    "    #we create a random number between 0 and 1\n",
    "    # alpha = torch.rand(half_batch, 1)\n",
    "    #we expand the alpha to the size of the first half of the batch\n",
    "    # alpha = alpha.expand(first_half.size())\n",
    "    #we create alpha as a random number between 0 and 1 which will allow us to interpolate between the first half and the second half\n",
    "    \n",
    "    alpha = torch.rand(half_batch, 1, 1, 1).repeat(1, C, H, W)\n",
    "    #we expand the alpha to the size of the first half of the batch\n",
    "    # alpha = alpha.expand(first_half.size())\n",
    "\n",
    "\n",
    "    #we move alpha to the device\n",
    "    alpha = alpha.to(device)\n",
    "    #we interpolate between the first half and the second half\n",
    "    interpolates = alpha * first_half + ((1 - alpha) * second_half)\n",
    "    #we move interpolates to the device\n",
    "    interpolates = interpolates.to(device)\n",
    "    # interpolates = interpolates\n",
    "    #we create a variable of interpolates\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    #we pass interpolates through the cnn\n",
    "    disc_interpolates = dnn(interpolates)\n",
    "    #we calculate the gradients\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    #we calculate the gradient penalty\n",
    "    # calculate gradient norm \n",
    "    gradients_norm = gradients.norm(2, dim=1)\n",
    "    #mean of the gradient norm without subtracting 1 or lambda\n",
    "    gradient_norm_mean = (gradients_norm **2).mean()\n",
    "    #max of sqrt of the gradient norm without subtracting 1 or lambda\n",
    "    # gradient_norm_max = (gradients_norm **2).max( dim=0, keepdim=True)[0]\n",
    "\n",
    "    #delete the variables from the memory\n",
    "    del first_half\n",
    "    del second_half\n",
    "    del alpha\n",
    "    del interpolates\n",
    "    del disc_interpolates\n",
    "    del gradients\n",
    "    del gradients_norm\n",
    "    #cache the garbage\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "    # gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()    #have to check this formula    / * LAMBDA\n",
    "    #gradient penalty  should be max(0, gradient_penalty-1)\n",
    "    #we return the gradient penalty\n",
    "    return gradient_norm_mean\n",
    "    # , gradient_norm_max\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this approach, we will use the source_cnn and the classifier to train on the source data\n",
    "#write a function for pretraining the source_cnn and the classifier: we will input the source_cnn and the classifier and the train_loader, and the number of epochs, and device\n",
    "#we will use cross entropy loss for the classifier and ADAM optimizer for both the source_cnn and the classifier\n",
    "#we will also use tensorboard to visualize the training process, and plot the loss and accuracy\n",
    "#we will print the loss and accuracy after each epoch and also plot the loss and accuracy after each epoch\n",
    "def pre_train(source_cnn, classifier, train_loader, epoch=NUM_EPOCHS_PRETRAINING, device=device):\n",
    "\n",
    "    #we will store the source cnn in file at path experiment_id + temp_storage\n",
    "    #and will load this ile anytime source_cnn is needed\n",
    "    \n",
    "    #define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #define the optimizer for the source_cnn and the classifier in a list, and we use ADAM optimizer combined for both the source_cnn and the classifier\n",
    "    optimizer = optim.Adam(list(source_cnn.parameters()) + list(classifier.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "    #make both the source_cnn and the classifier in train mode\n",
    "    source_cnn.train()\n",
    "    classifier.train()\n",
    "\n",
    "    #loop for each epoch\n",
    "    epoch_tracker = 0\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batch_tracker = 0\n",
    "    ep=0\n",
    "    #we will add th loss for each batch in the epoch and then divide by the number of batches\n",
    "    for ep in range(epoch):\n",
    "        epoch_total = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_total_loss = 0\n",
    "        #loop for each batch\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #send data to gpu\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #set the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            #forward pass\n",
    "            output = classifier(source_cnn(data))\n",
    "            #calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            #calculate the gradients\n",
    "            loss.backward()\n",
    "            #update the weights\n",
    "\n",
    "            #we get the gradient norm by sending the model as sequential of source_cnn and classifier\n",
    "            #we will calculate the gradient norm\n",
    "            gradient_n = gradient_norm(nn.Sequential(source_cnn, classifier), data)\n",
    "            #we will add the gradient norm to the tensorboard\n",
    "            writer.add_scalar('Pretraining Gradient Norm', gradient_n, batch_tracker)\n",
    "            optimizer.step()\n",
    "            #write the loss to tensorboard\n",
    "            writer.add_scalar('Pretraining Training loss', loss, global_step=batch_tracker)\n",
    "\n",
    "            #calculate the total loss\n",
    "            total_loss += loss.item()\n",
    "            #total epoch loss sum\n",
    "            epoch_total_loss += loss.item()\n",
    "\n",
    "\n",
    "            #calculate the accuracy\n",
    "            #get the max value from the output\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            #calculate the total number of labels\n",
    "            temp_total = target.size(0)\n",
    "            #calculate the correct predictions\n",
    "            temp_correct = (predicted == target).sum().item()\n",
    "            #add the total and correct predictions\n",
    "            total += temp_total\n",
    "            epoch_total += temp_total\n",
    "            correct += temp_correct\n",
    "            epoch_correct += temp_correct\n",
    "            #calculate the accuracy\n",
    "            epoch_accuracy = 100 * epoch_correct / epoch_total\n",
    "            #write the accuracy to tensorboard\n",
    "            writer.add_scalar('Pretraining Training accuracy', epoch_accuracy, global_step=batch_tracker)\n",
    "            #print the loss and accuracy\n",
    "            #and\n",
    "            #print the gradient norm\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.2f}%\\tGradient Norm: {:.6f}'.format(\n",
    "                ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "                epoch_accuracy, gradient_n))\n",
    "            \n",
    "\n",
    "            \n",
    "            #print the loss\n",
    "            # if batch_idx % log_interval == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "        \n",
    "            #write the epoch loss to tensorboard\n",
    "            #first average the loss over the batches in the epoch\n",
    "            batch_tracker += 1\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "\n",
    "        #write the loss to tensorboard\n",
    "        writer.add_scalar('Pretraining  Training - Epoch loss', epoch_loss, global_step=ep)\n",
    "        #calculate the accuracy\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        #write the accuracy to tensorboard\n",
    "        writer.add_scalar('Pretraining  Training - Epoch accuracy', epoch_accuracy, global_step=ep)\n",
    "\n",
    "        # #save the model after every epoch, the name be experiment_id_epoch\n",
    "        # #wew will save in the folder saved_models\n",
    "        # torch.save(model.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'.pth')\n",
    "        # #we will also save the optimizer\n",
    "        # torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'_optimizer.pth')\n",
    "        #we will save the best model till now based on loss\n",
    "        #check if first epoch, then save the model anyway\n",
    "        if ep == 0:\n",
    "            #save the model\n",
    "            torch.save(source_cnn.state_dict(), 'saved_models/'+experiment_id+'_source_cnn.pth')\n",
    "            torch.save(classifier.state_dict(), 'saved_models/'+experiment_id+'_classifier.pth')\n",
    "            #save the optimizer\n",
    "            torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_optimizer.pth')\n",
    "            #save the loss\n",
    "            best_loss = epoch_total_loss\n",
    "            #save the epoch\n",
    "            best_epoch = ep\n",
    "        #if not first epoch, then check if the loss is less than the best loss\n",
    "        else:\n",
    "            #if loss is less than the best loss, then save the model\n",
    "            if epoch_total_loss < best_loss:\n",
    "                #save the model\n",
    "                torch.save(source_cnn.state_dict(), 'saved_models/'+experiment_id+'_source_cnn.pth')\n",
    "                torch.save(classifier.state_dict(), 'saved_models/'+experiment_id+'_classifier.pth')\n",
    "                #save the optimizer\n",
    "                torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_optimizer.pth')\n",
    "                #save the loss\n",
    "                best_loss = epoch_total_loss\n",
    "                #save the epoch\n",
    "                best_epoch = ep\n",
    "\n",
    "        epoch_tracker += 1\n",
    "\n",
    "    #print the accuracy\n",
    "    total_accuracy = 100 * correct / total\n",
    "    print('Accuracy: ', total_accuracy)\n",
    "\n",
    "    #close the tensorboard writer\n",
    "    writer.close()\n",
    "    #save the model with name experiment_id and then the last epoch\n",
    "    torch.save(source_cnn.state_dict(), 'saved_models/'+experiment_id+'_source_cnn_'+str(ep)+'.pth')\n",
    "    torch.save(classifier.state_dict(), 'saved_models/'+experiment_id+'_classifier_'+str(ep)+'.pth')\n",
    "    #save the optimizer\n",
    "    torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_optimizer_'+str(ep)+'.pth')\n",
    "    \n",
    "\n",
    "    #return the models\n",
    "    return source_cnn, classifier\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13695/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [0/60000 (0%)]\tLoss: 2.300949\tAccuracy: 10.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50/60000 (0%)]\tLoss: 2.298323\tAccuracy: 11.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [100/60000 (0%)]\tLoss: 2.297180\tAccuracy: 8.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [150/60000 (0%)]\tLoss: 2.258967\tAccuracy: 13.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [200/60000 (0%)]\tLoss: 2.114667\tAccuracy: 17.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [250/60000 (0%)]\tLoss: 2.161726\tAccuracy: 20.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [300/60000 (0%)]\tLoss: 2.125104\tAccuracy: 23.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [350/60000 (1%)]\tLoss: 2.021822\tAccuracy: 26.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [400/60000 (1%)]\tLoss: 2.101834\tAccuracy: 27.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [450/60000 (1%)]\tLoss: 1.999885\tAccuracy: 30.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [500/60000 (1%)]\tLoss: 2.033925\tAccuracy: 30.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [550/60000 (1%)]\tLoss: 1.916865\tAccuracy: 33.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [600/60000 (1%)]\tLoss: 1.775469\tAccuracy: 37.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [650/60000 (1%)]\tLoss: 1.854608\tAccuracy: 39.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [700/60000 (1%)]\tLoss: 1.703063\tAccuracy: 42.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [750/60000 (1%)]\tLoss: 1.671307\tAccuracy: 45.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [800/60000 (1%)]\tLoss: 1.703321\tAccuracy: 47.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [850/60000 (1%)]\tLoss: 1.659629\tAccuracy: 49.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [900/60000 (2%)]\tLoss: 1.625701\tAccuracy: 51.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [950/60000 (2%)]\tLoss: 1.601354\tAccuracy: 53.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1000/60000 (2%)]\tLoss: 1.607542\tAccuracy: 54.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1050/60000 (2%)]\tLoss: 1.540546\tAccuracy: 56.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1100/60000 (2%)]\tLoss: 1.652090\tAccuracy: 57.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1150/60000 (2%)]\tLoss: 1.652597\tAccuracy: 58.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1200/60000 (2%)]\tLoss: 1.604759\tAccuracy: 59.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1250/60000 (2%)]\tLoss: 1.723289\tAccuracy: 60.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1300/60000 (2%)]\tLoss: 1.571687\tAccuracy: 61.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1350/60000 (2%)]\tLoss: 1.626886\tAccuracy: 62.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1400/60000 (2%)]\tLoss: 1.583609\tAccuracy: 63.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1450/60000 (2%)]\tLoss: 1.515056\tAccuracy: 64.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1500/60000 (2%)]\tLoss: 1.507847\tAccuracy: 65.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1550/60000 (3%)]\tLoss: 1.654518\tAccuracy: 65.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1600/60000 (3%)]\tLoss: 1.628307\tAccuracy: 66.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1650/60000 (3%)]\tLoss: 1.679681\tAccuracy: 66.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1700/60000 (3%)]\tLoss: 1.651968\tAccuracy: 67.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1750/60000 (3%)]\tLoss: 1.589007\tAccuracy: 67.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1800/60000 (3%)]\tLoss: 1.635618\tAccuracy: 67.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1850/60000 (3%)]\tLoss: 1.630844\tAccuracy: 68.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1900/60000 (3%)]\tLoss: 1.606640\tAccuracy: 68.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [1950/60000 (3%)]\tLoss: 1.639975\tAccuracy: 69.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2000/60000 (3%)]\tLoss: 1.589224\tAccuracy: 69.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2050/60000 (3%)]\tLoss: 1.555271\tAccuracy: 70.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2100/60000 (4%)]\tLoss: 1.570130\tAccuracy: 70.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2150/60000 (4%)]\tLoss: 1.593358\tAccuracy: 70.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2200/60000 (4%)]\tLoss: 1.581649\tAccuracy: 71.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2250/60000 (4%)]\tLoss: 1.643867\tAccuracy: 71.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2300/60000 (4%)]\tLoss: 1.595706\tAccuracy: 71.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2350/60000 (4%)]\tLoss: 1.578146\tAccuracy: 72.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2400/60000 (4%)]\tLoss: 1.633507\tAccuracy: 72.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2450/60000 (4%)]\tLoss: 1.557746\tAccuracy: 72.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2500/60000 (4%)]\tLoss: 1.684674\tAccuracy: 72.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2550/60000 (4%)]\tLoss: 1.674200\tAccuracy: 73.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2600/60000 (4%)]\tLoss: 1.581832\tAccuracy: 73.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2650/60000 (4%)]\tLoss: 1.657945\tAccuracy: 73.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2700/60000 (4%)]\tLoss: 1.505866\tAccuracy: 73.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2750/60000 (5%)]\tLoss: 1.591269\tAccuracy: 74.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2800/60000 (5%)]\tLoss: 1.637476\tAccuracy: 74.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2850/60000 (5%)]\tLoss: 1.529235\tAccuracy: 74.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2900/60000 (5%)]\tLoss: 1.596193\tAccuracy: 74.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [2950/60000 (5%)]\tLoss: 1.587328\tAccuracy: 75.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3000/60000 (5%)]\tLoss: 1.483937\tAccuracy: 75.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3050/60000 (5%)]\tLoss: 1.517941\tAccuracy: 75.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3100/60000 (5%)]\tLoss: 1.502274\tAccuracy: 76.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3150/60000 (5%)]\tLoss: 1.544961\tAccuracy: 76.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3200/60000 (5%)]\tLoss: 1.538101\tAccuracy: 76.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3250/60000 (5%)]\tLoss: 1.501533\tAccuracy: 77.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3300/60000 (6%)]\tLoss: 1.501208\tAccuracy: 77.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3350/60000 (6%)]\tLoss: 1.586889\tAccuracy: 77.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3400/60000 (6%)]\tLoss: 1.554799\tAccuracy: 77.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3450/60000 (6%)]\tLoss: 1.499663\tAccuracy: 77.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3500/60000 (6%)]\tLoss: 1.523349\tAccuracy: 78.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3550/60000 (6%)]\tLoss: 1.508816\tAccuracy: 78.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3600/60000 (6%)]\tLoss: 1.573188\tAccuracy: 78.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3650/60000 (6%)]\tLoss: 1.522277\tAccuracy: 78.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3700/60000 (6%)]\tLoss: 1.541132\tAccuracy: 78.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3750/60000 (6%)]\tLoss: 1.486095\tAccuracy: 79.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3800/60000 (6%)]\tLoss: 1.507895\tAccuracy: 79.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3850/60000 (6%)]\tLoss: 1.529653\tAccuracy: 79.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3900/60000 (6%)]\tLoss: 1.534620\tAccuracy: 79.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [3950/60000 (7%)]\tLoss: 1.655613\tAccuracy: 79.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4000/60000 (7%)]\tLoss: 1.571533\tAccuracy: 79.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4050/60000 (7%)]\tLoss: 1.599754\tAccuracy: 79.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4100/60000 (7%)]\tLoss: 1.574783\tAccuracy: 79.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4150/60000 (7%)]\tLoss: 1.521082\tAccuracy: 80.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4200/60000 (7%)]\tLoss: 1.600720\tAccuracy: 80.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4250/60000 (7%)]\tLoss: 1.610979\tAccuracy: 80.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4300/60000 (7%)]\tLoss: 1.578998\tAccuracy: 80.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4350/60000 (7%)]\tLoss: 1.593012\tAccuracy: 80.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4400/60000 (7%)]\tLoss: 1.557569\tAccuracy: 80.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4450/60000 (7%)]\tLoss: 1.632040\tAccuracy: 80.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4500/60000 (8%)]\tLoss: 1.550916\tAccuracy: 80.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4550/60000 (8%)]\tLoss: 1.524713\tAccuracy: 80.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4600/60000 (8%)]\tLoss: 1.556766\tAccuracy: 80.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4650/60000 (8%)]\tLoss: 1.554058\tAccuracy: 81.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4700/60000 (8%)]\tLoss: 1.551847\tAccuracy: 81.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4750/60000 (8%)]\tLoss: 1.592918\tAccuracy: 81.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4800/60000 (8%)]\tLoss: 1.519006\tAccuracy: 81.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4850/60000 (8%)]\tLoss: 1.521709\tAccuracy: 81.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4900/60000 (8%)]\tLoss: 1.610676\tAccuracy: 81.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [4950/60000 (8%)]\tLoss: 1.483670\tAccuracy: 81.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5000/60000 (8%)]\tLoss: 1.567043\tAccuracy: 81.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5050/60000 (8%)]\tLoss: 1.518839\tAccuracy: 81.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5100/60000 (8%)]\tLoss: 1.484227\tAccuracy: 82.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5150/60000 (9%)]\tLoss: 1.523615\tAccuracy: 82.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5200/60000 (9%)]\tLoss: 1.633651\tAccuracy: 82.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5250/60000 (9%)]\tLoss: 1.537260\tAccuracy: 82.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5300/60000 (9%)]\tLoss: 1.531591\tAccuracy: 82.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5350/60000 (9%)]\tLoss: 1.529625\tAccuracy: 82.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5400/60000 (9%)]\tLoss: 1.467178\tAccuracy: 82.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5450/60000 (9%)]\tLoss: 1.520673\tAccuracy: 82.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5500/60000 (9%)]\tLoss: 1.527550\tAccuracy: 82.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5550/60000 (9%)]\tLoss: 1.527413\tAccuracy: 82.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5600/60000 (9%)]\tLoss: 1.554411\tAccuracy: 82.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5650/60000 (9%)]\tLoss: 1.487081\tAccuracy: 83.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5700/60000 (10%)]\tLoss: 1.558160\tAccuracy: 83.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5750/60000 (10%)]\tLoss: 1.494597\tAccuracy: 83.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5800/60000 (10%)]\tLoss: 1.500808\tAccuracy: 83.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5850/60000 (10%)]\tLoss: 1.523473\tAccuracy: 83.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5900/60000 (10%)]\tLoss: 1.504735\tAccuracy: 83.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [5950/60000 (10%)]\tLoss: 1.553314\tAccuracy: 83.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6000/60000 (10%)]\tLoss: 1.528126\tAccuracy: 83.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6050/60000 (10%)]\tLoss: 1.560436\tAccuracy: 83.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6100/60000 (10%)]\tLoss: 1.462545\tAccuracy: 83.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6150/60000 (10%)]\tLoss: 1.538343\tAccuracy: 83.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6200/60000 (10%)]\tLoss: 1.539699\tAccuracy: 84.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6250/60000 (10%)]\tLoss: 1.533059\tAccuracy: 84.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6300/60000 (10%)]\tLoss: 1.539679\tAccuracy: 84.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6350/60000 (11%)]\tLoss: 1.492303\tAccuracy: 84.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6400/60000 (11%)]\tLoss: 1.557084\tAccuracy: 84.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6450/60000 (11%)]\tLoss: 1.484954\tAccuracy: 84.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6500/60000 (11%)]\tLoss: 1.492041\tAccuracy: 84.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6550/60000 (11%)]\tLoss: 1.519365\tAccuracy: 84.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6600/60000 (11%)]\tLoss: 1.581787\tAccuracy: 84.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6650/60000 (11%)]\tLoss: 1.518810\tAccuracy: 84.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6700/60000 (11%)]\tLoss: 1.490324\tAccuracy: 84.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6750/60000 (11%)]\tLoss: 1.510503\tAccuracy: 84.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6800/60000 (11%)]\tLoss: 1.484985\tAccuracy: 84.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6850/60000 (11%)]\tLoss: 1.494397\tAccuracy: 85.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6900/60000 (12%)]\tLoss: 1.505193\tAccuracy: 85.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [6950/60000 (12%)]\tLoss: 1.463760\tAccuracy: 85.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7000/60000 (12%)]\tLoss: 1.490498\tAccuracy: 85.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7050/60000 (12%)]\tLoss: 1.516030\tAccuracy: 85.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7100/60000 (12%)]\tLoss: 1.536086\tAccuracy: 85.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7150/60000 (12%)]\tLoss: 1.497050\tAccuracy: 85.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7200/60000 (12%)]\tLoss: 1.508354\tAccuracy: 85.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7250/60000 (12%)]\tLoss: 1.510579\tAccuracy: 85.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7300/60000 (12%)]\tLoss: 1.543910\tAccuracy: 85.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7350/60000 (12%)]\tLoss: 1.540048\tAccuracy: 85.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7400/60000 (12%)]\tLoss: 1.534150\tAccuracy: 85.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7450/60000 (12%)]\tLoss: 1.521587\tAccuracy: 85.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7500/60000 (12%)]\tLoss: 1.489385\tAccuracy: 85.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7550/60000 (13%)]\tLoss: 1.501166\tAccuracy: 85.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7600/60000 (13%)]\tLoss: 1.493086\tAccuracy: 86.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7650/60000 (13%)]\tLoss: 1.463794\tAccuracy: 86.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7700/60000 (13%)]\tLoss: 1.512447\tAccuracy: 86.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7750/60000 (13%)]\tLoss: 1.493569\tAccuracy: 86.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7800/60000 (13%)]\tLoss: 1.505425\tAccuracy: 86.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7850/60000 (13%)]\tLoss: 1.606932\tAccuracy: 86.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7900/60000 (13%)]\tLoss: 1.562292\tAccuracy: 86.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [7950/60000 (13%)]\tLoss: 1.551716\tAccuracy: 86.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8000/60000 (13%)]\tLoss: 1.532643\tAccuracy: 86.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8050/60000 (13%)]\tLoss: 1.495259\tAccuracy: 86.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8100/60000 (14%)]\tLoss: 1.537336\tAccuracy: 86.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8150/60000 (14%)]\tLoss: 1.514131\tAccuracy: 86.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8200/60000 (14%)]\tLoss: 1.516124\tAccuracy: 86.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8250/60000 (14%)]\tLoss: 1.522577\tAccuracy: 86.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8300/60000 (14%)]\tLoss: 1.482833\tAccuracy: 86.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8350/60000 (14%)]\tLoss: 1.515782\tAccuracy: 86.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8400/60000 (14%)]\tLoss: 1.547100\tAccuracy: 86.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8450/60000 (14%)]\tLoss: 1.526508\tAccuracy: 86.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8500/60000 (14%)]\tLoss: 1.507147\tAccuracy: 86.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8550/60000 (14%)]\tLoss: 1.499624\tAccuracy: 86.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8600/60000 (14%)]\tLoss: 1.532644\tAccuracy: 86.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8650/60000 (14%)]\tLoss: 1.532405\tAccuracy: 87.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8700/60000 (14%)]\tLoss: 1.524395\tAccuracy: 87.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8750/60000 (15%)]\tLoss: 1.496012\tAccuracy: 87.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8800/60000 (15%)]\tLoss: 1.506803\tAccuracy: 87.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8850/60000 (15%)]\tLoss: 1.461483\tAccuracy: 87.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8900/60000 (15%)]\tLoss: 1.466902\tAccuracy: 87.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [8950/60000 (15%)]\tLoss: 1.469275\tAccuracy: 87.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9000/60000 (15%)]\tLoss: 1.542581\tAccuracy: 87.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9050/60000 (15%)]\tLoss: 1.473164\tAccuracy: 87.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9100/60000 (15%)]\tLoss: 1.498498\tAccuracy: 87.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9150/60000 (15%)]\tLoss: 1.535199\tAccuracy: 87.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9200/60000 (15%)]\tLoss: 1.488011\tAccuracy: 87.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9250/60000 (15%)]\tLoss: 1.481062\tAccuracy: 87.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9300/60000 (16%)]\tLoss: 1.463552\tAccuracy: 87.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9350/60000 (16%)]\tLoss: 1.478202\tAccuracy: 87.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9400/60000 (16%)]\tLoss: 1.473272\tAccuracy: 87.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9450/60000 (16%)]\tLoss: 1.482559\tAccuracy: 87.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9500/60000 (16%)]\tLoss: 1.479015\tAccuracy: 87.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9550/60000 (16%)]\tLoss: 1.498748\tAccuracy: 87.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9600/60000 (16%)]\tLoss: 1.497407\tAccuracy: 88.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9650/60000 (16%)]\tLoss: 1.513782\tAccuracy: 88.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9700/60000 (16%)]\tLoss: 1.489790\tAccuracy: 88.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9750/60000 (16%)]\tLoss: 1.483220\tAccuracy: 88.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9800/60000 (16%)]\tLoss: 1.484470\tAccuracy: 88.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9850/60000 (16%)]\tLoss: 1.539568\tAccuracy: 88.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9900/60000 (16%)]\tLoss: 1.521730\tAccuracy: 88.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [9950/60000 (17%)]\tLoss: 1.489854\tAccuracy: 88.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10000/60000 (17%)]\tLoss: 1.493775\tAccuracy: 88.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10050/60000 (17%)]\tLoss: 1.520329\tAccuracy: 88.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10100/60000 (17%)]\tLoss: 1.486868\tAccuracy: 88.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10150/60000 (17%)]\tLoss: 1.510445\tAccuracy: 88.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10200/60000 (17%)]\tLoss: 1.517655\tAccuracy: 88.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10250/60000 (17%)]\tLoss: 1.461558\tAccuracy: 88.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10300/60000 (17%)]\tLoss: 1.500838\tAccuracy: 88.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10350/60000 (17%)]\tLoss: 1.493173\tAccuracy: 88.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10400/60000 (17%)]\tLoss: 1.487520\tAccuracy: 88.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10450/60000 (17%)]\tLoss: 1.546629\tAccuracy: 88.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10500/60000 (18%)]\tLoss: 1.500705\tAccuracy: 88.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10550/60000 (18%)]\tLoss: 1.499425\tAccuracy: 88.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10600/60000 (18%)]\tLoss: 1.529205\tAccuracy: 88.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10650/60000 (18%)]\tLoss: 1.519794\tAccuracy: 88.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10700/60000 (18%)]\tLoss: 1.469596\tAccuracy: 88.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10750/60000 (18%)]\tLoss: 1.490203\tAccuracy: 88.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10800/60000 (18%)]\tLoss: 1.494402\tAccuracy: 88.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10850/60000 (18%)]\tLoss: 1.492628\tAccuracy: 88.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10900/60000 (18%)]\tLoss: 1.481463\tAccuracy: 89.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [10950/60000 (18%)]\tLoss: 1.491366\tAccuracy: 89.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11000/60000 (18%)]\tLoss: 1.531402\tAccuracy: 89.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11050/60000 (18%)]\tLoss: 1.527013\tAccuracy: 89.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11100/60000 (18%)]\tLoss: 1.498054\tAccuracy: 89.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11150/60000 (19%)]\tLoss: 1.472117\tAccuracy: 89.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11200/60000 (19%)]\tLoss: 1.462098\tAccuracy: 89.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11250/60000 (19%)]\tLoss: 1.495115\tAccuracy: 89.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11300/60000 (19%)]\tLoss: 1.518734\tAccuracy: 89.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11350/60000 (19%)]\tLoss: 1.543895\tAccuracy: 89.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11400/60000 (19%)]\tLoss: 1.534525\tAccuracy: 89.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11450/60000 (19%)]\tLoss: 1.520835\tAccuracy: 89.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11500/60000 (19%)]\tLoss: 1.517415\tAccuracy: 89.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11550/60000 (19%)]\tLoss: 1.472391\tAccuracy: 89.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11600/60000 (19%)]\tLoss: 1.515522\tAccuracy: 89.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11650/60000 (19%)]\tLoss: 1.525208\tAccuracy: 89.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11700/60000 (20%)]\tLoss: 1.541853\tAccuracy: 89.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11750/60000 (20%)]\tLoss: 1.506746\tAccuracy: 89.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11800/60000 (20%)]\tLoss: 1.517326\tAccuracy: 89.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11850/60000 (20%)]\tLoss: 1.512678\tAccuracy: 89.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11900/60000 (20%)]\tLoss: 1.491312\tAccuracy: 89.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [11950/60000 (20%)]\tLoss: 1.505516\tAccuracy: 89.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12000/60000 (20%)]\tLoss: 1.520736\tAccuracy: 89.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12050/60000 (20%)]\tLoss: 1.539937\tAccuracy: 89.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12100/60000 (20%)]\tLoss: 1.463168\tAccuracy: 89.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12150/60000 (20%)]\tLoss: 1.490743\tAccuracy: 89.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12200/60000 (20%)]\tLoss: 1.553642\tAccuracy: 89.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12250/60000 (20%)]\tLoss: 1.515635\tAccuracy: 89.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12300/60000 (20%)]\tLoss: 1.499424\tAccuracy: 89.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12350/60000 (21%)]\tLoss: 1.466163\tAccuracy: 89.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12400/60000 (21%)]\tLoss: 1.467818\tAccuracy: 89.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12450/60000 (21%)]\tLoss: 1.488160\tAccuracy: 89.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12500/60000 (21%)]\tLoss: 1.477738\tAccuracy: 89.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12550/60000 (21%)]\tLoss: 1.477847\tAccuracy: 89.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12600/60000 (21%)]\tLoss: 1.529888\tAccuracy: 89.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12650/60000 (21%)]\tLoss: 1.519547\tAccuracy: 89.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12700/60000 (21%)]\tLoss: 1.581552\tAccuracy: 89.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12750/60000 (21%)]\tLoss: 1.551322\tAccuracy: 89.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12800/60000 (21%)]\tLoss: 1.541269\tAccuracy: 89.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12850/60000 (21%)]\tLoss: 1.521467\tAccuracy: 89.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12900/60000 (22%)]\tLoss: 1.519560\tAccuracy: 89.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [12950/60000 (22%)]\tLoss: 1.609187\tAccuracy: 89.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13000/60000 (22%)]\tLoss: 1.468811\tAccuracy: 90.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13050/60000 (22%)]\tLoss: 1.538492\tAccuracy: 90.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13100/60000 (22%)]\tLoss: 1.519488\tAccuracy: 90.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13150/60000 (22%)]\tLoss: 1.532924\tAccuracy: 90.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13200/60000 (22%)]\tLoss: 1.490191\tAccuracy: 90.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13250/60000 (22%)]\tLoss: 1.493816\tAccuracy: 90.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13300/60000 (22%)]\tLoss: 1.489753\tAccuracy: 90.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13350/60000 (22%)]\tLoss: 1.501290\tAccuracy: 90.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13400/60000 (22%)]\tLoss: 1.531975\tAccuracy: 90.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13450/60000 (22%)]\tLoss: 1.501850\tAccuracy: 90.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13500/60000 (22%)]\tLoss: 1.484968\tAccuracy: 90.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13550/60000 (23%)]\tLoss: 1.520311\tAccuracy: 90.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13600/60000 (23%)]\tLoss: 1.518539\tAccuracy: 90.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13650/60000 (23%)]\tLoss: 1.491780\tAccuracy: 90.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13700/60000 (23%)]\tLoss: 1.479358\tAccuracy: 90.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13750/60000 (23%)]\tLoss: 1.506503\tAccuracy: 90.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13800/60000 (23%)]\tLoss: 1.562590\tAccuracy: 90.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13850/60000 (23%)]\tLoss: 1.483439\tAccuracy: 90.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13900/60000 (23%)]\tLoss: 1.466221\tAccuracy: 90.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [13950/60000 (23%)]\tLoss: 1.491557\tAccuracy: 90.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14000/60000 (23%)]\tLoss: 1.502140\tAccuracy: 90.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14050/60000 (23%)]\tLoss: 1.505494\tAccuracy: 90.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14100/60000 (24%)]\tLoss: 1.578474\tAccuracy: 90.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14150/60000 (24%)]\tLoss: 1.513479\tAccuracy: 90.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14200/60000 (24%)]\tLoss: 1.462306\tAccuracy: 90.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14250/60000 (24%)]\tLoss: 1.493119\tAccuracy: 90.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14300/60000 (24%)]\tLoss: 1.468698\tAccuracy: 90.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14350/60000 (24%)]\tLoss: 1.481614\tAccuracy: 90.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14400/60000 (24%)]\tLoss: 1.468240\tAccuracy: 90.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14450/60000 (24%)]\tLoss: 1.491630\tAccuracy: 90.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14500/60000 (24%)]\tLoss: 1.468643\tAccuracy: 90.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14550/60000 (24%)]\tLoss: 1.484748\tAccuracy: 90.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14600/60000 (24%)]\tLoss: 1.563513\tAccuracy: 90.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14650/60000 (24%)]\tLoss: 1.481117\tAccuracy: 90.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14700/60000 (24%)]\tLoss: 1.498495\tAccuracy: 90.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14750/60000 (25%)]\tLoss: 1.548095\tAccuracy: 90.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14800/60000 (25%)]\tLoss: 1.544182\tAccuracy: 90.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14850/60000 (25%)]\tLoss: 1.549848\tAccuracy: 90.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14900/60000 (25%)]\tLoss: 1.469666\tAccuracy: 90.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [14950/60000 (25%)]\tLoss: 1.491135\tAccuracy: 90.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15000/60000 (25%)]\tLoss: 1.523156\tAccuracy: 90.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15050/60000 (25%)]\tLoss: 1.477296\tAccuracy: 90.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15100/60000 (25%)]\tLoss: 1.478761\tAccuracy: 90.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15150/60000 (25%)]\tLoss: 1.500283\tAccuracy: 90.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15200/60000 (25%)]\tLoss: 1.478475\tAccuracy: 90.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15250/60000 (25%)]\tLoss: 1.485888\tAccuracy: 90.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15300/60000 (26%)]\tLoss: 1.461282\tAccuracy: 90.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15350/60000 (26%)]\tLoss: 1.485696\tAccuracy: 90.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15400/60000 (26%)]\tLoss: 1.535468\tAccuracy: 90.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15450/60000 (26%)]\tLoss: 1.544119\tAccuracy: 90.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15500/60000 (26%)]\tLoss: 1.483475\tAccuracy: 90.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15550/60000 (26%)]\tLoss: 1.509648\tAccuracy: 90.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15600/60000 (26%)]\tLoss: 1.504093\tAccuracy: 91.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15650/60000 (26%)]\tLoss: 1.539029\tAccuracy: 91.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15700/60000 (26%)]\tLoss: 1.464007\tAccuracy: 91.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15750/60000 (26%)]\tLoss: 1.499997\tAccuracy: 91.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15800/60000 (26%)]\tLoss: 1.524434\tAccuracy: 91.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15850/60000 (26%)]\tLoss: 1.461833\tAccuracy: 91.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15900/60000 (26%)]\tLoss: 1.493242\tAccuracy: 91.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [15950/60000 (27%)]\tLoss: 1.482609\tAccuracy: 91.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16000/60000 (27%)]\tLoss: 1.496255\tAccuracy: 91.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16050/60000 (27%)]\tLoss: 1.461297\tAccuracy: 91.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16100/60000 (27%)]\tLoss: 1.487249\tAccuracy: 91.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16150/60000 (27%)]\tLoss: 1.520074\tAccuracy: 91.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16200/60000 (27%)]\tLoss: 1.473023\tAccuracy: 91.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16250/60000 (27%)]\tLoss: 1.490741\tAccuracy: 91.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16300/60000 (27%)]\tLoss: 1.526530\tAccuracy: 91.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16350/60000 (27%)]\tLoss: 1.502705\tAccuracy: 91.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16400/60000 (27%)]\tLoss: 1.485308\tAccuracy: 91.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16450/60000 (27%)]\tLoss: 1.532213\tAccuracy: 91.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16500/60000 (28%)]\tLoss: 1.502603\tAccuracy: 91.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16550/60000 (28%)]\tLoss: 1.486220\tAccuracy: 91.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16600/60000 (28%)]\tLoss: 1.472411\tAccuracy: 91.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16650/60000 (28%)]\tLoss: 1.583207\tAccuracy: 91.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16700/60000 (28%)]\tLoss: 1.507125\tAccuracy: 91.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16750/60000 (28%)]\tLoss: 1.525416\tAccuracy: 91.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16800/60000 (28%)]\tLoss: 1.481066\tAccuracy: 91.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16850/60000 (28%)]\tLoss: 1.491000\tAccuracy: 91.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16900/60000 (28%)]\tLoss: 1.500303\tAccuracy: 91.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [16950/60000 (28%)]\tLoss: 1.553107\tAccuracy: 91.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17000/60000 (28%)]\tLoss: 1.536768\tAccuracy: 91.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17050/60000 (28%)]\tLoss: 1.573022\tAccuracy: 91.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17100/60000 (28%)]\tLoss: 1.528181\tAccuracy: 91.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17150/60000 (29%)]\tLoss: 1.511466\tAccuracy: 91.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17200/60000 (29%)]\tLoss: 1.485397\tAccuracy: 91.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17250/60000 (29%)]\tLoss: 1.461515\tAccuracy: 91.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17300/60000 (29%)]\tLoss: 1.510694\tAccuracy: 91.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17350/60000 (29%)]\tLoss: 1.463280\tAccuracy: 91.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17400/60000 (29%)]\tLoss: 1.507583\tAccuracy: 91.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17450/60000 (29%)]\tLoss: 1.489280\tAccuracy: 91.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17500/60000 (29%)]\tLoss: 1.469446\tAccuracy: 91.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17550/60000 (29%)]\tLoss: 1.504606\tAccuracy: 91.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17600/60000 (29%)]\tLoss: 1.503597\tAccuracy: 91.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17650/60000 (29%)]\tLoss: 1.485594\tAccuracy: 91.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17700/60000 (30%)]\tLoss: 1.510135\tAccuracy: 91.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17750/60000 (30%)]\tLoss: 1.465507\tAccuracy: 91.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17800/60000 (30%)]\tLoss: 1.563012\tAccuracy: 91.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17850/60000 (30%)]\tLoss: 1.503987\tAccuracy: 91.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17900/60000 (30%)]\tLoss: 1.466386\tAccuracy: 91.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [17950/60000 (30%)]\tLoss: 1.461623\tAccuracy: 91.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18000/60000 (30%)]\tLoss: 1.497453\tAccuracy: 91.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18050/60000 (30%)]\tLoss: 1.507486\tAccuracy: 91.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18100/60000 (30%)]\tLoss: 1.582363\tAccuracy: 91.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18150/60000 (30%)]\tLoss: 1.463938\tAccuracy: 91.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18200/60000 (30%)]\tLoss: 1.492681\tAccuracy: 91.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18250/60000 (30%)]\tLoss: 1.462940\tAccuracy: 91.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18300/60000 (30%)]\tLoss: 1.550166\tAccuracy: 91.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18350/60000 (31%)]\tLoss: 1.537739\tAccuracy: 91.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18400/60000 (31%)]\tLoss: 1.552311\tAccuracy: 91.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18450/60000 (31%)]\tLoss: 1.467038\tAccuracy: 91.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18500/60000 (31%)]\tLoss: 1.484583\tAccuracy: 91.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18550/60000 (31%)]\tLoss: 1.497489\tAccuracy: 91.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18600/60000 (31%)]\tLoss: 1.499306\tAccuracy: 91.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18650/60000 (31%)]\tLoss: 1.489593\tAccuracy: 91.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18700/60000 (31%)]\tLoss: 1.522908\tAccuracy: 91.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18750/60000 (31%)]\tLoss: 1.510003\tAccuracy: 91.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18800/60000 (31%)]\tLoss: 1.506448\tAccuracy: 91.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18850/60000 (31%)]\tLoss: 1.524083\tAccuracy: 91.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18900/60000 (32%)]\tLoss: 1.482900\tAccuracy: 91.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [18950/60000 (32%)]\tLoss: 1.523154\tAccuracy: 91.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19000/60000 (32%)]\tLoss: 1.525916\tAccuracy: 91.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19050/60000 (32%)]\tLoss: 1.481158\tAccuracy: 91.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19100/60000 (32%)]\tLoss: 1.502739\tAccuracy: 91.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19150/60000 (32%)]\tLoss: 1.487508\tAccuracy: 91.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19200/60000 (32%)]\tLoss: 1.519961\tAccuracy: 91.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19250/60000 (32%)]\tLoss: 1.505302\tAccuracy: 91.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19300/60000 (32%)]\tLoss: 1.487117\tAccuracy: 91.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19350/60000 (32%)]\tLoss: 1.543511\tAccuracy: 91.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19400/60000 (32%)]\tLoss: 1.521396\tAccuracy: 91.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19450/60000 (32%)]\tLoss: 1.535719\tAccuracy: 91.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19500/60000 (32%)]\tLoss: 1.524177\tAccuracy: 91.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19550/60000 (33%)]\tLoss: 1.500693\tAccuracy: 91.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19600/60000 (33%)]\tLoss: 1.487784\tAccuracy: 92.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19650/60000 (33%)]\tLoss: 1.560074\tAccuracy: 91.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19700/60000 (33%)]\tLoss: 1.545625\tAccuracy: 91.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19750/60000 (33%)]\tLoss: 1.584540\tAccuracy: 91.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19800/60000 (33%)]\tLoss: 1.504430\tAccuracy: 91.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19850/60000 (33%)]\tLoss: 1.531538\tAccuracy: 92.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19900/60000 (33%)]\tLoss: 1.474156\tAccuracy: 92.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [19950/60000 (33%)]\tLoss: 1.514784\tAccuracy: 92.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20000/60000 (33%)]\tLoss: 1.469777\tAccuracy: 92.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20050/60000 (33%)]\tLoss: 1.538367\tAccuracy: 92.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20100/60000 (34%)]\tLoss: 1.554581\tAccuracy: 92.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20150/60000 (34%)]\tLoss: 1.462014\tAccuracy: 92.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20200/60000 (34%)]\tLoss: 1.517930\tAccuracy: 92.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20250/60000 (34%)]\tLoss: 1.493658\tAccuracy: 92.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20300/60000 (34%)]\tLoss: 1.498923\tAccuracy: 92.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20350/60000 (34%)]\tLoss: 1.514323\tAccuracy: 92.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20400/60000 (34%)]\tLoss: 1.511582\tAccuracy: 92.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20450/60000 (34%)]\tLoss: 1.502023\tAccuracy: 92.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20500/60000 (34%)]\tLoss: 1.510838\tAccuracy: 92.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20550/60000 (34%)]\tLoss: 1.525132\tAccuracy: 92.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20600/60000 (34%)]\tLoss: 1.481604\tAccuracy: 92.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20650/60000 (34%)]\tLoss: 1.511779\tAccuracy: 92.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20700/60000 (34%)]\tLoss: 1.478491\tAccuracy: 92.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20750/60000 (35%)]\tLoss: 1.483133\tAccuracy: 92.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20800/60000 (35%)]\tLoss: 1.479382\tAccuracy: 92.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20850/60000 (35%)]\tLoss: 1.518563\tAccuracy: 92.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20900/60000 (35%)]\tLoss: 1.543729\tAccuracy: 92.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [20950/60000 (35%)]\tLoss: 1.509920\tAccuracy: 92.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21000/60000 (35%)]\tLoss: 1.501227\tAccuracy: 92.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21050/60000 (35%)]\tLoss: 1.507585\tAccuracy: 92.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21100/60000 (35%)]\tLoss: 1.512244\tAccuracy: 92.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21150/60000 (35%)]\tLoss: 1.503473\tAccuracy: 92.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21200/60000 (35%)]\tLoss: 1.489769\tAccuracy: 92.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21250/60000 (35%)]\tLoss: 1.487751\tAccuracy: 92.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21300/60000 (36%)]\tLoss: 1.501034\tAccuracy: 92.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21350/60000 (36%)]\tLoss: 1.485123\tAccuracy: 92.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21400/60000 (36%)]\tLoss: 1.502914\tAccuracy: 92.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21450/60000 (36%)]\tLoss: 1.482182\tAccuracy: 92.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21500/60000 (36%)]\tLoss: 1.461250\tAccuracy: 92.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21550/60000 (36%)]\tLoss: 1.469271\tAccuracy: 92.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21600/60000 (36%)]\tLoss: 1.468314\tAccuracy: 92.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21650/60000 (36%)]\tLoss: 1.469936\tAccuracy: 92.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21700/60000 (36%)]\tLoss: 1.462791\tAccuracy: 92.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21750/60000 (36%)]\tLoss: 1.508829\tAccuracy: 92.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21800/60000 (36%)]\tLoss: 1.499552\tAccuracy: 92.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21850/60000 (36%)]\tLoss: 1.536393\tAccuracy: 92.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21900/60000 (36%)]\tLoss: 1.470188\tAccuracy: 92.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [21950/60000 (37%)]\tLoss: 1.482090\tAccuracy: 92.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22000/60000 (37%)]\tLoss: 1.585336\tAccuracy: 92.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22050/60000 (37%)]\tLoss: 1.462103\tAccuracy: 92.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22100/60000 (37%)]\tLoss: 1.497940\tAccuracy: 92.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22150/60000 (37%)]\tLoss: 1.508633\tAccuracy: 92.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22200/60000 (37%)]\tLoss: 1.515562\tAccuracy: 92.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22250/60000 (37%)]\tLoss: 1.532219\tAccuracy: 92.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22300/60000 (37%)]\tLoss: 1.520912\tAccuracy: 92.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22350/60000 (37%)]\tLoss: 1.493778\tAccuracy: 92.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22400/60000 (37%)]\tLoss: 1.481523\tAccuracy: 92.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22450/60000 (37%)]\tLoss: 1.501630\tAccuracy: 92.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22500/60000 (38%)]\tLoss: 1.528981\tAccuracy: 92.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22550/60000 (38%)]\tLoss: 1.489809\tAccuracy: 92.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22600/60000 (38%)]\tLoss: 1.462258\tAccuracy: 92.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22650/60000 (38%)]\tLoss: 1.469634\tAccuracy: 92.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22700/60000 (38%)]\tLoss: 1.541883\tAccuracy: 92.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22750/60000 (38%)]\tLoss: 1.463591\tAccuracy: 92.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22800/60000 (38%)]\tLoss: 1.561452\tAccuracy: 92.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22850/60000 (38%)]\tLoss: 1.503526\tAccuracy: 92.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22900/60000 (38%)]\tLoss: 1.461221\tAccuracy: 92.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [22950/60000 (38%)]\tLoss: 1.483750\tAccuracy: 92.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23000/60000 (38%)]\tLoss: 1.542541\tAccuracy: 92.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23050/60000 (38%)]\tLoss: 1.526833\tAccuracy: 92.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23100/60000 (38%)]\tLoss: 1.538517\tAccuracy: 92.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23150/60000 (39%)]\tLoss: 1.505544\tAccuracy: 92.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23200/60000 (39%)]\tLoss: 1.477950\tAccuracy: 92.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23250/60000 (39%)]\tLoss: 1.480856\tAccuracy: 92.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23300/60000 (39%)]\tLoss: 1.466941\tAccuracy: 92.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23350/60000 (39%)]\tLoss: 1.516643\tAccuracy: 92.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23400/60000 (39%)]\tLoss: 1.543118\tAccuracy: 92.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23450/60000 (39%)]\tLoss: 1.479613\tAccuracy: 92.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23500/60000 (39%)]\tLoss: 1.528809\tAccuracy: 92.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23550/60000 (39%)]\tLoss: 1.521830\tAccuracy: 92.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23600/60000 (39%)]\tLoss: 1.474638\tAccuracy: 92.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23650/60000 (39%)]\tLoss: 1.526405\tAccuracy: 92.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23700/60000 (40%)]\tLoss: 1.528666\tAccuracy: 92.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23750/60000 (40%)]\tLoss: 1.508763\tAccuracy: 92.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23800/60000 (40%)]\tLoss: 1.540337\tAccuracy: 92.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23850/60000 (40%)]\tLoss: 1.497421\tAccuracy: 92.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23900/60000 (40%)]\tLoss: 1.576697\tAccuracy: 92.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [23950/60000 (40%)]\tLoss: 1.493472\tAccuracy: 92.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24000/60000 (40%)]\tLoss: 1.485112\tAccuracy: 92.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24050/60000 (40%)]\tLoss: 1.497647\tAccuracy: 92.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24100/60000 (40%)]\tLoss: 1.512917\tAccuracy: 92.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24150/60000 (40%)]\tLoss: 1.475329\tAccuracy: 92.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24200/60000 (40%)]\tLoss: 1.551293\tAccuracy: 92.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24250/60000 (40%)]\tLoss: 1.530160\tAccuracy: 92.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24300/60000 (40%)]\tLoss: 1.518199\tAccuracy: 92.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24350/60000 (41%)]\tLoss: 1.504646\tAccuracy: 92.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24400/60000 (41%)]\tLoss: 1.484184\tAccuracy: 92.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24450/60000 (41%)]\tLoss: 1.493488\tAccuracy: 92.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24500/60000 (41%)]\tLoss: 1.495748\tAccuracy: 92.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24550/60000 (41%)]\tLoss: 1.479467\tAccuracy: 92.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24600/60000 (41%)]\tLoss: 1.461201\tAccuracy: 92.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24650/60000 (41%)]\tLoss: 1.539731\tAccuracy: 92.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24700/60000 (41%)]\tLoss: 1.515813\tAccuracy: 92.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24750/60000 (41%)]\tLoss: 1.498599\tAccuracy: 92.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24800/60000 (41%)]\tLoss: 1.512644\tAccuracy: 92.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24850/60000 (41%)]\tLoss: 1.464229\tAccuracy: 92.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24900/60000 (42%)]\tLoss: 1.509703\tAccuracy: 92.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [24950/60000 (42%)]\tLoss: 1.464460\tAccuracy: 92.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25000/60000 (42%)]\tLoss: 1.467234\tAccuracy: 92.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25050/60000 (42%)]\tLoss: 1.486861\tAccuracy: 92.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25100/60000 (42%)]\tLoss: 1.535051\tAccuracy: 92.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25150/60000 (42%)]\tLoss: 1.489040\tAccuracy: 92.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25200/60000 (42%)]\tLoss: 1.481102\tAccuracy: 92.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25250/60000 (42%)]\tLoss: 1.507146\tAccuracy: 92.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25300/60000 (42%)]\tLoss: 1.483311\tAccuracy: 92.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25350/60000 (42%)]\tLoss: 1.481759\tAccuracy: 92.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25400/60000 (42%)]\tLoss: 1.492035\tAccuracy: 92.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25450/60000 (42%)]\tLoss: 1.559489\tAccuracy: 92.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25500/60000 (42%)]\tLoss: 1.511868\tAccuracy: 92.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25550/60000 (43%)]\tLoss: 1.481193\tAccuracy: 92.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25600/60000 (43%)]\tLoss: 1.478196\tAccuracy: 92.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25650/60000 (43%)]\tLoss: 1.521801\tAccuracy: 92.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25700/60000 (43%)]\tLoss: 1.486223\tAccuracy: 92.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25750/60000 (43%)]\tLoss: 1.502872\tAccuracy: 92.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25800/60000 (43%)]\tLoss: 1.507364\tAccuracy: 92.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25850/60000 (43%)]\tLoss: 1.472856\tAccuracy: 92.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25900/60000 (43%)]\tLoss: 1.482182\tAccuracy: 92.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [25950/60000 (43%)]\tLoss: 1.461783\tAccuracy: 92.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26000/60000 (43%)]\tLoss: 1.484326\tAccuracy: 92.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26050/60000 (43%)]\tLoss: 1.551682\tAccuracy: 92.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26100/60000 (44%)]\tLoss: 1.467145\tAccuracy: 92.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26150/60000 (44%)]\tLoss: 1.505750\tAccuracy: 93.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26200/60000 (44%)]\tLoss: 1.468682\tAccuracy: 93.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26250/60000 (44%)]\tLoss: 1.466687\tAccuracy: 93.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26300/60000 (44%)]\tLoss: 1.481918\tAccuracy: 93.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26350/60000 (44%)]\tLoss: 1.463299\tAccuracy: 93.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26400/60000 (44%)]\tLoss: 1.467167\tAccuracy: 93.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26450/60000 (44%)]\tLoss: 1.530765\tAccuracy: 93.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26500/60000 (44%)]\tLoss: 1.492451\tAccuracy: 93.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26550/60000 (44%)]\tLoss: 1.541212\tAccuracy: 93.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26600/60000 (44%)]\tLoss: 1.486786\tAccuracy: 93.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26650/60000 (44%)]\tLoss: 1.461850\tAccuracy: 93.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26700/60000 (44%)]\tLoss: 1.517043\tAccuracy: 93.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26750/60000 (45%)]\tLoss: 1.461360\tAccuracy: 93.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26800/60000 (45%)]\tLoss: 1.499710\tAccuracy: 93.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26850/60000 (45%)]\tLoss: 1.481624\tAccuracy: 93.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26900/60000 (45%)]\tLoss: 1.493834\tAccuracy: 93.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [26950/60000 (45%)]\tLoss: 1.495408\tAccuracy: 93.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27000/60000 (45%)]\tLoss: 1.483378\tAccuracy: 93.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27050/60000 (45%)]\tLoss: 1.489746\tAccuracy: 93.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27100/60000 (45%)]\tLoss: 1.500872\tAccuracy: 93.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27150/60000 (45%)]\tLoss: 1.466965\tAccuracy: 93.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27200/60000 (45%)]\tLoss: 1.536349\tAccuracy: 93.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27250/60000 (45%)]\tLoss: 1.461244\tAccuracy: 93.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27300/60000 (46%)]\tLoss: 1.490307\tAccuracy: 93.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27350/60000 (46%)]\tLoss: 1.491775\tAccuracy: 93.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27400/60000 (46%)]\tLoss: 1.496205\tAccuracy: 93.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27450/60000 (46%)]\tLoss: 1.480814\tAccuracy: 93.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27500/60000 (46%)]\tLoss: 1.485204\tAccuracy: 93.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27550/60000 (46%)]\tLoss: 1.464908\tAccuracy: 93.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27600/60000 (46%)]\tLoss: 1.509078\tAccuracy: 93.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27650/60000 (46%)]\tLoss: 1.473985\tAccuracy: 93.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27700/60000 (46%)]\tLoss: 1.534701\tAccuracy: 93.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27750/60000 (46%)]\tLoss: 1.497887\tAccuracy: 93.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27800/60000 (46%)]\tLoss: 1.462716\tAccuracy: 93.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27850/60000 (46%)]\tLoss: 1.552613\tAccuracy: 93.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27900/60000 (46%)]\tLoss: 1.499591\tAccuracy: 93.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [27950/60000 (47%)]\tLoss: 1.495213\tAccuracy: 93.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28000/60000 (47%)]\tLoss: 1.461268\tAccuracy: 93.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28050/60000 (47%)]\tLoss: 1.520437\tAccuracy: 93.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28100/60000 (47%)]\tLoss: 1.496533\tAccuracy: 93.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28150/60000 (47%)]\tLoss: 1.519689\tAccuracy: 93.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28200/60000 (47%)]\tLoss: 1.491244\tAccuracy: 93.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28250/60000 (47%)]\tLoss: 1.486917\tAccuracy: 93.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28300/60000 (47%)]\tLoss: 1.510297\tAccuracy: 93.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28350/60000 (47%)]\tLoss: 1.494026\tAccuracy: 93.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28400/60000 (47%)]\tLoss: 1.496841\tAccuracy: 93.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28450/60000 (47%)]\tLoss: 1.537814\tAccuracy: 93.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28500/60000 (48%)]\tLoss: 1.504446\tAccuracy: 93.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28550/60000 (48%)]\tLoss: 1.492475\tAccuracy: 93.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28600/60000 (48%)]\tLoss: 1.461449\tAccuracy: 93.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28650/60000 (48%)]\tLoss: 1.481585\tAccuracy: 93.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28700/60000 (48%)]\tLoss: 1.461234\tAccuracy: 93.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28750/60000 (48%)]\tLoss: 1.477313\tAccuracy: 93.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28800/60000 (48%)]\tLoss: 1.500605\tAccuracy: 93.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28850/60000 (48%)]\tLoss: 1.501528\tAccuracy: 93.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28900/60000 (48%)]\tLoss: 1.504555\tAccuracy: 93.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [28950/60000 (48%)]\tLoss: 1.494188\tAccuracy: 93.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29000/60000 (48%)]\tLoss: 1.486547\tAccuracy: 93.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29050/60000 (48%)]\tLoss: 1.519719\tAccuracy: 93.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29100/60000 (48%)]\tLoss: 1.461353\tAccuracy: 93.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29150/60000 (49%)]\tLoss: 1.521383\tAccuracy: 93.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29200/60000 (49%)]\tLoss: 1.526886\tAccuracy: 93.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29250/60000 (49%)]\tLoss: 1.506419\tAccuracy: 93.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29300/60000 (49%)]\tLoss: 1.495915\tAccuracy: 93.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29350/60000 (49%)]\tLoss: 1.494700\tAccuracy: 93.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29400/60000 (49%)]\tLoss: 1.483366\tAccuracy: 93.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29450/60000 (49%)]\tLoss: 1.483031\tAccuracy: 93.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29500/60000 (49%)]\tLoss: 1.507403\tAccuracy: 93.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29550/60000 (49%)]\tLoss: 1.494259\tAccuracy: 93.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29600/60000 (49%)]\tLoss: 1.461433\tAccuracy: 93.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29650/60000 (49%)]\tLoss: 1.481030\tAccuracy: 93.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29700/60000 (50%)]\tLoss: 1.469390\tAccuracy: 93.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29750/60000 (50%)]\tLoss: 1.485232\tAccuracy: 93.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29800/60000 (50%)]\tLoss: 1.461989\tAccuracy: 93.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29850/60000 (50%)]\tLoss: 1.496548\tAccuracy: 93.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29900/60000 (50%)]\tLoss: 1.523381\tAccuracy: 93.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [29950/60000 (50%)]\tLoss: 1.480916\tAccuracy: 93.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30000/60000 (50%)]\tLoss: 1.485650\tAccuracy: 93.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30050/60000 (50%)]\tLoss: 1.509057\tAccuracy: 93.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30100/60000 (50%)]\tLoss: 1.484980\tAccuracy: 93.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30150/60000 (50%)]\tLoss: 1.476940\tAccuracy: 93.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30200/60000 (50%)]\tLoss: 1.467256\tAccuracy: 93.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30250/60000 (50%)]\tLoss: 1.461327\tAccuracy: 93.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30300/60000 (50%)]\tLoss: 1.484913\tAccuracy: 93.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30350/60000 (51%)]\tLoss: 1.505007\tAccuracy: 93.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30400/60000 (51%)]\tLoss: 1.470326\tAccuracy: 93.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30450/60000 (51%)]\tLoss: 1.481678\tAccuracy: 93.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30500/60000 (51%)]\tLoss: 1.493663\tAccuracy: 93.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30550/60000 (51%)]\tLoss: 1.483151\tAccuracy: 93.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30600/60000 (51%)]\tLoss: 1.499794\tAccuracy: 93.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30650/60000 (51%)]\tLoss: 1.461156\tAccuracy: 93.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30700/60000 (51%)]\tLoss: 1.464218\tAccuracy: 93.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30750/60000 (51%)]\tLoss: 1.488414\tAccuracy: 93.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30800/60000 (51%)]\tLoss: 1.480674\tAccuracy: 93.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30850/60000 (51%)]\tLoss: 1.461425\tAccuracy: 93.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30900/60000 (52%)]\tLoss: 1.493522\tAccuracy: 93.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [30950/60000 (52%)]\tLoss: 1.483017\tAccuracy: 93.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31000/60000 (52%)]\tLoss: 1.482874\tAccuracy: 93.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31050/60000 (52%)]\tLoss: 1.484993\tAccuracy: 93.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31100/60000 (52%)]\tLoss: 1.461386\tAccuracy: 93.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31150/60000 (52%)]\tLoss: 1.463032\tAccuracy: 93.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31200/60000 (52%)]\tLoss: 1.470920\tAccuracy: 93.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31250/60000 (52%)]\tLoss: 1.480375\tAccuracy: 93.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31300/60000 (52%)]\tLoss: 1.465190\tAccuracy: 93.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31350/60000 (52%)]\tLoss: 1.490381\tAccuracy: 93.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31400/60000 (52%)]\tLoss: 1.506171\tAccuracy: 93.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31450/60000 (52%)]\tLoss: 1.462291\tAccuracy: 93.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31500/60000 (52%)]\tLoss: 1.485020\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31550/60000 (53%)]\tLoss: 1.498916\tAccuracy: 93.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31600/60000 (53%)]\tLoss: 1.462872\tAccuracy: 93.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31650/60000 (53%)]\tLoss: 1.474821\tAccuracy: 93.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31700/60000 (53%)]\tLoss: 1.522945\tAccuracy: 93.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31750/60000 (53%)]\tLoss: 1.485661\tAccuracy: 93.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31800/60000 (53%)]\tLoss: 1.463758\tAccuracy: 93.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31850/60000 (53%)]\tLoss: 1.494815\tAccuracy: 93.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31900/60000 (53%)]\tLoss: 1.480120\tAccuracy: 93.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [31950/60000 (53%)]\tLoss: 1.483409\tAccuracy: 93.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32000/60000 (53%)]\tLoss: 1.461381\tAccuracy: 93.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32050/60000 (53%)]\tLoss: 1.491804\tAccuracy: 93.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32100/60000 (54%)]\tLoss: 1.461165\tAccuracy: 93.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32150/60000 (54%)]\tLoss: 1.504386\tAccuracy: 93.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32200/60000 (54%)]\tLoss: 1.461591\tAccuracy: 93.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32250/60000 (54%)]\tLoss: 1.482003\tAccuracy: 93.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32300/60000 (54%)]\tLoss: 1.489035\tAccuracy: 93.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32350/60000 (54%)]\tLoss: 1.474046\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32400/60000 (54%)]\tLoss: 1.494967\tAccuracy: 93.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32450/60000 (54%)]\tLoss: 1.480238\tAccuracy: 93.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32500/60000 (54%)]\tLoss: 1.461355\tAccuracy: 93.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32550/60000 (54%)]\tLoss: 1.462549\tAccuracy: 93.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32600/60000 (54%)]\tLoss: 1.519197\tAccuracy: 93.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32650/60000 (54%)]\tLoss: 1.461798\tAccuracy: 93.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32700/60000 (54%)]\tLoss: 1.484180\tAccuracy: 93.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32750/60000 (55%)]\tLoss: 1.468977\tAccuracy: 93.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32800/60000 (55%)]\tLoss: 1.490878\tAccuracy: 93.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32850/60000 (55%)]\tLoss: 1.501742\tAccuracy: 93.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32900/60000 (55%)]\tLoss: 1.481360\tAccuracy: 93.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [32950/60000 (55%)]\tLoss: 1.481508\tAccuracy: 93.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33000/60000 (55%)]\tLoss: 1.461575\tAccuracy: 93.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33050/60000 (55%)]\tLoss: 1.502049\tAccuracy: 93.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33100/60000 (55%)]\tLoss: 1.474751\tAccuracy: 93.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33150/60000 (55%)]\tLoss: 1.537570\tAccuracy: 93.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33200/60000 (55%)]\tLoss: 1.505538\tAccuracy: 93.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33250/60000 (55%)]\tLoss: 1.470537\tAccuracy: 93.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33300/60000 (56%)]\tLoss: 1.468032\tAccuracy: 93.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33350/60000 (56%)]\tLoss: 1.509261\tAccuracy: 93.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33400/60000 (56%)]\tLoss: 1.492076\tAccuracy: 93.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33450/60000 (56%)]\tLoss: 1.555292\tAccuracy: 93.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33500/60000 (56%)]\tLoss: 1.521669\tAccuracy: 93.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33550/60000 (56%)]\tLoss: 1.519872\tAccuracy: 93.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33600/60000 (56%)]\tLoss: 1.488772\tAccuracy: 93.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33650/60000 (56%)]\tLoss: 1.479254\tAccuracy: 93.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33700/60000 (56%)]\tLoss: 1.515127\tAccuracy: 93.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33750/60000 (56%)]\tLoss: 1.462216\tAccuracy: 93.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33800/60000 (56%)]\tLoss: 1.484127\tAccuracy: 93.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33850/60000 (56%)]\tLoss: 1.483540\tAccuracy: 93.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33900/60000 (56%)]\tLoss: 1.496698\tAccuracy: 93.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [33950/60000 (57%)]\tLoss: 1.496510\tAccuracy: 93.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34000/60000 (57%)]\tLoss: 1.537541\tAccuracy: 93.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34050/60000 (57%)]\tLoss: 1.521091\tAccuracy: 93.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34100/60000 (57%)]\tLoss: 1.462448\tAccuracy: 93.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34150/60000 (57%)]\tLoss: 1.507005\tAccuracy: 94.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34200/60000 (57%)]\tLoss: 1.499573\tAccuracy: 94.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34250/60000 (57%)]\tLoss: 1.514983\tAccuracy: 94.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34300/60000 (57%)]\tLoss: 1.483075\tAccuracy: 94.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34350/60000 (57%)]\tLoss: 1.481372\tAccuracy: 94.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34400/60000 (57%)]\tLoss: 1.480844\tAccuracy: 94.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34450/60000 (57%)]\tLoss: 1.481236\tAccuracy: 94.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34500/60000 (58%)]\tLoss: 1.469750\tAccuracy: 94.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34550/60000 (58%)]\tLoss: 1.483255\tAccuracy: 94.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34600/60000 (58%)]\tLoss: 1.497003\tAccuracy: 94.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34650/60000 (58%)]\tLoss: 1.461442\tAccuracy: 94.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34700/60000 (58%)]\tLoss: 1.500794\tAccuracy: 94.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34750/60000 (58%)]\tLoss: 1.470999\tAccuracy: 94.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34800/60000 (58%)]\tLoss: 1.501605\tAccuracy: 94.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34850/60000 (58%)]\tLoss: 1.480842\tAccuracy: 94.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34900/60000 (58%)]\tLoss: 1.482119\tAccuracy: 94.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [34950/60000 (58%)]\tLoss: 1.461518\tAccuracy: 94.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35000/60000 (58%)]\tLoss: 1.512624\tAccuracy: 94.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35050/60000 (58%)]\tLoss: 1.488097\tAccuracy: 94.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35100/60000 (58%)]\tLoss: 1.501527\tAccuracy: 94.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35150/60000 (59%)]\tLoss: 1.483193\tAccuracy: 94.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35200/60000 (59%)]\tLoss: 1.462668\tAccuracy: 94.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35250/60000 (59%)]\tLoss: 1.466938\tAccuracy: 94.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35300/60000 (59%)]\tLoss: 1.482454\tAccuracy: 94.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35350/60000 (59%)]\tLoss: 1.501532\tAccuracy: 94.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35400/60000 (59%)]\tLoss: 1.501571\tAccuracy: 94.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35450/60000 (59%)]\tLoss: 1.481202\tAccuracy: 94.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35500/60000 (59%)]\tLoss: 1.512602\tAccuracy: 94.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35550/60000 (59%)]\tLoss: 1.511966\tAccuracy: 94.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35600/60000 (59%)]\tLoss: 1.487177\tAccuracy: 94.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35650/60000 (59%)]\tLoss: 1.489122\tAccuracy: 94.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35700/60000 (60%)]\tLoss: 1.461173\tAccuracy: 94.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35750/60000 (60%)]\tLoss: 1.500012\tAccuracy: 94.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35800/60000 (60%)]\tLoss: 1.489578\tAccuracy: 94.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35850/60000 (60%)]\tLoss: 1.482914\tAccuracy: 94.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35900/60000 (60%)]\tLoss: 1.506308\tAccuracy: 94.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [35950/60000 (60%)]\tLoss: 1.529873\tAccuracy: 94.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36000/60000 (60%)]\tLoss: 1.552420\tAccuracy: 94.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36050/60000 (60%)]\tLoss: 1.461810\tAccuracy: 94.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36100/60000 (60%)]\tLoss: 1.524390\tAccuracy: 94.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36150/60000 (60%)]\tLoss: 1.482184\tAccuracy: 94.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36200/60000 (60%)]\tLoss: 1.468860\tAccuracy: 94.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36250/60000 (60%)]\tLoss: 1.516380\tAccuracy: 94.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36300/60000 (60%)]\tLoss: 1.461520\tAccuracy: 94.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36350/60000 (61%)]\tLoss: 1.480795\tAccuracy: 94.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36400/60000 (61%)]\tLoss: 1.521170\tAccuracy: 94.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36450/60000 (61%)]\tLoss: 1.482558\tAccuracy: 94.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36500/60000 (61%)]\tLoss: 1.500243\tAccuracy: 94.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36550/60000 (61%)]\tLoss: 1.488640\tAccuracy: 94.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36600/60000 (61%)]\tLoss: 1.485565\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36650/60000 (61%)]\tLoss: 1.473112\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36700/60000 (61%)]\tLoss: 1.525226\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36750/60000 (61%)]\tLoss: 1.508994\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36800/60000 (61%)]\tLoss: 1.500151\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36850/60000 (61%)]\tLoss: 1.601399\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36900/60000 (62%)]\tLoss: 1.506028\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [36950/60000 (62%)]\tLoss: 1.535052\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37000/60000 (62%)]\tLoss: 1.502000\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37050/60000 (62%)]\tLoss: 1.538255\tAccuracy: 94.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37100/60000 (62%)]\tLoss: 1.502079\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37150/60000 (62%)]\tLoss: 1.489874\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37200/60000 (62%)]\tLoss: 1.513745\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37250/60000 (62%)]\tLoss: 1.496821\tAccuracy: 94.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37300/60000 (62%)]\tLoss: 1.482835\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37350/60000 (62%)]\tLoss: 1.485407\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37400/60000 (62%)]\tLoss: 1.508684\tAccuracy: 94.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37450/60000 (62%)]\tLoss: 1.462750\tAccuracy: 94.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37500/60000 (62%)]\tLoss: 1.463044\tAccuracy: 94.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37550/60000 (63%)]\tLoss: 1.521236\tAccuracy: 94.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37600/60000 (63%)]\tLoss: 1.473797\tAccuracy: 94.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37650/60000 (63%)]\tLoss: 1.461460\tAccuracy: 94.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37700/60000 (63%)]\tLoss: 1.496586\tAccuracy: 94.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37750/60000 (63%)]\tLoss: 1.517913\tAccuracy: 94.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37800/60000 (63%)]\tLoss: 1.511951\tAccuracy: 94.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37850/60000 (63%)]\tLoss: 1.481537\tAccuracy: 94.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37900/60000 (63%)]\tLoss: 1.462448\tAccuracy: 94.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [37950/60000 (63%)]\tLoss: 1.478328\tAccuracy: 94.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38000/60000 (63%)]\tLoss: 1.493350\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38050/60000 (63%)]\tLoss: 1.540701\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38100/60000 (64%)]\tLoss: 1.520265\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38150/60000 (64%)]\tLoss: 1.496760\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38200/60000 (64%)]\tLoss: 1.535136\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38250/60000 (64%)]\tLoss: 1.508115\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38300/60000 (64%)]\tLoss: 1.490260\tAccuracy: 94.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38350/60000 (64%)]\tLoss: 1.469733\tAccuracy: 94.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38400/60000 (64%)]\tLoss: 1.507621\tAccuracy: 94.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38450/60000 (64%)]\tLoss: 1.464219\tAccuracy: 94.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38500/60000 (64%)]\tLoss: 1.490902\tAccuracy: 94.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38550/60000 (64%)]\tLoss: 1.462277\tAccuracy: 94.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38600/60000 (64%)]\tLoss: 1.496913\tAccuracy: 94.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38650/60000 (64%)]\tLoss: 1.481223\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38700/60000 (64%)]\tLoss: 1.481856\tAccuracy: 94.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38750/60000 (65%)]\tLoss: 1.461438\tAccuracy: 94.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38800/60000 (65%)]\tLoss: 1.501247\tAccuracy: 94.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38850/60000 (65%)]\tLoss: 1.480804\tAccuracy: 94.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38900/60000 (65%)]\tLoss: 1.491694\tAccuracy: 94.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [38950/60000 (65%)]\tLoss: 1.461624\tAccuracy: 94.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39000/60000 (65%)]\tLoss: 1.482690\tAccuracy: 94.35%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39050/60000 (65%)]\tLoss: 1.479504\tAccuracy: 94.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39100/60000 (65%)]\tLoss: 1.482207\tAccuracy: 94.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39150/60000 (65%)]\tLoss: 1.524841\tAccuracy: 94.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39200/60000 (65%)]\tLoss: 1.499104\tAccuracy: 94.36%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39250/60000 (65%)]\tLoss: 1.473082\tAccuracy: 94.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39300/60000 (66%)]\tLoss: 1.497886\tAccuracy: 94.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39350/60000 (66%)]\tLoss: 1.481586\tAccuracy: 94.37%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39400/60000 (66%)]\tLoss: 1.478891\tAccuracy: 94.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39450/60000 (66%)]\tLoss: 1.475945\tAccuracy: 94.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39500/60000 (66%)]\tLoss: 1.497558\tAccuracy: 94.38%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39550/60000 (66%)]\tLoss: 1.493697\tAccuracy: 94.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39600/60000 (66%)]\tLoss: 1.479662\tAccuracy: 94.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39650/60000 (66%)]\tLoss: 1.491677\tAccuracy: 94.39%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39700/60000 (66%)]\tLoss: 1.481205\tAccuracy: 94.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39750/60000 (66%)]\tLoss: 1.514624\tAccuracy: 94.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39800/60000 (66%)]\tLoss: 1.462399\tAccuracy: 94.40%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39850/60000 (66%)]\tLoss: 1.491548\tAccuracy: 94.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39900/60000 (66%)]\tLoss: 1.503199\tAccuracy: 94.41%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [39950/60000 (67%)]\tLoss: 1.462450\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40000/60000 (67%)]\tLoss: 1.502781\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40050/60000 (67%)]\tLoss: 1.474431\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40100/60000 (67%)]\tLoss: 1.534559\tAccuracy: 94.42%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40150/60000 (67%)]\tLoss: 1.461176\tAccuracy: 94.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40200/60000 (67%)]\tLoss: 1.480703\tAccuracy: 94.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40250/60000 (67%)]\tLoss: 1.521090\tAccuracy: 94.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40300/60000 (67%)]\tLoss: 1.535833\tAccuracy: 94.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40350/60000 (67%)]\tLoss: 1.481147\tAccuracy: 94.43%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40400/60000 (67%)]\tLoss: 1.469310\tAccuracy: 94.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40450/60000 (67%)]\tLoss: 1.478079\tAccuracy: 94.44%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40500/60000 (68%)]\tLoss: 1.481433\tAccuracy: 94.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40550/60000 (68%)]\tLoss: 1.479455\tAccuracy: 94.45%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40600/60000 (68%)]\tLoss: 1.481211\tAccuracy: 94.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40650/60000 (68%)]\tLoss: 1.461229\tAccuracy: 94.46%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40700/60000 (68%)]\tLoss: 1.498191\tAccuracy: 94.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40750/60000 (68%)]\tLoss: 1.473381\tAccuracy: 94.47%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40800/60000 (68%)]\tLoss: 1.461244\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40850/60000 (68%)]\tLoss: 1.502339\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40900/60000 (68%)]\tLoss: 1.517287\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [40950/60000 (68%)]\tLoss: 1.480373\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41000/60000 (68%)]\tLoss: 1.501272\tAccuracy: 94.48%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41050/60000 (68%)]\tLoss: 1.501549\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41100/60000 (68%)]\tLoss: 1.472943\tAccuracy: 94.49%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41150/60000 (69%)]\tLoss: 1.467126\tAccuracy: 94.50%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41200/60000 (69%)]\tLoss: 1.461219\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41250/60000 (69%)]\tLoss: 1.504503\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41300/60000 (69%)]\tLoss: 1.480975\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41350/60000 (69%)]\tLoss: 1.501941\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41400/60000 (69%)]\tLoss: 1.508711\tAccuracy: 94.51%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41450/60000 (69%)]\tLoss: 1.500246\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41500/60000 (69%)]\tLoss: 1.489301\tAccuracy: 94.52%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41550/60000 (69%)]\tLoss: 1.471930\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41600/60000 (69%)]\tLoss: 1.498990\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41650/60000 (69%)]\tLoss: 1.496178\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41700/60000 (70%)]\tLoss: 1.533545\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41750/60000 (70%)]\tLoss: 1.497419\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41800/60000 (70%)]\tLoss: 1.464957\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41850/60000 (70%)]\tLoss: 1.515283\tAccuracy: 94.53%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41900/60000 (70%)]\tLoss: 1.482643\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [41950/60000 (70%)]\tLoss: 1.513364\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42000/60000 (70%)]\tLoss: 1.481714\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42050/60000 (70%)]\tLoss: 1.520008\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42100/60000 (70%)]\tLoss: 1.506746\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42150/60000 (70%)]\tLoss: 1.501169\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42200/60000 (70%)]\tLoss: 1.493752\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42250/60000 (70%)]\tLoss: 1.539876\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42300/60000 (70%)]\tLoss: 1.525968\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42350/60000 (71%)]\tLoss: 1.461865\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42400/60000 (71%)]\tLoss: 1.500998\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42450/60000 (71%)]\tLoss: 1.533811\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42500/60000 (71%)]\tLoss: 1.508383\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42550/60000 (71%)]\tLoss: 1.563515\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42600/60000 (71%)]\tLoss: 1.487295\tAccuracy: 94.54%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42650/60000 (71%)]\tLoss: 1.481639\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42700/60000 (71%)]\tLoss: 1.461258\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42750/60000 (71%)]\tLoss: 1.462705\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42800/60000 (71%)]\tLoss: 1.544471\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42850/60000 (71%)]\tLoss: 1.500419\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42900/60000 (72%)]\tLoss: 1.522200\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [42950/60000 (72%)]\tLoss: 1.494337\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43000/60000 (72%)]\tLoss: 1.554520\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43050/60000 (72%)]\tLoss: 1.518848\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43100/60000 (72%)]\tLoss: 1.537114\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43150/60000 (72%)]\tLoss: 1.481341\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43200/60000 (72%)]\tLoss: 1.556034\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43250/60000 (72%)]\tLoss: 1.516920\tAccuracy: 94.55%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43300/60000 (72%)]\tLoss: 1.487252\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43350/60000 (72%)]\tLoss: 1.499905\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43400/60000 (72%)]\tLoss: 1.479643\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43450/60000 (72%)]\tLoss: 1.511263\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43500/60000 (72%)]\tLoss: 1.521167\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43550/60000 (73%)]\tLoss: 1.499037\tAccuracy: 94.56%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43600/60000 (73%)]\tLoss: 1.461903\tAccuracy: 94.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43650/60000 (73%)]\tLoss: 1.461420\tAccuracy: 94.57%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43700/60000 (73%)]\tLoss: 1.503104\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43750/60000 (73%)]\tLoss: 1.504781\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43800/60000 (73%)]\tLoss: 1.481223\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43850/60000 (73%)]\tLoss: 1.481253\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43900/60000 (73%)]\tLoss: 1.538849\tAccuracy: 94.58%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [43950/60000 (73%)]\tLoss: 1.483148\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44000/60000 (73%)]\tLoss: 1.517682\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44050/60000 (73%)]\tLoss: 1.465967\tAccuracy: 94.59%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44100/60000 (74%)]\tLoss: 1.482105\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44150/60000 (74%)]\tLoss: 1.485852\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44200/60000 (74%)]\tLoss: 1.501213\tAccuracy: 94.60%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44250/60000 (74%)]\tLoss: 1.462581\tAccuracy: 94.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44300/60000 (74%)]\tLoss: 1.467854\tAccuracy: 94.61%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44350/60000 (74%)]\tLoss: 1.462294\tAccuracy: 94.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44400/60000 (74%)]\tLoss: 1.480099\tAccuracy: 94.62%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44450/60000 (74%)]\tLoss: 1.473404\tAccuracy: 94.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44500/60000 (74%)]\tLoss: 1.499820\tAccuracy: 94.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44550/60000 (74%)]\tLoss: 1.474548\tAccuracy: 94.63%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44600/60000 (74%)]\tLoss: 1.481160\tAccuracy: 94.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44650/60000 (74%)]\tLoss: 1.501353\tAccuracy: 94.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44700/60000 (74%)]\tLoss: 1.463277\tAccuracy: 94.64%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44750/60000 (75%)]\tLoss: 1.461233\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44800/60000 (75%)]\tLoss: 1.502133\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44850/60000 (75%)]\tLoss: 1.501153\tAccuracy: 94.65%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44900/60000 (75%)]\tLoss: 1.461847\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [44950/60000 (75%)]\tLoss: 1.461273\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45000/60000 (75%)]\tLoss: 1.513168\tAccuracy: 94.66%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45050/60000 (75%)]\tLoss: 1.461813\tAccuracy: 94.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45100/60000 (75%)]\tLoss: 1.485215\tAccuracy: 94.67%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45150/60000 (75%)]\tLoss: 1.465093\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45200/60000 (75%)]\tLoss: 1.461255\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45250/60000 (75%)]\tLoss: 1.502981\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45300/60000 (76%)]\tLoss: 1.495481\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45350/60000 (76%)]\tLoss: 1.518535\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45400/60000 (76%)]\tLoss: 1.558678\tAccuracy: 94.68%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45450/60000 (76%)]\tLoss: 1.461189\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45500/60000 (76%)]\tLoss: 1.482158\tAccuracy: 94.69%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45550/60000 (76%)]\tLoss: 1.461856\tAccuracy: 94.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45600/60000 (76%)]\tLoss: 1.506381\tAccuracy: 94.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45650/60000 (76%)]\tLoss: 1.461391\tAccuracy: 94.70%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45700/60000 (76%)]\tLoss: 1.482992\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45750/60000 (76%)]\tLoss: 1.480274\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45800/60000 (76%)]\tLoss: 1.479300\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45850/60000 (76%)]\tLoss: 1.501124\tAccuracy: 94.71%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45900/60000 (76%)]\tLoss: 1.506816\tAccuracy: 94.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [45950/60000 (77%)]\tLoss: 1.525877\tAccuracy: 94.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46000/60000 (77%)]\tLoss: 1.481915\tAccuracy: 94.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46050/60000 (77%)]\tLoss: 1.481154\tAccuracy: 94.72%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46100/60000 (77%)]\tLoss: 1.462502\tAccuracy: 94.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46150/60000 (77%)]\tLoss: 1.486704\tAccuracy: 94.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46200/60000 (77%)]\tLoss: 1.482066\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46250/60000 (77%)]\tLoss: 1.525740\tAccuracy: 94.73%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46300/60000 (77%)]\tLoss: 1.487856\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46350/60000 (77%)]\tLoss: 1.475679\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46400/60000 (77%)]\tLoss: 1.501364\tAccuracy: 94.74%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46450/60000 (77%)]\tLoss: 1.479731\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46500/60000 (78%)]\tLoss: 1.499239\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46550/60000 (78%)]\tLoss: 1.485901\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46600/60000 (78%)]\tLoss: 1.482186\tAccuracy: 94.75%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46650/60000 (78%)]\tLoss: 1.483583\tAccuracy: 94.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46700/60000 (78%)]\tLoss: 1.511626\tAccuracy: 94.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46750/60000 (78%)]\tLoss: 1.461982\tAccuracy: 94.76%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46800/60000 (78%)]\tLoss: 1.489939\tAccuracy: 94.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46850/60000 (78%)]\tLoss: 1.462499\tAccuracy: 94.77%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46900/60000 (78%)]\tLoss: 1.461637\tAccuracy: 94.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [46950/60000 (78%)]\tLoss: 1.500984\tAccuracy: 94.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47000/60000 (78%)]\tLoss: 1.514317\tAccuracy: 94.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47050/60000 (78%)]\tLoss: 1.511305\tAccuracy: 94.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47100/60000 (78%)]\tLoss: 1.493631\tAccuracy: 94.78%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47150/60000 (79%)]\tLoss: 1.464391\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47200/60000 (79%)]\tLoss: 1.461514\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47250/60000 (79%)]\tLoss: 1.547224\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47300/60000 (79%)]\tLoss: 1.509059\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47350/60000 (79%)]\tLoss: 1.491926\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47400/60000 (79%)]\tLoss: 1.514833\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47450/60000 (79%)]\tLoss: 1.481163\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47500/60000 (79%)]\tLoss: 1.499490\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47550/60000 (79%)]\tLoss: 1.517886\tAccuracy: 94.79%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47600/60000 (79%)]\tLoss: 1.463869\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47650/60000 (79%)]\tLoss: 1.484584\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47700/60000 (80%)]\tLoss: 1.499967\tAccuracy: 94.80%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47750/60000 (80%)]\tLoss: 1.483568\tAccuracy: 94.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47800/60000 (80%)]\tLoss: 1.461701\tAccuracy: 94.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47850/60000 (80%)]\tLoss: 1.502015\tAccuracy: 94.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47900/60000 (80%)]\tLoss: 1.540849\tAccuracy: 94.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [47950/60000 (80%)]\tLoss: 1.461259\tAccuracy: 94.81%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48000/60000 (80%)]\tLoss: 1.461498\tAccuracy: 94.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48050/60000 (80%)]\tLoss: 1.514890\tAccuracy: 94.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48100/60000 (80%)]\tLoss: 1.521084\tAccuracy: 94.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48150/60000 (80%)]\tLoss: 1.476246\tAccuracy: 94.82%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48200/60000 (80%)]\tLoss: 1.461274\tAccuracy: 94.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48250/60000 (80%)]\tLoss: 1.462300\tAccuracy: 94.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48300/60000 (80%)]\tLoss: 1.522291\tAccuracy: 94.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48350/60000 (81%)]\tLoss: 1.474612\tAccuracy: 94.83%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48400/60000 (81%)]\tLoss: 1.479146\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48450/60000 (81%)]\tLoss: 1.485979\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48500/60000 (81%)]\tLoss: 1.498940\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48550/60000 (81%)]\tLoss: 1.481298\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48600/60000 (81%)]\tLoss: 1.496644\tAccuracy: 94.84%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48650/60000 (81%)]\tLoss: 1.497203\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48700/60000 (81%)]\tLoss: 1.509137\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48750/60000 (81%)]\tLoss: 1.469989\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48800/60000 (81%)]\tLoss: 1.507887\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48850/60000 (81%)]\tLoss: 1.493904\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48900/60000 (82%)]\tLoss: 1.517429\tAccuracy: 94.85%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [48950/60000 (82%)]\tLoss: 1.490809\tAccuracy: 94.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49000/60000 (82%)]\tLoss: 1.461299\tAccuracy: 94.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49050/60000 (82%)]\tLoss: 1.500003\tAccuracy: 94.86%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49100/60000 (82%)]\tLoss: 1.464035\tAccuracy: 94.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49150/60000 (82%)]\tLoss: 1.474199\tAccuracy: 94.87%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49200/60000 (82%)]\tLoss: 1.502315\tAccuracy: 94.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49250/60000 (82%)]\tLoss: 1.466334\tAccuracy: 94.88%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49300/60000 (82%)]\tLoss: 1.477805\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49350/60000 (82%)]\tLoss: 1.462538\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49400/60000 (82%)]\tLoss: 1.470265\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49450/60000 (82%)]\tLoss: 1.522350\tAccuracy: 94.89%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49500/60000 (82%)]\tLoss: 1.461169\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49550/60000 (83%)]\tLoss: 1.494066\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49600/60000 (83%)]\tLoss: 1.513330\tAccuracy: 94.90%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49650/60000 (83%)]\tLoss: 1.465073\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49700/60000 (83%)]\tLoss: 1.482135\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49750/60000 (83%)]\tLoss: 1.496847\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49800/60000 (83%)]\tLoss: 1.520751\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49850/60000 (83%)]\tLoss: 1.463200\tAccuracy: 94.91%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49900/60000 (83%)]\tLoss: 1.468752\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [49950/60000 (83%)]\tLoss: 1.464584\tAccuracy: 94.92%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50000/60000 (83%)]\tLoss: 1.462474\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50050/60000 (83%)]\tLoss: 1.489012\tAccuracy: 94.93%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50100/60000 (84%)]\tLoss: 1.466626\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50150/60000 (84%)]\tLoss: 1.473362\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50200/60000 (84%)]\tLoss: 1.488612\tAccuracy: 94.94%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50250/60000 (84%)]\tLoss: 1.475822\tAccuracy: 94.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50300/60000 (84%)]\tLoss: 1.461556\tAccuracy: 94.95%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50350/60000 (84%)]\tLoss: 1.463266\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50400/60000 (84%)]\tLoss: 1.482346\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50450/60000 (84%)]\tLoss: 1.482293\tAccuracy: 94.96%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50500/60000 (84%)]\tLoss: 1.490488\tAccuracy: 94.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50550/60000 (84%)]\tLoss: 1.461271\tAccuracy: 94.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50600/60000 (84%)]\tLoss: 1.482901\tAccuracy: 94.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50650/60000 (84%)]\tLoss: 1.499062\tAccuracy: 94.97%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50700/60000 (84%)]\tLoss: 1.461583\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50750/60000 (85%)]\tLoss: 1.510344\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50800/60000 (85%)]\tLoss: 1.527638\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50850/60000 (85%)]\tLoss: 1.462371\tAccuracy: 94.98%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50900/60000 (85%)]\tLoss: 1.461555\tAccuracy: 94.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [50950/60000 (85%)]\tLoss: 1.461159\tAccuracy: 94.99%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51000/60000 (85%)]\tLoss: 1.461185\tAccuracy: 95.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51050/60000 (85%)]\tLoss: 1.475317\tAccuracy: 95.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51100/60000 (85%)]\tLoss: 1.477594\tAccuracy: 95.00%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51150/60000 (85%)]\tLoss: 1.463331\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51200/60000 (85%)]\tLoss: 1.463961\tAccuracy: 95.01%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51250/60000 (85%)]\tLoss: 1.462607\tAccuracy: 95.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51300/60000 (86%)]\tLoss: 1.461526\tAccuracy: 95.02%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51350/60000 (86%)]\tLoss: 1.481094\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51400/60000 (86%)]\tLoss: 1.473607\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51450/60000 (86%)]\tLoss: 1.463971\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51500/60000 (86%)]\tLoss: 1.503524\tAccuracy: 95.03%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51550/60000 (86%)]\tLoss: 1.470964\tAccuracy: 95.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51600/60000 (86%)]\tLoss: 1.461342\tAccuracy: 95.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51650/60000 (86%)]\tLoss: 1.501527\tAccuracy: 95.04%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51700/60000 (86%)]\tLoss: 1.483396\tAccuracy: 95.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51750/60000 (86%)]\tLoss: 1.461732\tAccuracy: 95.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51800/60000 (86%)]\tLoss: 1.500215\tAccuracy: 95.05%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51850/60000 (86%)]\tLoss: 1.463831\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51900/60000 (86%)]\tLoss: 1.504122\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [51950/60000 (87%)]\tLoss: 1.462121\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52000/60000 (87%)]\tLoss: 1.551976\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52050/60000 (87%)]\tLoss: 1.513201\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52100/60000 (87%)]\tLoss: 1.520068\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52150/60000 (87%)]\tLoss: 1.462891\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52200/60000 (87%)]\tLoss: 1.492471\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52250/60000 (87%)]\tLoss: 1.509811\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52300/60000 (87%)]\tLoss: 1.506108\tAccuracy: 95.06%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52350/60000 (87%)]\tLoss: 1.464775\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52400/60000 (87%)]\tLoss: 1.465061\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52450/60000 (87%)]\tLoss: 1.478235\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52500/60000 (88%)]\tLoss: 1.481318\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52550/60000 (88%)]\tLoss: 1.536031\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52600/60000 (88%)]\tLoss: 1.462068\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52650/60000 (88%)]\tLoss: 1.528904\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52700/60000 (88%)]\tLoss: 1.521177\tAccuracy: 95.07%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52750/60000 (88%)]\tLoss: 1.506226\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52800/60000 (88%)]\tLoss: 1.497078\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52850/60000 (88%)]\tLoss: 1.501623\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52900/60000 (88%)]\tLoss: 1.461244\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [52950/60000 (88%)]\tLoss: 1.497095\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53000/60000 (88%)]\tLoss: 1.481227\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53050/60000 (88%)]\tLoss: 1.530289\tAccuracy: 95.08%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53100/60000 (88%)]\tLoss: 1.461167\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53150/60000 (89%)]\tLoss: 1.474782\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53200/60000 (89%)]\tLoss: 1.481417\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53250/60000 (89%)]\tLoss: 1.493422\tAccuracy: 95.09%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53300/60000 (89%)]\tLoss: 1.481121\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53350/60000 (89%)]\tLoss: 1.479698\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53400/60000 (89%)]\tLoss: 1.461153\tAccuracy: 95.10%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53450/60000 (89%)]\tLoss: 1.465016\tAccuracy: 95.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53500/60000 (89%)]\tLoss: 1.504724\tAccuracy: 95.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53550/60000 (89%)]\tLoss: 1.501068\tAccuracy: 95.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53600/60000 (89%)]\tLoss: 1.465089\tAccuracy: 95.11%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53650/60000 (89%)]\tLoss: 1.479751\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53700/60000 (90%)]\tLoss: 1.483898\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53750/60000 (90%)]\tLoss: 1.470623\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53800/60000 (90%)]\tLoss: 1.537587\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53850/60000 (90%)]\tLoss: 1.501923\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53900/60000 (90%)]\tLoss: 1.501150\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [53950/60000 (90%)]\tLoss: 1.501396\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54000/60000 (90%)]\tLoss: 1.490505\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54050/60000 (90%)]\tLoss: 1.502219\tAccuracy: 95.12%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54100/60000 (90%)]\tLoss: 1.461155\tAccuracy: 95.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54150/60000 (90%)]\tLoss: 1.483767\tAccuracy: 95.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54200/60000 (90%)]\tLoss: 1.504290\tAccuracy: 95.13%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54250/60000 (90%)]\tLoss: 1.466568\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54300/60000 (90%)]\tLoss: 1.476847\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54350/60000 (91%)]\tLoss: 1.495165\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54400/60000 (91%)]\tLoss: 1.522023\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54450/60000 (91%)]\tLoss: 1.462477\tAccuracy: 95.14%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54500/60000 (91%)]\tLoss: 1.481199\tAccuracy: 95.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54550/60000 (91%)]\tLoss: 1.480548\tAccuracy: 95.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54600/60000 (91%)]\tLoss: 1.473676\tAccuracy: 95.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54650/60000 (91%)]\tLoss: 1.482147\tAccuracy: 95.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54700/60000 (91%)]\tLoss: 1.496421\tAccuracy: 95.15%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54750/60000 (91%)]\tLoss: 1.465408\tAccuracy: 95.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54800/60000 (91%)]\tLoss: 1.465941\tAccuracy: 95.16%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54850/60000 (91%)]\tLoss: 1.482827\tAccuracy: 95.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54900/60000 (92%)]\tLoss: 1.465248\tAccuracy: 95.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [54950/60000 (92%)]\tLoss: 1.480452\tAccuracy: 95.17%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55000/60000 (92%)]\tLoss: 1.461331\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55050/60000 (92%)]\tLoss: 1.482200\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55100/60000 (92%)]\tLoss: 1.496331\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55150/60000 (92%)]\tLoss: 1.514181\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55200/60000 (92%)]\tLoss: 1.473548\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55250/60000 (92%)]\tLoss: 1.477283\tAccuracy: 95.18%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55300/60000 (92%)]\tLoss: 1.503933\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55350/60000 (92%)]\tLoss: 1.461236\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55400/60000 (92%)]\tLoss: 1.485560\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55450/60000 (92%)]\tLoss: 1.494113\tAccuracy: 95.19%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55500/60000 (92%)]\tLoss: 1.461189\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55550/60000 (93%)]\tLoss: 1.482830\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55600/60000 (93%)]\tLoss: 1.485789\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55650/60000 (93%)]\tLoss: 1.481351\tAccuracy: 95.20%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55700/60000 (93%)]\tLoss: 1.466808\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55750/60000 (93%)]\tLoss: 1.461156\tAccuracy: 95.21%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55800/60000 (93%)]\tLoss: 1.462750\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55850/60000 (93%)]\tLoss: 1.461453\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55900/60000 (93%)]\tLoss: 1.483559\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [55950/60000 (93%)]\tLoss: 1.509401\tAccuracy: 95.22%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56000/60000 (93%)]\tLoss: 1.480620\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56050/60000 (93%)]\tLoss: 1.481284\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56100/60000 (94%)]\tLoss: 1.481156\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56150/60000 (94%)]\tLoss: 1.499672\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56200/60000 (94%)]\tLoss: 1.511261\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56250/60000 (94%)]\tLoss: 1.482629\tAccuracy: 95.23%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56300/60000 (94%)]\tLoss: 1.467145\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56350/60000 (94%)]\tLoss: 1.483971\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56400/60000 (94%)]\tLoss: 1.523791\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56450/60000 (94%)]\tLoss: 1.476264\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56500/60000 (94%)]\tLoss: 1.481413\tAccuracy: 95.24%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56550/60000 (94%)]\tLoss: 1.484310\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56600/60000 (94%)]\tLoss: 1.497511\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56650/60000 (94%)]\tLoss: 1.489329\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56700/60000 (94%)]\tLoss: 1.481251\tAccuracy: 95.25%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56750/60000 (95%)]\tLoss: 1.461315\tAccuracy: 95.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56800/60000 (95%)]\tLoss: 1.468564\tAccuracy: 95.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56850/60000 (95%)]\tLoss: 1.493993\tAccuracy: 95.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56900/60000 (95%)]\tLoss: 1.470079\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [56950/60000 (95%)]\tLoss: 1.523421\tAccuracy: 95.26%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57000/60000 (95%)]\tLoss: 1.482520\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57050/60000 (95%)]\tLoss: 1.522106\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57100/60000 (95%)]\tLoss: 1.523532\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57150/60000 (95%)]\tLoss: 1.488140\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57200/60000 (95%)]\tLoss: 1.501274\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57250/60000 (95%)]\tLoss: 1.479194\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57300/60000 (96%)]\tLoss: 1.525541\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57350/60000 (96%)]\tLoss: 1.481044\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57400/60000 (96%)]\tLoss: 1.481700\tAccuracy: 95.27%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57450/60000 (96%)]\tLoss: 1.473539\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57500/60000 (96%)]\tLoss: 1.481889\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57550/60000 (96%)]\tLoss: 1.461224\tAccuracy: 95.28%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57600/60000 (96%)]\tLoss: 1.480245\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57650/60000 (96%)]\tLoss: 1.471046\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57700/60000 (96%)]\tLoss: 1.495202\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57750/60000 (96%)]\tLoss: 1.461349\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57800/60000 (96%)]\tLoss: 1.484923\tAccuracy: 95.29%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57850/60000 (96%)]\tLoss: 1.497580\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57900/60000 (96%)]\tLoss: 1.481495\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [57950/60000 (97%)]\tLoss: 1.481385\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58000/60000 (97%)]\tLoss: 1.506057\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58050/60000 (97%)]\tLoss: 1.471936\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58100/60000 (97%)]\tLoss: 1.508737\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58150/60000 (97%)]\tLoss: 1.473792\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58200/60000 (97%)]\tLoss: 1.510970\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58250/60000 (97%)]\tLoss: 1.580106\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58300/60000 (97%)]\tLoss: 1.501705\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58350/60000 (97%)]\tLoss: 1.518144\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58400/60000 (97%)]\tLoss: 1.487745\tAccuracy: 95.30%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58450/60000 (97%)]\tLoss: 1.476875\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58500/60000 (98%)]\tLoss: 1.464316\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58550/60000 (98%)]\tLoss: 1.480279\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58600/60000 (98%)]\tLoss: 1.518207\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58650/60000 (98%)]\tLoss: 1.501987\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58700/60000 (98%)]\tLoss: 1.481492\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58750/60000 (98%)]\tLoss: 1.471720\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58800/60000 (98%)]\tLoss: 1.514367\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58850/60000 (98%)]\tLoss: 1.540907\tAccuracy: 95.31%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58900/60000 (98%)]\tLoss: 1.461275\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [58950/60000 (98%)]\tLoss: 1.471595\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59000/60000 (98%)]\tLoss: 1.481281\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59050/60000 (98%)]\tLoss: 1.532966\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59100/60000 (98%)]\tLoss: 1.511137\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59150/60000 (99%)]\tLoss: 1.480364\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59200/60000 (99%)]\tLoss: 1.505127\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59250/60000 (99%)]\tLoss: 1.485638\tAccuracy: 95.32%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59300/60000 (99%)]\tLoss: 1.461196\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59350/60000 (99%)]\tLoss: 1.462468\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59400/60000 (99%)]\tLoss: 1.471225\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59450/60000 (99%)]\tLoss: 1.461218\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59500/60000 (99%)]\tLoss: 1.462431\tAccuracy: 95.34%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59550/60000 (99%)]\tLoss: 1.568543\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59600/60000 (99%)]\tLoss: 1.556769\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59650/60000 (99%)]\tLoss: 1.486133\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59700/60000 (100%)]\tLoss: 1.540018\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59750/60000 (100%)]\tLoss: 1.498260\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59800/60000 (100%)]\tLoss: 1.525170\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59850/60000 (100%)]\tLoss: 1.519115\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59900/60000 (100%)]\tLoss: 1.493206\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 0 [59950/60000 (100%)]\tLoss: 1.482158\tAccuracy: 95.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [0/60000 (0%)]\tLoss: 1.503571\tAccuracy: 96.00%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50/60000 (0%)]\tLoss: 1.491601\tAccuracy: 97.00%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [100/60000 (0%)]\tLoss: 1.472673\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [150/60000 (0%)]\tLoss: 1.521063\tAccuracy: 96.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [200/60000 (0%)]\tLoss: 1.493010\tAccuracy: 96.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [250/60000 (0%)]\tLoss: 1.504414\tAccuracy: 96.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [300/60000 (0%)]\tLoss: 1.515092\tAccuracy: 96.00%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [350/60000 (1%)]\tLoss: 1.478320\tAccuracy: 96.25%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [400/60000 (1%)]\tLoss: 1.461174\tAccuracy: 96.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [450/60000 (1%)]\tLoss: 1.469548\tAccuracy: 97.00%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [500/60000 (1%)]\tLoss: 1.518984\tAccuracy: 96.73%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [550/60000 (1%)]\tLoss: 1.531756\tAccuracy: 96.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [600/60000 (1%)]\tLoss: 1.514786\tAccuracy: 96.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [650/60000 (1%)]\tLoss: 1.492580\tAccuracy: 96.29%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [700/60000 (1%)]\tLoss: 1.481391\tAccuracy: 96.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [750/60000 (1%)]\tLoss: 1.487121\tAccuracy: 96.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [800/60000 (1%)]\tLoss: 1.461433\tAccuracy: 96.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [850/60000 (1%)]\tLoss: 1.502557\tAccuracy: 96.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [900/60000 (2%)]\tLoss: 1.461192\tAccuracy: 96.74%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [950/60000 (2%)]\tLoss: 1.489110\tAccuracy: 96.80%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1000/60000 (2%)]\tLoss: 1.540420\tAccuracy: 96.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1050/60000 (2%)]\tLoss: 1.461206\tAccuracy: 96.73%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1100/60000 (2%)]\tLoss: 1.483194\tAccuracy: 96.78%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1150/60000 (2%)]\tLoss: 1.510097\tAccuracy: 96.75%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1200/60000 (2%)]\tLoss: 1.494422\tAccuracy: 96.80%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1250/60000 (2%)]\tLoss: 1.517733\tAccuracy: 96.77%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1300/60000 (2%)]\tLoss: 1.519053\tAccuracy: 96.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1350/60000 (2%)]\tLoss: 1.473981\tAccuracy: 96.71%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1400/60000 (2%)]\tLoss: 1.486254\tAccuracy: 96.76%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1450/60000 (2%)]\tLoss: 1.461154\tAccuracy: 96.87%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1500/60000 (2%)]\tLoss: 1.495079\tAccuracy: 96.84%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1550/60000 (3%)]\tLoss: 1.502220\tAccuracy: 96.81%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1600/60000 (3%)]\tLoss: 1.500236\tAccuracy: 96.79%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1650/60000 (3%)]\tLoss: 1.462316\tAccuracy: 96.88%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1700/60000 (3%)]\tLoss: 1.492145\tAccuracy: 96.86%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1750/60000 (3%)]\tLoss: 1.477999\tAccuracy: 96.89%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1800/60000 (3%)]\tLoss: 1.513766\tAccuracy: 96.86%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1850/60000 (3%)]\tLoss: 1.463605\tAccuracy: 96.95%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1900/60000 (3%)]\tLoss: 1.482003\tAccuracy: 96.97%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [1950/60000 (3%)]\tLoss: 1.486179\tAccuracy: 97.00%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2000/60000 (3%)]\tLoss: 1.485808\tAccuracy: 97.02%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2050/60000 (3%)]\tLoss: 1.501069\tAccuracy: 97.00%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2100/60000 (4%)]\tLoss: 1.482276\tAccuracy: 97.02%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2150/60000 (4%)]\tLoss: 1.461164\tAccuracy: 97.09%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2200/60000 (4%)]\tLoss: 1.481950\tAccuracy: 97.11%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2250/60000 (4%)]\tLoss: 1.461214\tAccuracy: 97.17%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2300/60000 (4%)]\tLoss: 1.461176\tAccuracy: 97.23%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2350/60000 (4%)]\tLoss: 1.462952\tAccuracy: 97.29%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2400/60000 (4%)]\tLoss: 1.474516\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2450/60000 (4%)]\tLoss: 1.484888\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2500/60000 (4%)]\tLoss: 1.484967\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2550/60000 (4%)]\tLoss: 1.545684\tAccuracy: 97.27%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2600/60000 (4%)]\tLoss: 1.520143\tAccuracy: 97.21%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2650/60000 (4%)]\tLoss: 1.468678\tAccuracy: 97.26%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2700/60000 (4%)]\tLoss: 1.461170\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2750/60000 (5%)]\tLoss: 1.481954\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2800/60000 (5%)]\tLoss: 1.465650\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2850/60000 (5%)]\tLoss: 1.461173\tAccuracy: 97.41%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2900/60000 (5%)]\tLoss: 1.461599\tAccuracy: 97.46%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [2950/60000 (5%)]\tLoss: 1.462358\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3000/60000 (5%)]\tLoss: 1.482116\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3050/60000 (5%)]\tLoss: 1.501207\tAccuracy: 97.48%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3100/60000 (5%)]\tLoss: 1.483620\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3150/60000 (5%)]\tLoss: 1.476796\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3200/60000 (5%)]\tLoss: 1.461406\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3250/60000 (5%)]\tLoss: 1.480482\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3300/60000 (6%)]\tLoss: 1.479524\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3350/60000 (6%)]\tLoss: 1.481163\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3400/60000 (6%)]\tLoss: 1.462290\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3450/60000 (6%)]\tLoss: 1.492644\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3500/60000 (6%)]\tLoss: 1.481136\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3550/60000 (6%)]\tLoss: 1.495316\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3600/60000 (6%)]\tLoss: 1.482362\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3650/60000 (6%)]\tLoss: 1.468430\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3700/60000 (6%)]\tLoss: 1.496397\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3750/60000 (6%)]\tLoss: 1.501260\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3800/60000 (6%)]\tLoss: 1.467486\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3850/60000 (6%)]\tLoss: 1.462451\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3900/60000 (6%)]\tLoss: 1.469546\tAccuracy: 97.70%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [3950/60000 (7%)]\tLoss: 1.497121\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4000/60000 (7%)]\tLoss: 1.502472\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4050/60000 (7%)]\tLoss: 1.499562\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4100/60000 (7%)]\tLoss: 1.484674\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4150/60000 (7%)]\tLoss: 1.461371\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4200/60000 (7%)]\tLoss: 1.465196\tAccuracy: 97.69%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4250/60000 (7%)]\tLoss: 1.518256\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4300/60000 (7%)]\tLoss: 1.518178\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4350/60000 (7%)]\tLoss: 1.583427\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4400/60000 (7%)]\tLoss: 1.519457\tAccuracy: 97.46%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4450/60000 (7%)]\tLoss: 1.461154\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4500/60000 (8%)]\tLoss: 1.501507\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4550/60000 (8%)]\tLoss: 1.464971\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4600/60000 (8%)]\tLoss: 1.481198\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4650/60000 (8%)]\tLoss: 1.512433\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4700/60000 (8%)]\tLoss: 1.501744\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4750/60000 (8%)]\tLoss: 1.480915\tAccuracy: 97.48%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4800/60000 (8%)]\tLoss: 1.482486\tAccuracy: 97.48%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4850/60000 (8%)]\tLoss: 1.515148\tAccuracy: 97.45%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4900/60000 (8%)]\tLoss: 1.461154\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [4950/60000 (8%)]\tLoss: 1.485730\tAccuracy: 97.48%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5000/60000 (8%)]\tLoss: 1.501382\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5050/60000 (8%)]\tLoss: 1.562953\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5100/60000 (8%)]\tLoss: 1.499288\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5150/60000 (9%)]\tLoss: 1.461776\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5200/60000 (9%)]\tLoss: 1.489619\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5250/60000 (9%)]\tLoss: 1.500127\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5300/60000 (9%)]\tLoss: 1.494523\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5350/60000 (9%)]\tLoss: 1.461157\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5400/60000 (9%)]\tLoss: 1.491430\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5450/60000 (9%)]\tLoss: 1.462641\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5500/60000 (9%)]\tLoss: 1.530821\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5550/60000 (9%)]\tLoss: 1.560006\tAccuracy: 97.30%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5600/60000 (9%)]\tLoss: 1.498770\tAccuracy: 97.29%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5650/60000 (9%)]\tLoss: 1.481154\tAccuracy: 97.30%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5700/60000 (10%)]\tLoss: 1.461157\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5750/60000 (10%)]\tLoss: 1.464478\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5800/60000 (10%)]\tLoss: 1.528302\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5850/60000 (10%)]\tLoss: 1.481138\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5900/60000 (10%)]\tLoss: 1.462899\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [5950/60000 (10%)]\tLoss: 1.463182\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6000/60000 (10%)]\tLoss: 1.461505\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6050/60000 (10%)]\tLoss: 1.478459\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6100/60000 (10%)]\tLoss: 1.488082\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6150/60000 (10%)]\tLoss: 1.480041\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6200/60000 (10%)]\tLoss: 1.524160\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6250/60000 (10%)]\tLoss: 1.472946\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6300/60000 (10%)]\tLoss: 1.505001\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6350/60000 (11%)]\tLoss: 1.522223\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6400/60000 (11%)]\tLoss: 1.481113\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6450/60000 (11%)]\tLoss: 1.461917\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6500/60000 (11%)]\tLoss: 1.498047\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6550/60000 (11%)]\tLoss: 1.477637\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6600/60000 (11%)]\tLoss: 1.489267\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6650/60000 (11%)]\tLoss: 1.481210\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6700/60000 (11%)]\tLoss: 1.500916\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6750/60000 (11%)]\tLoss: 1.482810\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6800/60000 (11%)]\tLoss: 1.481161\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6850/60000 (11%)]\tLoss: 1.487026\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6900/60000 (12%)]\tLoss: 1.482659\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [6950/60000 (12%)]\tLoss: 1.510588\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7000/60000 (12%)]\tLoss: 1.535145\tAccuracy: 97.29%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7050/60000 (12%)]\tLoss: 1.477856\tAccuracy: 97.30%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7100/60000 (12%)]\tLoss: 1.468923\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7150/60000 (12%)]\tLoss: 1.479197\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7200/60000 (12%)]\tLoss: 1.466590\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7250/60000 (12%)]\tLoss: 1.461437\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7300/60000 (12%)]\tLoss: 1.499611\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7350/60000 (12%)]\tLoss: 1.514248\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7400/60000 (12%)]\tLoss: 1.489118\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7450/60000 (12%)]\tLoss: 1.481132\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7500/60000 (12%)]\tLoss: 1.517981\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7550/60000 (13%)]\tLoss: 1.481758\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7600/60000 (13%)]\tLoss: 1.502439\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7650/60000 (13%)]\tLoss: 1.515946\tAccuracy: 97.29%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7700/60000 (13%)]\tLoss: 1.465924\tAccuracy: 97.30%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7750/60000 (13%)]\tLoss: 1.484115\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7800/60000 (13%)]\tLoss: 1.494544\tAccuracy: 97.30%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7850/60000 (13%)]\tLoss: 1.468574\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7900/60000 (13%)]\tLoss: 1.482024\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [7950/60000 (13%)]\tLoss: 1.492597\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8000/60000 (13%)]\tLoss: 1.462938\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8050/60000 (13%)]\tLoss: 1.496563\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8100/60000 (14%)]\tLoss: 1.531543\tAccuracy: 97.29%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8150/60000 (14%)]\tLoss: 1.482068\tAccuracy: 97.29%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8200/60000 (14%)]\tLoss: 1.461165\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8250/60000 (14%)]\tLoss: 1.486585\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8300/60000 (14%)]\tLoss: 1.479277\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8350/60000 (14%)]\tLoss: 1.479668\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8400/60000 (14%)]\tLoss: 1.468397\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8450/60000 (14%)]\tLoss: 1.473169\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8500/60000 (14%)]\tLoss: 1.499504\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8550/60000 (14%)]\tLoss: 1.482901\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8600/60000 (14%)]\tLoss: 1.502012\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8650/60000 (14%)]\tLoss: 1.461178\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8700/60000 (14%)]\tLoss: 1.504461\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8750/60000 (15%)]\tLoss: 1.461208\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8800/60000 (15%)]\tLoss: 1.500620\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8850/60000 (15%)]\tLoss: 1.496976\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8900/60000 (15%)]\tLoss: 1.487384\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [8950/60000 (15%)]\tLoss: 1.497778\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9000/60000 (15%)]\tLoss: 1.495952\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9050/60000 (15%)]\tLoss: 1.493025\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9100/60000 (15%)]\tLoss: 1.482324\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9150/60000 (15%)]\tLoss: 1.498463\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9200/60000 (15%)]\tLoss: 1.461501\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9250/60000 (15%)]\tLoss: 1.465339\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9300/60000 (16%)]\tLoss: 1.461246\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9350/60000 (16%)]\tLoss: 1.490952\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9400/60000 (16%)]\tLoss: 1.477613\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9450/60000 (16%)]\tLoss: 1.501668\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9500/60000 (16%)]\tLoss: 1.504027\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9550/60000 (16%)]\tLoss: 1.481146\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9600/60000 (16%)]\tLoss: 1.474415\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9650/60000 (16%)]\tLoss: 1.463504\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9700/60000 (16%)]\tLoss: 1.531595\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9750/60000 (16%)]\tLoss: 1.501240\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9800/60000 (16%)]\tLoss: 1.461248\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9850/60000 (16%)]\tLoss: 1.480123\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9900/60000 (16%)]\tLoss: 1.485851\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [9950/60000 (17%)]\tLoss: 1.472265\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10000/60000 (17%)]\tLoss: 1.519208\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10050/60000 (17%)]\tLoss: 1.461335\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10100/60000 (17%)]\tLoss: 1.511178\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10150/60000 (17%)]\tLoss: 1.478404\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10200/60000 (17%)]\tLoss: 1.466559\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10250/60000 (17%)]\tLoss: 1.541041\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10300/60000 (17%)]\tLoss: 1.464845\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10350/60000 (17%)]\tLoss: 1.461317\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10400/60000 (17%)]\tLoss: 1.481153\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10450/60000 (17%)]\tLoss: 1.461319\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10500/60000 (18%)]\tLoss: 1.501065\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10550/60000 (18%)]\tLoss: 1.503759\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10600/60000 (18%)]\tLoss: 1.482969\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10650/60000 (18%)]\tLoss: 1.461180\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10700/60000 (18%)]\tLoss: 1.481155\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10750/60000 (18%)]\tLoss: 1.480242\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10800/60000 (18%)]\tLoss: 1.500839\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10850/60000 (18%)]\tLoss: 1.484758\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10900/60000 (18%)]\tLoss: 1.488557\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [10950/60000 (18%)]\tLoss: 1.522094\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11000/60000 (18%)]\tLoss: 1.461184\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11050/60000 (18%)]\tLoss: 1.505407\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11100/60000 (18%)]\tLoss: 1.481209\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11150/60000 (19%)]\tLoss: 1.493340\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11200/60000 (19%)]\tLoss: 1.462023\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11250/60000 (19%)]\tLoss: 1.481518\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11300/60000 (19%)]\tLoss: 1.501079\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11350/60000 (19%)]\tLoss: 1.471542\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11400/60000 (19%)]\tLoss: 1.481145\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11450/60000 (19%)]\tLoss: 1.481186\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11500/60000 (19%)]\tLoss: 1.495564\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11550/60000 (19%)]\tLoss: 1.461307\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11600/60000 (19%)]\tLoss: 1.481574\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11650/60000 (19%)]\tLoss: 1.461164\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11700/60000 (20%)]\tLoss: 1.481067\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11750/60000 (20%)]\tLoss: 1.481670\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11800/60000 (20%)]\tLoss: 1.481154\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11850/60000 (20%)]\tLoss: 1.481139\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11900/60000 (20%)]\tLoss: 1.461420\tAccuracy: 97.41%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [11950/60000 (20%)]\tLoss: 1.478387\tAccuracy: 97.41%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12000/60000 (20%)]\tLoss: 1.476434\tAccuracy: 97.41%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12050/60000 (20%)]\tLoss: 1.516010\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12100/60000 (20%)]\tLoss: 1.521214\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12150/60000 (20%)]\tLoss: 1.484092\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12200/60000 (20%)]\tLoss: 1.481799\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12250/60000 (20%)]\tLoss: 1.461487\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12300/60000 (20%)]\tLoss: 1.518927\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12350/60000 (21%)]\tLoss: 1.478001\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12400/60000 (21%)]\tLoss: 1.495557\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12450/60000 (21%)]\tLoss: 1.496825\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12500/60000 (21%)]\tLoss: 1.517893\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12550/60000 (21%)]\tLoss: 1.479348\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12600/60000 (21%)]\tLoss: 1.481281\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12650/60000 (21%)]\tLoss: 1.497637\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12700/60000 (21%)]\tLoss: 1.482089\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12750/60000 (21%)]\tLoss: 1.504136\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12800/60000 (21%)]\tLoss: 1.500608\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12850/60000 (21%)]\tLoss: 1.490587\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12900/60000 (22%)]\tLoss: 1.523705\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [12950/60000 (22%)]\tLoss: 1.521386\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13000/60000 (22%)]\tLoss: 1.491331\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13050/60000 (22%)]\tLoss: 1.482785\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13100/60000 (22%)]\tLoss: 1.464075\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13150/60000 (22%)]\tLoss: 1.509832\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13200/60000 (22%)]\tLoss: 1.499377\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13250/60000 (22%)]\tLoss: 1.502569\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13300/60000 (22%)]\tLoss: 1.461900\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13350/60000 (22%)]\tLoss: 1.469924\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13400/60000 (22%)]\tLoss: 1.487209\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13450/60000 (22%)]\tLoss: 1.467814\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13500/60000 (22%)]\tLoss: 1.461154\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13550/60000 (23%)]\tLoss: 1.513123\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13600/60000 (23%)]\tLoss: 1.501282\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13650/60000 (23%)]\tLoss: 1.477862\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13700/60000 (23%)]\tLoss: 1.461151\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13750/60000 (23%)]\tLoss: 1.481965\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13800/60000 (23%)]\tLoss: 1.481177\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13850/60000 (23%)]\tLoss: 1.468701\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13900/60000 (23%)]\tLoss: 1.484678\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [13950/60000 (23%)]\tLoss: 1.485910\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14000/60000 (23%)]\tLoss: 1.482313\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14050/60000 (23%)]\tLoss: 1.510952\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14100/60000 (24%)]\tLoss: 1.461527\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14150/60000 (24%)]\tLoss: 1.494409\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14200/60000 (24%)]\tLoss: 1.501193\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14250/60000 (24%)]\tLoss: 1.479780\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14300/60000 (24%)]\tLoss: 1.481802\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14350/60000 (24%)]\tLoss: 1.507025\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14400/60000 (24%)]\tLoss: 1.497494\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14450/60000 (24%)]\tLoss: 1.464921\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14500/60000 (24%)]\tLoss: 1.520090\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14550/60000 (24%)]\tLoss: 1.481494\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14600/60000 (24%)]\tLoss: 1.539331\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14650/60000 (24%)]\tLoss: 1.501357\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14700/60000 (24%)]\tLoss: 1.518593\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14750/60000 (25%)]\tLoss: 1.505247\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14800/60000 (25%)]\tLoss: 1.478168\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14850/60000 (25%)]\tLoss: 1.492611\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14900/60000 (25%)]\tLoss: 1.490160\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [14950/60000 (25%)]\tLoss: 1.548302\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15000/60000 (25%)]\tLoss: 1.476740\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15050/60000 (25%)]\tLoss: 1.479872\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15100/60000 (25%)]\tLoss: 1.481158\tAccuracy: 97.31%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15150/60000 (25%)]\tLoss: 1.480906\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15200/60000 (25%)]\tLoss: 1.461178\tAccuracy: 97.32%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15250/60000 (25%)]\tLoss: 1.461155\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15300/60000 (26%)]\tLoss: 1.481165\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15350/60000 (26%)]\tLoss: 1.463473\tAccuracy: 97.34%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15400/60000 (26%)]\tLoss: 1.461247\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15450/60000 (26%)]\tLoss: 1.473705\tAccuracy: 97.35%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15500/60000 (26%)]\tLoss: 1.482293\tAccuracy: 97.36%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15550/60000 (26%)]\tLoss: 1.461335\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15600/60000 (26%)]\tLoss: 1.477028\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15650/60000 (26%)]\tLoss: 1.478597\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15700/60000 (26%)]\tLoss: 1.462056\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15750/60000 (26%)]\tLoss: 1.502479\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15800/60000 (26%)]\tLoss: 1.462004\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15850/60000 (26%)]\tLoss: 1.477973\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15900/60000 (26%)]\tLoss: 1.498307\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [15950/60000 (27%)]\tLoss: 1.500733\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16000/60000 (27%)]\tLoss: 1.474968\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16050/60000 (27%)]\tLoss: 1.481820\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16100/60000 (27%)]\tLoss: 1.490986\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16150/60000 (27%)]\tLoss: 1.463662\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16200/60000 (27%)]\tLoss: 1.461764\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16250/60000 (27%)]\tLoss: 1.509422\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16300/60000 (27%)]\tLoss: 1.463510\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16350/60000 (27%)]\tLoss: 1.543549\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16400/60000 (27%)]\tLoss: 1.515775\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16450/60000 (27%)]\tLoss: 1.481859\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16500/60000 (28%)]\tLoss: 1.476812\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16550/60000 (28%)]\tLoss: 1.496234\tAccuracy: 97.37%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16600/60000 (28%)]\tLoss: 1.481990\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16650/60000 (28%)]\tLoss: 1.474287\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16700/60000 (28%)]\tLoss: 1.481560\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16750/60000 (28%)]\tLoss: 1.483620\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16800/60000 (28%)]\tLoss: 1.481776\tAccuracy: 97.38%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16850/60000 (28%)]\tLoss: 1.469957\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16900/60000 (28%)]\tLoss: 1.461502\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [16950/60000 (28%)]\tLoss: 1.481151\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17000/60000 (28%)]\tLoss: 1.501996\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17050/60000 (28%)]\tLoss: 1.498373\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17100/60000 (28%)]\tLoss: 1.481398\tAccuracy: 97.39%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17150/60000 (29%)]\tLoss: 1.482798\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17200/60000 (29%)]\tLoss: 1.469936\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17250/60000 (29%)]\tLoss: 1.498855\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17300/60000 (29%)]\tLoss: 1.481431\tAccuracy: 97.40%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17350/60000 (29%)]\tLoss: 1.461174\tAccuracy: 97.41%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17400/60000 (29%)]\tLoss: 1.461165\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17450/60000 (29%)]\tLoss: 1.483633\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17500/60000 (29%)]\tLoss: 1.483273\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17550/60000 (29%)]\tLoss: 1.507514\tAccuracy: 97.41%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17600/60000 (29%)]\tLoss: 1.461201\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17650/60000 (29%)]\tLoss: 1.461346\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17700/60000 (30%)]\tLoss: 1.490028\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17750/60000 (30%)]\tLoss: 1.500235\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17800/60000 (30%)]\tLoss: 1.465906\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17850/60000 (30%)]\tLoss: 1.464352\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17900/60000 (30%)]\tLoss: 1.540049\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [17950/60000 (30%)]\tLoss: 1.485935\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18000/60000 (30%)]\tLoss: 1.466862\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18050/60000 (30%)]\tLoss: 1.469935\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18100/60000 (30%)]\tLoss: 1.489707\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18150/60000 (30%)]\tLoss: 1.469884\tAccuracy: 97.45%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18200/60000 (30%)]\tLoss: 1.499394\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18250/60000 (30%)]\tLoss: 1.480624\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18300/60000 (30%)]\tLoss: 1.500461\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18350/60000 (31%)]\tLoss: 1.513633\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18400/60000 (31%)]\tLoss: 1.495289\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18450/60000 (31%)]\tLoss: 1.484646\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18500/60000 (31%)]\tLoss: 1.465778\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18550/60000 (31%)]\tLoss: 1.539852\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18600/60000 (31%)]\tLoss: 1.462271\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18650/60000 (31%)]\tLoss: 1.467571\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18700/60000 (31%)]\tLoss: 1.500948\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18750/60000 (31%)]\tLoss: 1.462273\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18800/60000 (31%)]\tLoss: 1.503518\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18850/60000 (31%)]\tLoss: 1.499849\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18900/60000 (32%)]\tLoss: 1.461429\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [18950/60000 (32%)]\tLoss: 1.501570\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19000/60000 (32%)]\tLoss: 1.481325\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19050/60000 (32%)]\tLoss: 1.482887\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19100/60000 (32%)]\tLoss: 1.539694\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19150/60000 (32%)]\tLoss: 1.483602\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19200/60000 (32%)]\tLoss: 1.481150\tAccuracy: 97.42%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19250/60000 (32%)]\tLoss: 1.461493\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19300/60000 (32%)]\tLoss: 1.497374\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19350/60000 (32%)]\tLoss: 1.461183\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19400/60000 (32%)]\tLoss: 1.477577\tAccuracy: 97.43%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19450/60000 (32%)]\tLoss: 1.463089\tAccuracy: 97.44%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19500/60000 (32%)]\tLoss: 1.469832\tAccuracy: 97.45%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19550/60000 (33%)]\tLoss: 1.472807\tAccuracy: 97.45%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19600/60000 (33%)]\tLoss: 1.464615\tAccuracy: 97.46%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19650/60000 (33%)]\tLoss: 1.497517\tAccuracy: 97.45%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19700/60000 (33%)]\tLoss: 1.461158\tAccuracy: 97.46%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19750/60000 (33%)]\tLoss: 1.482204\tAccuracy: 97.46%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19800/60000 (33%)]\tLoss: 1.461151\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19850/60000 (33%)]\tLoss: 1.491907\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19900/60000 (33%)]\tLoss: 1.481096\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [19950/60000 (33%)]\tLoss: 1.462960\tAccuracy: 97.47%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20000/60000 (33%)]\tLoss: 1.480748\tAccuracy: 97.48%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20050/60000 (33%)]\tLoss: 1.470387\tAccuracy: 97.48%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20100/60000 (34%)]\tLoss: 1.483811\tAccuracy: 97.48%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20150/60000 (34%)]\tLoss: 1.467739\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20200/60000 (34%)]\tLoss: 1.500537\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20250/60000 (34%)]\tLoss: 1.481114\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20300/60000 (34%)]\tLoss: 1.475006\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20350/60000 (34%)]\tLoss: 1.481059\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20400/60000 (34%)]\tLoss: 1.503802\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20450/60000 (34%)]\tLoss: 1.461161\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20500/60000 (34%)]\tLoss: 1.475973\tAccuracy: 97.49%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20550/60000 (34%)]\tLoss: 1.476889\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20600/60000 (34%)]\tLoss: 1.481225\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20650/60000 (34%)]\tLoss: 1.486082\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20700/60000 (34%)]\tLoss: 1.466419\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20750/60000 (35%)]\tLoss: 1.465890\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20800/60000 (35%)]\tLoss: 1.461151\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20850/60000 (35%)]\tLoss: 1.537831\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20900/60000 (35%)]\tLoss: 1.482105\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [20950/60000 (35%)]\tLoss: 1.461152\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21000/60000 (35%)]\tLoss: 1.496755\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21050/60000 (35%)]\tLoss: 1.470655\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21100/60000 (35%)]\tLoss: 1.502055\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21150/60000 (35%)]\tLoss: 1.467976\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21200/60000 (35%)]\tLoss: 1.461200\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21250/60000 (35%)]\tLoss: 1.461168\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21300/60000 (36%)]\tLoss: 1.461200\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21350/60000 (36%)]\tLoss: 1.481089\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21400/60000 (36%)]\tLoss: 1.488825\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21450/60000 (36%)]\tLoss: 1.462409\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21500/60000 (36%)]\tLoss: 1.493546\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21550/60000 (36%)]\tLoss: 1.481027\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21600/60000 (36%)]\tLoss: 1.503496\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21650/60000 (36%)]\tLoss: 1.481999\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21700/60000 (36%)]\tLoss: 1.467791\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21750/60000 (36%)]\tLoss: 1.494292\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21800/60000 (36%)]\tLoss: 1.470674\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21850/60000 (36%)]\tLoss: 1.501181\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21900/60000 (36%)]\tLoss: 1.470678\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [21950/60000 (37%)]\tLoss: 1.482663\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22000/60000 (37%)]\tLoss: 1.480939\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22050/60000 (37%)]\tLoss: 1.481155\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22100/60000 (37%)]\tLoss: 1.485438\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22150/60000 (37%)]\tLoss: 1.483759\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22200/60000 (37%)]\tLoss: 1.462272\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22250/60000 (37%)]\tLoss: 1.479378\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22300/60000 (37%)]\tLoss: 1.502408\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22350/60000 (37%)]\tLoss: 1.499031\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22400/60000 (37%)]\tLoss: 1.507693\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22450/60000 (37%)]\tLoss: 1.535274\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22500/60000 (38%)]\tLoss: 1.520827\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22550/60000 (38%)]\tLoss: 1.488478\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22600/60000 (38%)]\tLoss: 1.501715\tAccuracy: 97.51%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22650/60000 (38%)]\tLoss: 1.466114\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22700/60000 (38%)]\tLoss: 1.471148\tAccuracy: 97.52%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22750/60000 (38%)]\tLoss: 1.462266\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22800/60000 (38%)]\tLoss: 1.464242\tAccuracy: 97.53%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22850/60000 (38%)]\tLoss: 1.466537\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22900/60000 (38%)]\tLoss: 1.478431\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [22950/60000 (38%)]\tLoss: 1.468287\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23000/60000 (38%)]\tLoss: 1.461240\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23050/60000 (38%)]\tLoss: 1.491896\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23100/60000 (38%)]\tLoss: 1.483163\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23150/60000 (39%)]\tLoss: 1.467184\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23200/60000 (39%)]\tLoss: 1.464230\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23250/60000 (39%)]\tLoss: 1.482507\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23300/60000 (39%)]\tLoss: 1.472771\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23350/60000 (39%)]\tLoss: 1.502638\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23400/60000 (39%)]\tLoss: 1.488935\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23450/60000 (39%)]\tLoss: 1.484867\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23500/60000 (39%)]\tLoss: 1.500801\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23550/60000 (39%)]\tLoss: 1.522217\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23600/60000 (39%)]\tLoss: 1.463592\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23650/60000 (39%)]\tLoss: 1.481036\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23700/60000 (40%)]\tLoss: 1.520741\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23750/60000 (40%)]\tLoss: 1.499418\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23800/60000 (40%)]\tLoss: 1.496615\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23850/60000 (40%)]\tLoss: 1.489493\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23900/60000 (40%)]\tLoss: 1.486772\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [23950/60000 (40%)]\tLoss: 1.481155\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24000/60000 (40%)]\tLoss: 1.476607\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24050/60000 (40%)]\tLoss: 1.497832\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24100/60000 (40%)]\tLoss: 1.494869\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24150/60000 (40%)]\tLoss: 1.488582\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24200/60000 (40%)]\tLoss: 1.461450\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24250/60000 (40%)]\tLoss: 1.479861\tAccuracy: 97.54%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24300/60000 (40%)]\tLoss: 1.461186\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24350/60000 (41%)]\tLoss: 1.481447\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24400/60000 (41%)]\tLoss: 1.501191\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24450/60000 (41%)]\tLoss: 1.469604\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24500/60000 (41%)]\tLoss: 1.464462\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24550/60000 (41%)]\tLoss: 1.477662\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24600/60000 (41%)]\tLoss: 1.461315\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24650/60000 (41%)]\tLoss: 1.461207\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24700/60000 (41%)]\tLoss: 1.485465\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24750/60000 (41%)]\tLoss: 1.481167\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24800/60000 (41%)]\tLoss: 1.500655\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24850/60000 (41%)]\tLoss: 1.472223\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24900/60000 (42%)]\tLoss: 1.480320\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [24950/60000 (42%)]\tLoss: 1.479283\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25000/60000 (42%)]\tLoss: 1.461162\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25050/60000 (42%)]\tLoss: 1.463673\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25100/60000 (42%)]\tLoss: 1.486621\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25150/60000 (42%)]\tLoss: 1.481490\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25200/60000 (42%)]\tLoss: 1.461168\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25250/60000 (42%)]\tLoss: 1.461163\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25300/60000 (42%)]\tLoss: 1.461152\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25350/60000 (42%)]\tLoss: 1.481571\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25400/60000 (42%)]\tLoss: 1.492084\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25450/60000 (42%)]\tLoss: 1.482111\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25500/60000 (42%)]\tLoss: 1.481562\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25550/60000 (43%)]\tLoss: 1.498102\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25600/60000 (43%)]\tLoss: 1.505408\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25650/60000 (43%)]\tLoss: 1.467761\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25700/60000 (43%)]\tLoss: 1.507405\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25750/60000 (43%)]\tLoss: 1.496540\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25800/60000 (43%)]\tLoss: 1.461352\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25850/60000 (43%)]\tLoss: 1.461347\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25900/60000 (43%)]\tLoss: 1.482274\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [25950/60000 (43%)]\tLoss: 1.472662\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26000/60000 (43%)]\tLoss: 1.481294\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26050/60000 (43%)]\tLoss: 1.481043\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26100/60000 (44%)]\tLoss: 1.513008\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26150/60000 (44%)]\tLoss: 1.465102\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26200/60000 (44%)]\tLoss: 1.480909\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26250/60000 (44%)]\tLoss: 1.481159\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26300/60000 (44%)]\tLoss: 1.476302\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26350/60000 (44%)]\tLoss: 1.469543\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26400/60000 (44%)]\tLoss: 1.476831\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26450/60000 (44%)]\tLoss: 1.481481\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26500/60000 (44%)]\tLoss: 1.483235\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26550/60000 (44%)]\tLoss: 1.518140\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26600/60000 (44%)]\tLoss: 1.489022\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26650/60000 (44%)]\tLoss: 1.480810\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26700/60000 (44%)]\tLoss: 1.462708\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26750/60000 (45%)]\tLoss: 1.483750\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26800/60000 (45%)]\tLoss: 1.473926\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26850/60000 (45%)]\tLoss: 1.473762\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26900/60000 (45%)]\tLoss: 1.481196\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [26950/60000 (45%)]\tLoss: 1.481824\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27000/60000 (45%)]\tLoss: 1.468556\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27050/60000 (45%)]\tLoss: 1.473098\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27100/60000 (45%)]\tLoss: 1.462219\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27150/60000 (45%)]\tLoss: 1.482383\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27200/60000 (45%)]\tLoss: 1.481030\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27250/60000 (45%)]\tLoss: 1.478610\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27300/60000 (46%)]\tLoss: 1.481151\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27350/60000 (46%)]\tLoss: 1.461242\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27400/60000 (46%)]\tLoss: 1.500828\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27450/60000 (46%)]\tLoss: 1.499922\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27500/60000 (46%)]\tLoss: 1.498212\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27550/60000 (46%)]\tLoss: 1.470127\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27600/60000 (46%)]\tLoss: 1.461297\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27650/60000 (46%)]\tLoss: 1.502566\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27700/60000 (46%)]\tLoss: 1.501277\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27750/60000 (46%)]\tLoss: 1.465366\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27800/60000 (46%)]\tLoss: 1.470688\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27850/60000 (46%)]\tLoss: 1.540991\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27900/60000 (46%)]\tLoss: 1.468968\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [27950/60000 (47%)]\tLoss: 1.480017\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28000/60000 (47%)]\tLoss: 1.482314\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28050/60000 (47%)]\tLoss: 1.462048\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28100/60000 (47%)]\tLoss: 1.514985\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28150/60000 (47%)]\tLoss: 1.486570\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28200/60000 (47%)]\tLoss: 1.535770\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28250/60000 (47%)]\tLoss: 1.481352\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28300/60000 (47%)]\tLoss: 1.478466\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28350/60000 (47%)]\tLoss: 1.490262\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28400/60000 (47%)]\tLoss: 1.461156\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28450/60000 (47%)]\tLoss: 1.461290\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28500/60000 (48%)]\tLoss: 1.467222\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28550/60000 (48%)]\tLoss: 1.520537\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28600/60000 (48%)]\tLoss: 1.479295\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28650/60000 (48%)]\tLoss: 1.504606\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28700/60000 (48%)]\tLoss: 1.484825\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28750/60000 (48%)]\tLoss: 1.481153\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28800/60000 (48%)]\tLoss: 1.502188\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28850/60000 (48%)]\tLoss: 1.474978\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28900/60000 (48%)]\tLoss: 1.500551\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [28950/60000 (48%)]\tLoss: 1.479944\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29000/60000 (48%)]\tLoss: 1.476750\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29050/60000 (48%)]\tLoss: 1.481180\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29100/60000 (48%)]\tLoss: 1.495695\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29150/60000 (49%)]\tLoss: 1.503315\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29200/60000 (49%)]\tLoss: 1.527593\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29250/60000 (49%)]\tLoss: 1.494256\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29300/60000 (49%)]\tLoss: 1.511932\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29350/60000 (49%)]\tLoss: 1.491613\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29400/60000 (49%)]\tLoss: 1.468414\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29450/60000 (49%)]\tLoss: 1.486370\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29500/60000 (49%)]\tLoss: 1.501078\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29550/60000 (49%)]\tLoss: 1.471377\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29600/60000 (49%)]\tLoss: 1.512537\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29650/60000 (49%)]\tLoss: 1.474129\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29700/60000 (50%)]\tLoss: 1.481130\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29750/60000 (50%)]\tLoss: 1.506688\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29800/60000 (50%)]\tLoss: 1.489516\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29850/60000 (50%)]\tLoss: 1.501510\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29900/60000 (50%)]\tLoss: 1.464537\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [29950/60000 (50%)]\tLoss: 1.461157\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30000/60000 (50%)]\tLoss: 1.501042\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30050/60000 (50%)]\tLoss: 1.533470\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30100/60000 (50%)]\tLoss: 1.505551\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30150/60000 (50%)]\tLoss: 1.500412\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30200/60000 (50%)]\tLoss: 1.497211\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30250/60000 (50%)]\tLoss: 1.502537\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30300/60000 (50%)]\tLoss: 1.522343\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30350/60000 (51%)]\tLoss: 1.471030\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30400/60000 (51%)]\tLoss: 1.496136\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30450/60000 (51%)]\tLoss: 1.534036\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30500/60000 (51%)]\tLoss: 1.477450\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30550/60000 (51%)]\tLoss: 1.502612\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30600/60000 (51%)]\tLoss: 1.481113\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30650/60000 (51%)]\tLoss: 1.461512\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30700/60000 (51%)]\tLoss: 1.464998\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30750/60000 (51%)]\tLoss: 1.517081\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30800/60000 (51%)]\tLoss: 1.465459\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30850/60000 (51%)]\tLoss: 1.480820\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30900/60000 (52%)]\tLoss: 1.481152\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [30950/60000 (52%)]\tLoss: 1.516174\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31000/60000 (52%)]\tLoss: 1.466874\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31050/60000 (52%)]\tLoss: 1.466093\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31100/60000 (52%)]\tLoss: 1.464506\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31150/60000 (52%)]\tLoss: 1.496777\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31200/60000 (52%)]\tLoss: 1.502033\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31250/60000 (52%)]\tLoss: 1.461156\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31300/60000 (52%)]\tLoss: 1.498581\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31350/60000 (52%)]\tLoss: 1.490076\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31400/60000 (52%)]\tLoss: 1.524926\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31450/60000 (52%)]\tLoss: 1.462225\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31500/60000 (52%)]\tLoss: 1.469352\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31550/60000 (53%)]\tLoss: 1.480173\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31600/60000 (53%)]\tLoss: 1.461205\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31650/60000 (53%)]\tLoss: 1.509988\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31700/60000 (53%)]\tLoss: 1.504089\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31750/60000 (53%)]\tLoss: 1.476847\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31800/60000 (53%)]\tLoss: 1.521429\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31850/60000 (53%)]\tLoss: 1.481570\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31900/60000 (53%)]\tLoss: 1.468449\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [31950/60000 (53%)]\tLoss: 1.514009\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32000/60000 (53%)]\tLoss: 1.484921\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32050/60000 (53%)]\tLoss: 1.518625\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32100/60000 (54%)]\tLoss: 1.461152\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32150/60000 (54%)]\tLoss: 1.526447\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32200/60000 (54%)]\tLoss: 1.462231\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32250/60000 (54%)]\tLoss: 1.482392\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32300/60000 (54%)]\tLoss: 1.461609\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32350/60000 (54%)]\tLoss: 1.500913\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32400/60000 (54%)]\tLoss: 1.461510\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32450/60000 (54%)]\tLoss: 1.481138\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32500/60000 (54%)]\tLoss: 1.477284\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32550/60000 (54%)]\tLoss: 1.461184\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32600/60000 (54%)]\tLoss: 1.514788\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32650/60000 (54%)]\tLoss: 1.473746\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32700/60000 (54%)]\tLoss: 1.501939\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32750/60000 (55%)]\tLoss: 1.481126\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32800/60000 (55%)]\tLoss: 1.480736\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32850/60000 (55%)]\tLoss: 1.481156\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32900/60000 (55%)]\tLoss: 1.461996\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [32950/60000 (55%)]\tLoss: 1.490193\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33000/60000 (55%)]\tLoss: 1.501342\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33050/60000 (55%)]\tLoss: 1.486383\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33100/60000 (55%)]\tLoss: 1.500471\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33150/60000 (55%)]\tLoss: 1.500866\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33200/60000 (55%)]\tLoss: 1.466822\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33250/60000 (55%)]\tLoss: 1.461155\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33300/60000 (56%)]\tLoss: 1.461586\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33350/60000 (56%)]\tLoss: 1.461644\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33400/60000 (56%)]\tLoss: 1.461152\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33450/60000 (56%)]\tLoss: 1.463687\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33500/60000 (56%)]\tLoss: 1.482578\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33550/60000 (56%)]\tLoss: 1.481785\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33600/60000 (56%)]\tLoss: 1.461652\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33650/60000 (56%)]\tLoss: 1.465612\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33700/60000 (56%)]\tLoss: 1.481180\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33750/60000 (56%)]\tLoss: 1.501015\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33800/60000 (56%)]\tLoss: 1.496707\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33850/60000 (56%)]\tLoss: 1.461161\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33900/60000 (56%)]\tLoss: 1.462713\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [33950/60000 (57%)]\tLoss: 1.461425\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34000/60000 (57%)]\tLoss: 1.481350\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34050/60000 (57%)]\tLoss: 1.468813\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34100/60000 (57%)]\tLoss: 1.480993\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34150/60000 (57%)]\tLoss: 1.463343\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34200/60000 (57%)]\tLoss: 1.461159\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34250/60000 (57%)]\tLoss: 1.467615\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34300/60000 (57%)]\tLoss: 1.484473\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34350/60000 (57%)]\tLoss: 1.461175\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34400/60000 (57%)]\tLoss: 1.481026\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34450/60000 (57%)]\tLoss: 1.476154\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34500/60000 (58%)]\tLoss: 1.487560\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34550/60000 (58%)]\tLoss: 1.461372\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34600/60000 (58%)]\tLoss: 1.461208\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34650/60000 (58%)]\tLoss: 1.482354\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34700/60000 (58%)]\tLoss: 1.489545\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34750/60000 (58%)]\tLoss: 1.502902\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34800/60000 (58%)]\tLoss: 1.487770\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34850/60000 (58%)]\tLoss: 1.472899\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34900/60000 (58%)]\tLoss: 1.497057\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [34950/60000 (58%)]\tLoss: 1.461172\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35000/60000 (58%)]\tLoss: 1.465218\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35050/60000 (58%)]\tLoss: 1.501684\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35100/60000 (58%)]\tLoss: 1.467652\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35150/60000 (59%)]\tLoss: 1.481395\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35200/60000 (59%)]\tLoss: 1.485633\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35250/60000 (59%)]\tLoss: 1.480707\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35300/60000 (59%)]\tLoss: 1.468848\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35350/60000 (59%)]\tLoss: 1.485890\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35400/60000 (59%)]\tLoss: 1.461192\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35450/60000 (59%)]\tLoss: 1.468053\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35500/60000 (59%)]\tLoss: 1.504906\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35550/60000 (59%)]\tLoss: 1.461270\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35600/60000 (59%)]\tLoss: 1.537454\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35650/60000 (59%)]\tLoss: 1.494545\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35700/60000 (60%)]\tLoss: 1.463513\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35750/60000 (60%)]\tLoss: 1.503765\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35800/60000 (60%)]\tLoss: 1.461216\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35850/60000 (60%)]\tLoss: 1.481167\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35900/60000 (60%)]\tLoss: 1.461942\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [35950/60000 (60%)]\tLoss: 1.468991\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36000/60000 (60%)]\tLoss: 1.506213\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36050/60000 (60%)]\tLoss: 1.503043\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36100/60000 (60%)]\tLoss: 1.528304\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36150/60000 (60%)]\tLoss: 1.477438\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36200/60000 (60%)]\tLoss: 1.494051\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36250/60000 (60%)]\tLoss: 1.466212\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36300/60000 (60%)]\tLoss: 1.462141\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36350/60000 (61%)]\tLoss: 1.501156\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36400/60000 (61%)]\tLoss: 1.502701\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36450/60000 (61%)]\tLoss: 1.461155\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36500/60000 (61%)]\tLoss: 1.539297\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36550/60000 (61%)]\tLoss: 1.497865\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36600/60000 (61%)]\tLoss: 1.461333\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36650/60000 (61%)]\tLoss: 1.468519\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36700/60000 (61%)]\tLoss: 1.481302\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36750/60000 (61%)]\tLoss: 1.479756\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36800/60000 (61%)]\tLoss: 1.575374\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36850/60000 (61%)]\tLoss: 1.481829\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36900/60000 (62%)]\tLoss: 1.481231\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [36950/60000 (62%)]\tLoss: 1.462523\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37000/60000 (62%)]\tLoss: 1.463244\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37050/60000 (62%)]\tLoss: 1.464667\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37100/60000 (62%)]\tLoss: 1.500162\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37150/60000 (62%)]\tLoss: 1.480748\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37200/60000 (62%)]\tLoss: 1.461151\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37250/60000 (62%)]\tLoss: 1.464798\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37300/60000 (62%)]\tLoss: 1.481154\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37350/60000 (62%)]\tLoss: 1.461332\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37400/60000 (62%)]\tLoss: 1.489046\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37450/60000 (62%)]\tLoss: 1.489260\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37500/60000 (62%)]\tLoss: 1.501030\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37550/60000 (63%)]\tLoss: 1.463771\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37600/60000 (63%)]\tLoss: 1.510584\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37650/60000 (63%)]\tLoss: 1.461195\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37700/60000 (63%)]\tLoss: 1.461151\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37750/60000 (63%)]\tLoss: 1.465182\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37800/60000 (63%)]\tLoss: 1.484886\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37850/60000 (63%)]\tLoss: 1.496981\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37900/60000 (63%)]\tLoss: 1.464195\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [37950/60000 (63%)]\tLoss: 1.462715\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38000/60000 (63%)]\tLoss: 1.481152\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38050/60000 (63%)]\tLoss: 1.478146\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38100/60000 (64%)]\tLoss: 1.481142\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38150/60000 (64%)]\tLoss: 1.475765\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38200/60000 (64%)]\tLoss: 1.461194\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38250/60000 (64%)]\tLoss: 1.525275\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38300/60000 (64%)]\tLoss: 1.464957\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38350/60000 (64%)]\tLoss: 1.473378\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38400/60000 (64%)]\tLoss: 1.462605\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38450/60000 (64%)]\tLoss: 1.461460\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38500/60000 (64%)]\tLoss: 1.488388\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38550/60000 (64%)]\tLoss: 1.501480\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38600/60000 (64%)]\tLoss: 1.500509\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38650/60000 (64%)]\tLoss: 1.508459\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38700/60000 (64%)]\tLoss: 1.461435\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38750/60000 (65%)]\tLoss: 1.525749\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38800/60000 (65%)]\tLoss: 1.505381\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38850/60000 (65%)]\tLoss: 1.490603\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38900/60000 (65%)]\tLoss: 1.475546\tAccuracy: 97.66%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [38950/60000 (65%)]\tLoss: 1.550907\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39000/60000 (65%)]\tLoss: 1.461563\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39050/60000 (65%)]\tLoss: 1.505046\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39100/60000 (65%)]\tLoss: 1.513628\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39150/60000 (65%)]\tLoss: 1.501020\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39200/60000 (65%)]\tLoss: 1.496003\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39250/60000 (65%)]\tLoss: 1.491396\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39300/60000 (66%)]\tLoss: 1.481696\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39350/60000 (66%)]\tLoss: 1.468337\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39400/60000 (66%)]\tLoss: 1.461152\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39450/60000 (66%)]\tLoss: 1.502372\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39500/60000 (66%)]\tLoss: 1.507432\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39550/60000 (66%)]\tLoss: 1.536887\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39600/60000 (66%)]\tLoss: 1.501903\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39650/60000 (66%)]\tLoss: 1.496714\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39700/60000 (66%)]\tLoss: 1.498367\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39750/60000 (66%)]\tLoss: 1.495532\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39800/60000 (66%)]\tLoss: 1.481295\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39850/60000 (66%)]\tLoss: 1.463961\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39900/60000 (66%)]\tLoss: 1.484963\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [39950/60000 (67%)]\tLoss: 1.481166\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40000/60000 (67%)]\tLoss: 1.501113\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40050/60000 (67%)]\tLoss: 1.481266\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40100/60000 (67%)]\tLoss: 1.474836\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40150/60000 (67%)]\tLoss: 1.481329\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40200/60000 (67%)]\tLoss: 1.461151\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40250/60000 (67%)]\tLoss: 1.479141\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40300/60000 (67%)]\tLoss: 1.481139\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40350/60000 (67%)]\tLoss: 1.461168\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40400/60000 (67%)]\tLoss: 1.503571\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40450/60000 (67%)]\tLoss: 1.501999\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40500/60000 (68%)]\tLoss: 1.462087\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40550/60000 (68%)]\tLoss: 1.520121\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40600/60000 (68%)]\tLoss: 1.465015\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40650/60000 (68%)]\tLoss: 1.463704\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40700/60000 (68%)]\tLoss: 1.502417\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40750/60000 (68%)]\tLoss: 1.461151\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40800/60000 (68%)]\tLoss: 1.463146\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40850/60000 (68%)]\tLoss: 1.519200\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40900/60000 (68%)]\tLoss: 1.472796\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [40950/60000 (68%)]\tLoss: 1.461483\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41000/60000 (68%)]\tLoss: 1.481152\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41050/60000 (68%)]\tLoss: 1.479229\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41100/60000 (68%)]\tLoss: 1.536956\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41150/60000 (69%)]\tLoss: 1.500285\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41200/60000 (69%)]\tLoss: 1.500600\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41250/60000 (69%)]\tLoss: 1.481149\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41300/60000 (69%)]\tLoss: 1.501209\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41350/60000 (69%)]\tLoss: 1.481430\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41400/60000 (69%)]\tLoss: 1.479323\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41450/60000 (69%)]\tLoss: 1.466605\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41500/60000 (69%)]\tLoss: 1.498838\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41550/60000 (69%)]\tLoss: 1.500902\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41600/60000 (69%)]\tLoss: 1.462972\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41650/60000 (69%)]\tLoss: 1.475008\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41700/60000 (70%)]\tLoss: 1.461181\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41750/60000 (70%)]\tLoss: 1.461164\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41800/60000 (70%)]\tLoss: 1.488701\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41850/60000 (70%)]\tLoss: 1.520116\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41900/60000 (70%)]\tLoss: 1.500798\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [41950/60000 (70%)]\tLoss: 1.461168\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42000/60000 (70%)]\tLoss: 1.461532\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42050/60000 (70%)]\tLoss: 1.461151\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42100/60000 (70%)]\tLoss: 1.481145\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42150/60000 (70%)]\tLoss: 1.485685\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42200/60000 (70%)]\tLoss: 1.471286\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42250/60000 (70%)]\tLoss: 1.469442\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42300/60000 (70%)]\tLoss: 1.480981\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42350/60000 (71%)]\tLoss: 1.461151\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42400/60000 (71%)]\tLoss: 1.493768\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42450/60000 (71%)]\tLoss: 1.461151\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42500/60000 (71%)]\tLoss: 1.513596\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42550/60000 (71%)]\tLoss: 1.500992\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42600/60000 (71%)]\tLoss: 1.481151\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42650/60000 (71%)]\tLoss: 1.484789\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42700/60000 (71%)]\tLoss: 1.501278\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42750/60000 (71%)]\tLoss: 1.495480\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42800/60000 (71%)]\tLoss: 1.461188\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42850/60000 (71%)]\tLoss: 1.461173\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42900/60000 (72%)]\tLoss: 1.461159\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [42950/60000 (72%)]\tLoss: 1.471366\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43000/60000 (72%)]\tLoss: 1.464145\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43050/60000 (72%)]\tLoss: 1.474558\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43100/60000 (72%)]\tLoss: 1.461152\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43150/60000 (72%)]\tLoss: 1.500840\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43200/60000 (72%)]\tLoss: 1.484111\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43250/60000 (72%)]\tLoss: 1.493361\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43300/60000 (72%)]\tLoss: 1.477618\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43350/60000 (72%)]\tLoss: 1.500410\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43400/60000 (72%)]\tLoss: 1.481351\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43450/60000 (72%)]\tLoss: 1.507002\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43500/60000 (72%)]\tLoss: 1.474438\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43550/60000 (73%)]\tLoss: 1.511742\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43600/60000 (73%)]\tLoss: 1.480914\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43650/60000 (73%)]\tLoss: 1.481648\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43700/60000 (73%)]\tLoss: 1.470435\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43750/60000 (73%)]\tLoss: 1.465422\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43800/60000 (73%)]\tLoss: 1.509947\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43850/60000 (73%)]\tLoss: 1.520639\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43900/60000 (73%)]\tLoss: 1.464793\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [43950/60000 (73%)]\tLoss: 1.518493\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44000/60000 (73%)]\tLoss: 1.530704\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44050/60000 (73%)]\tLoss: 1.461346\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44100/60000 (74%)]\tLoss: 1.470051\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44150/60000 (74%)]\tLoss: 1.525190\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44200/60000 (74%)]\tLoss: 1.467420\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44250/60000 (74%)]\tLoss: 1.481655\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44300/60000 (74%)]\tLoss: 1.461212\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44350/60000 (74%)]\tLoss: 1.484124\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44400/60000 (74%)]\tLoss: 1.481064\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44450/60000 (74%)]\tLoss: 1.481151\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44500/60000 (74%)]\tLoss: 1.481154\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44550/60000 (74%)]\tLoss: 1.501153\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44600/60000 (74%)]\tLoss: 1.462671\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44650/60000 (74%)]\tLoss: 1.488599\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44700/60000 (74%)]\tLoss: 1.472609\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44750/60000 (75%)]\tLoss: 1.463772\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44800/60000 (75%)]\tLoss: 1.461157\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44850/60000 (75%)]\tLoss: 1.506799\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44900/60000 (75%)]\tLoss: 1.481059\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [44950/60000 (75%)]\tLoss: 1.461156\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45000/60000 (75%)]\tLoss: 1.461576\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45050/60000 (75%)]\tLoss: 1.464671\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45100/60000 (75%)]\tLoss: 1.493008\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45150/60000 (75%)]\tLoss: 1.468959\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45200/60000 (75%)]\tLoss: 1.481820\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45250/60000 (75%)]\tLoss: 1.575300\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45300/60000 (76%)]\tLoss: 1.532509\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45350/60000 (76%)]\tLoss: 1.530358\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45400/60000 (76%)]\tLoss: 1.501189\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45450/60000 (76%)]\tLoss: 1.483646\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45500/60000 (76%)]\tLoss: 1.481151\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45550/60000 (76%)]\tLoss: 1.501464\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45600/60000 (76%)]\tLoss: 1.501159\tAccuracy: 97.63%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45650/60000 (76%)]\tLoss: 1.520880\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45700/60000 (76%)]\tLoss: 1.502391\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45750/60000 (76%)]\tLoss: 1.512258\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45800/60000 (76%)]\tLoss: 1.469317\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45850/60000 (76%)]\tLoss: 1.507366\tAccuracy: 97.62%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45900/60000 (76%)]\tLoss: 1.531690\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [45950/60000 (77%)]\tLoss: 1.497050\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46000/60000 (77%)]\tLoss: 1.504386\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46050/60000 (77%)]\tLoss: 1.506643\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46100/60000 (77%)]\tLoss: 1.482772\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46150/60000 (77%)]\tLoss: 1.520917\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46200/60000 (77%)]\tLoss: 1.461155\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46250/60000 (77%)]\tLoss: 1.512012\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46300/60000 (77%)]\tLoss: 1.462296\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46350/60000 (77%)]\tLoss: 1.477740\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46400/60000 (77%)]\tLoss: 1.494977\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46450/60000 (77%)]\tLoss: 1.488662\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46500/60000 (78%)]\tLoss: 1.477438\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46550/60000 (78%)]\tLoss: 1.481152\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46600/60000 (78%)]\tLoss: 1.481215\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46650/60000 (78%)]\tLoss: 1.461946\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46700/60000 (78%)]\tLoss: 1.520963\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46750/60000 (78%)]\tLoss: 1.501632\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46800/60000 (78%)]\tLoss: 1.461523\tAccuracy: 97.61%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46850/60000 (78%)]\tLoss: 1.495746\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46900/60000 (78%)]\tLoss: 1.540229\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [46950/60000 (78%)]\tLoss: 1.478780\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47000/60000 (78%)]\tLoss: 1.508924\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47050/60000 (78%)]\tLoss: 1.485687\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47100/60000 (78%)]\tLoss: 1.481314\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47150/60000 (79%)]\tLoss: 1.506059\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47200/60000 (79%)]\tLoss: 1.481192\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47250/60000 (79%)]\tLoss: 1.461168\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47300/60000 (79%)]\tLoss: 1.539400\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47350/60000 (79%)]\tLoss: 1.461169\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47400/60000 (79%)]\tLoss: 1.494519\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47450/60000 (79%)]\tLoss: 1.500581\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47500/60000 (79%)]\tLoss: 1.496373\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47550/60000 (79%)]\tLoss: 1.478060\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47600/60000 (79%)]\tLoss: 1.520473\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47650/60000 (79%)]\tLoss: 1.470541\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47700/60000 (80%)]\tLoss: 1.491423\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47750/60000 (80%)]\tLoss: 1.489686\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47800/60000 (80%)]\tLoss: 1.496319\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47850/60000 (80%)]\tLoss: 1.474274\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47900/60000 (80%)]\tLoss: 1.465194\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [47950/60000 (80%)]\tLoss: 1.462519\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48000/60000 (80%)]\tLoss: 1.461870\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48050/60000 (80%)]\tLoss: 1.461192\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48100/60000 (80%)]\tLoss: 1.480864\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48150/60000 (80%)]\tLoss: 1.484438\tAccuracy: 97.60%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48200/60000 (80%)]\tLoss: 1.516822\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48250/60000 (80%)]\tLoss: 1.479725\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48300/60000 (80%)]\tLoss: 1.481984\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48350/60000 (81%)]\tLoss: 1.502772\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48400/60000 (81%)]\tLoss: 1.479073\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48450/60000 (81%)]\tLoss: 1.463954\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48500/60000 (81%)]\tLoss: 1.512635\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48550/60000 (81%)]\tLoss: 1.462378\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48600/60000 (81%)]\tLoss: 1.481281\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48650/60000 (81%)]\tLoss: 1.481228\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48700/60000 (81%)]\tLoss: 1.503568\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48750/60000 (81%)]\tLoss: 1.491176\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48800/60000 (81%)]\tLoss: 1.506441\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48850/60000 (81%)]\tLoss: 1.486530\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48900/60000 (82%)]\tLoss: 1.461156\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [48950/60000 (82%)]\tLoss: 1.544683\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49000/60000 (82%)]\tLoss: 1.483752\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49050/60000 (82%)]\tLoss: 1.521142\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49100/60000 (82%)]\tLoss: 1.501623\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49150/60000 (82%)]\tLoss: 1.462127\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49200/60000 (82%)]\tLoss: 1.481180\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49250/60000 (82%)]\tLoss: 1.483357\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49300/60000 (82%)]\tLoss: 1.483436\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49350/60000 (82%)]\tLoss: 1.483402\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49400/60000 (82%)]\tLoss: 1.461879\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49450/60000 (82%)]\tLoss: 1.475934\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49500/60000 (82%)]\tLoss: 1.491330\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49550/60000 (83%)]\tLoss: 1.470110\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49600/60000 (83%)]\tLoss: 1.461235\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49650/60000 (83%)]\tLoss: 1.494328\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49700/60000 (83%)]\tLoss: 1.514119\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49750/60000 (83%)]\tLoss: 1.481091\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49800/60000 (83%)]\tLoss: 1.461752\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49850/60000 (83%)]\tLoss: 1.461399\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49900/60000 (83%)]\tLoss: 1.503049\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [49950/60000 (83%)]\tLoss: 1.518725\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50000/60000 (83%)]\tLoss: 1.479262\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50050/60000 (83%)]\tLoss: 1.494741\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50100/60000 (84%)]\tLoss: 1.461162\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50150/60000 (84%)]\tLoss: 1.495925\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50200/60000 (84%)]\tLoss: 1.500502\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50250/60000 (84%)]\tLoss: 1.497109\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50300/60000 (84%)]\tLoss: 1.461522\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50350/60000 (84%)]\tLoss: 1.481151\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50400/60000 (84%)]\tLoss: 1.509828\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50450/60000 (84%)]\tLoss: 1.480426\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50500/60000 (84%)]\tLoss: 1.518366\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50550/60000 (84%)]\tLoss: 1.497746\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50600/60000 (84%)]\tLoss: 1.461467\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50650/60000 (84%)]\tLoss: 1.481205\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50700/60000 (84%)]\tLoss: 1.481078\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50750/60000 (85%)]\tLoss: 1.463057\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50800/60000 (85%)]\tLoss: 1.480503\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50850/60000 (85%)]\tLoss: 1.466703\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50900/60000 (85%)]\tLoss: 1.462978\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [50950/60000 (85%)]\tLoss: 1.485679\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51000/60000 (85%)]\tLoss: 1.494227\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51050/60000 (85%)]\tLoss: 1.500044\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51100/60000 (85%)]\tLoss: 1.501142\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51150/60000 (85%)]\tLoss: 1.519225\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51200/60000 (85%)]\tLoss: 1.481115\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51250/60000 (85%)]\tLoss: 1.481091\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51300/60000 (86%)]\tLoss: 1.472037\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51350/60000 (86%)]\tLoss: 1.501159\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51400/60000 (86%)]\tLoss: 1.461433\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51450/60000 (86%)]\tLoss: 1.494528\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51500/60000 (86%)]\tLoss: 1.498538\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51550/60000 (86%)]\tLoss: 1.490630\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51600/60000 (86%)]\tLoss: 1.461809\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51650/60000 (86%)]\tLoss: 1.497710\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51700/60000 (86%)]\tLoss: 1.540819\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51750/60000 (86%)]\tLoss: 1.479082\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51800/60000 (86%)]\tLoss: 1.474325\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51850/60000 (86%)]\tLoss: 1.461686\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51900/60000 (86%)]\tLoss: 1.479524\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [51950/60000 (87%)]\tLoss: 1.492629\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52000/60000 (87%)]\tLoss: 1.461152\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52050/60000 (87%)]\tLoss: 1.473571\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52100/60000 (87%)]\tLoss: 1.511060\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52150/60000 (87%)]\tLoss: 1.479146\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52200/60000 (87%)]\tLoss: 1.480993\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52250/60000 (87%)]\tLoss: 1.563503\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52300/60000 (87%)]\tLoss: 1.484244\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52350/60000 (87%)]\tLoss: 1.489971\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52400/60000 (87%)]\tLoss: 1.497207\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52450/60000 (87%)]\tLoss: 1.501274\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52500/60000 (88%)]\tLoss: 1.461697\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52550/60000 (88%)]\tLoss: 1.480689\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52600/60000 (88%)]\tLoss: 1.481677\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52650/60000 (88%)]\tLoss: 1.515738\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52700/60000 (88%)]\tLoss: 1.498372\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52750/60000 (88%)]\tLoss: 1.479334\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52800/60000 (88%)]\tLoss: 1.519551\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52850/60000 (88%)]\tLoss: 1.507371\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52900/60000 (88%)]\tLoss: 1.521863\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [52950/60000 (88%)]\tLoss: 1.523109\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53000/60000 (88%)]\tLoss: 1.461269\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53050/60000 (88%)]\tLoss: 1.485270\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53100/60000 (88%)]\tLoss: 1.501148\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53150/60000 (89%)]\tLoss: 1.509889\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53200/60000 (89%)]\tLoss: 1.474727\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53250/60000 (89%)]\tLoss: 1.481922\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53300/60000 (89%)]\tLoss: 1.481990\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53350/60000 (89%)]\tLoss: 1.481048\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53400/60000 (89%)]\tLoss: 1.527071\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53450/60000 (89%)]\tLoss: 1.502209\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53500/60000 (89%)]\tLoss: 1.489318\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53550/60000 (89%)]\tLoss: 1.494850\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53600/60000 (89%)]\tLoss: 1.481443\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53650/60000 (89%)]\tLoss: 1.467781\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53700/60000 (90%)]\tLoss: 1.473961\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53750/60000 (90%)]\tLoss: 1.461163\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53800/60000 (90%)]\tLoss: 1.492057\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53850/60000 (90%)]\tLoss: 1.491189\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53900/60000 (90%)]\tLoss: 1.465477\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [53950/60000 (90%)]\tLoss: 1.491611\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54000/60000 (90%)]\tLoss: 1.480526\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54050/60000 (90%)]\tLoss: 1.481248\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54100/60000 (90%)]\tLoss: 1.461222\tAccuracy: 97.56%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54150/60000 (90%)]\tLoss: 1.461158\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54200/60000 (90%)]\tLoss: 1.489996\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54250/60000 (90%)]\tLoss: 1.481158\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54300/60000 (90%)]\tLoss: 1.473564\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54350/60000 (91%)]\tLoss: 1.480204\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54400/60000 (91%)]\tLoss: 1.462306\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54450/60000 (91%)]\tLoss: 1.483044\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54500/60000 (91%)]\tLoss: 1.468211\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54550/60000 (91%)]\tLoss: 1.481090\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54600/60000 (91%)]\tLoss: 1.502980\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54650/60000 (91%)]\tLoss: 1.461195\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54700/60000 (91%)]\tLoss: 1.497446\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54750/60000 (91%)]\tLoss: 1.481380\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54800/60000 (91%)]\tLoss: 1.560418\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54850/60000 (91%)]\tLoss: 1.473297\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54900/60000 (92%)]\tLoss: 1.461184\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [54950/60000 (92%)]\tLoss: 1.481152\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55000/60000 (92%)]\tLoss: 1.495417\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55050/60000 (92%)]\tLoss: 1.479088\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55100/60000 (92%)]\tLoss: 1.480985\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55150/60000 (92%)]\tLoss: 1.481508\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55200/60000 (92%)]\tLoss: 1.475113\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55250/60000 (92%)]\tLoss: 1.500955\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55300/60000 (92%)]\tLoss: 1.461263\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55350/60000 (92%)]\tLoss: 1.501822\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55400/60000 (92%)]\tLoss: 1.501653\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55450/60000 (92%)]\tLoss: 1.495906\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55500/60000 (92%)]\tLoss: 1.468483\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55550/60000 (93%)]\tLoss: 1.461695\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55600/60000 (93%)]\tLoss: 1.481035\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55650/60000 (93%)]\tLoss: 1.499163\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55700/60000 (93%)]\tLoss: 1.517185\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55750/60000 (93%)]\tLoss: 1.462851\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55800/60000 (93%)]\tLoss: 1.462154\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55850/60000 (93%)]\tLoss: 1.476874\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55900/60000 (93%)]\tLoss: 1.481426\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [55950/60000 (93%)]\tLoss: 1.461178\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56000/60000 (93%)]\tLoss: 1.503633\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56050/60000 (93%)]\tLoss: 1.486108\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56100/60000 (94%)]\tLoss: 1.483875\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56150/60000 (94%)]\tLoss: 1.473679\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56200/60000 (94%)]\tLoss: 1.495076\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56250/60000 (94%)]\tLoss: 1.481152\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56300/60000 (94%)]\tLoss: 1.497134\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56350/60000 (94%)]\tLoss: 1.479640\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56400/60000 (94%)]\tLoss: 1.461190\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56450/60000 (94%)]\tLoss: 1.485439\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56500/60000 (94%)]\tLoss: 1.495394\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56550/60000 (94%)]\tLoss: 1.474937\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56600/60000 (94%)]\tLoss: 1.481149\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56650/60000 (94%)]\tLoss: 1.462614\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56700/60000 (94%)]\tLoss: 1.475104\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56750/60000 (95%)]\tLoss: 1.541394\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56800/60000 (95%)]\tLoss: 1.480878\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56850/60000 (95%)]\tLoss: 1.481177\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56900/60000 (95%)]\tLoss: 1.461920\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [56950/60000 (95%)]\tLoss: 1.480398\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57000/60000 (95%)]\tLoss: 1.501142\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57050/60000 (95%)]\tLoss: 1.466208\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57100/60000 (95%)]\tLoss: 1.483736\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57150/60000 (95%)]\tLoss: 1.481348\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57200/60000 (95%)]\tLoss: 1.520818\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57250/60000 (95%)]\tLoss: 1.501151\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57300/60000 (96%)]\tLoss: 1.495076\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57350/60000 (96%)]\tLoss: 1.463576\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57400/60000 (96%)]\tLoss: 1.500243\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57450/60000 (96%)]\tLoss: 1.526733\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57500/60000 (96%)]\tLoss: 1.461161\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57550/60000 (96%)]\tLoss: 1.467931\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57600/60000 (96%)]\tLoss: 1.522636\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57650/60000 (96%)]\tLoss: 1.461210\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57700/60000 (96%)]\tLoss: 1.463358\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57750/60000 (96%)]\tLoss: 1.489891\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57800/60000 (96%)]\tLoss: 1.501028\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57850/60000 (96%)]\tLoss: 1.461277\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57900/60000 (96%)]\tLoss: 1.504431\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [57950/60000 (97%)]\tLoss: 1.461151\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58000/60000 (97%)]\tLoss: 1.521864\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58050/60000 (97%)]\tLoss: 1.553581\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58100/60000 (97%)]\tLoss: 1.481131\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58150/60000 (97%)]\tLoss: 1.480685\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58200/60000 (97%)]\tLoss: 1.481311\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58250/60000 (97%)]\tLoss: 1.461158\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58300/60000 (97%)]\tLoss: 1.461344\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58350/60000 (97%)]\tLoss: 1.501428\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58400/60000 (97%)]\tLoss: 1.471438\tAccuracy: 97.57%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58450/60000 (97%)]\tLoss: 1.469047\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58500/60000 (98%)]\tLoss: 1.481147\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58550/60000 (98%)]\tLoss: 1.479772\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58600/60000 (98%)]\tLoss: 1.481634\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58650/60000 (98%)]\tLoss: 1.483654\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58700/60000 (98%)]\tLoss: 1.461182\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58750/60000 (98%)]\tLoss: 1.461180\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58800/60000 (98%)]\tLoss: 1.494248\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58850/60000 (98%)]\tLoss: 1.462104\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58900/60000 (98%)]\tLoss: 1.501285\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [58950/60000 (98%)]\tLoss: 1.461151\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59000/60000 (98%)]\tLoss: 1.499537\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59050/60000 (98%)]\tLoss: 1.476861\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59100/60000 (98%)]\tLoss: 1.481244\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59150/60000 (99%)]\tLoss: 1.466299\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59200/60000 (99%)]\tLoss: 1.482423\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59250/60000 (99%)]\tLoss: 1.462065\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59300/60000 (99%)]\tLoss: 1.461151\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59350/60000 (99%)]\tLoss: 1.501365\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59400/60000 (99%)]\tLoss: 1.476555\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59450/60000 (99%)]\tLoss: 1.480802\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59500/60000 (99%)]\tLoss: 1.480380\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59550/60000 (99%)]\tLoss: 1.499729\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59600/60000 (99%)]\tLoss: 1.461269\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59650/60000 (99%)]\tLoss: 1.499840\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59700/60000 (100%)]\tLoss: 1.500546\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59750/60000 (100%)]\tLoss: 1.467965\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59800/60000 (100%)]\tLoss: 1.461674\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59850/60000 (100%)]\tLoss: 1.461542\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59900/60000 (100%)]\tLoss: 1.481645\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 1 [59950/60000 (100%)]\tLoss: 1.522723\tAccuracy: 97.59%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [0/60000 (0%)]\tLoss: 1.500773\tAccuracy: 96.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50/60000 (0%)]\tLoss: 1.505116\tAccuracy: 96.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [100/60000 (0%)]\tLoss: 1.481819\tAccuracy: 96.67%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [150/60000 (0%)]\tLoss: 1.484626\tAccuracy: 97.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [200/60000 (0%)]\tLoss: 1.481252\tAccuracy: 97.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [250/60000 (0%)]\tLoss: 1.473941\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [300/60000 (0%)]\tLoss: 1.461151\tAccuracy: 97.71%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [350/60000 (1%)]\tLoss: 1.496789\tAccuracy: 97.50%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [400/60000 (1%)]\tLoss: 1.461530\tAccuracy: 97.78%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [450/60000 (1%)]\tLoss: 1.461153\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [500/60000 (1%)]\tLoss: 1.481153\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [550/60000 (1%)]\tLoss: 1.520605\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [600/60000 (1%)]\tLoss: 1.461403\tAccuracy: 97.85%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [650/60000 (1%)]\tLoss: 1.481182\tAccuracy: 97.86%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [700/60000 (1%)]\tLoss: 1.461326\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [750/60000 (1%)]\tLoss: 1.467212\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [800/60000 (1%)]\tLoss: 1.480454\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [850/60000 (1%)]\tLoss: 1.532544\tAccuracy: 97.78%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [900/60000 (2%)]\tLoss: 1.461781\tAccuracy: 97.89%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [950/60000 (2%)]\tLoss: 1.478134\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1000/60000 (2%)]\tLoss: 1.518987\tAccuracy: 97.71%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1050/60000 (2%)]\tLoss: 1.517791\tAccuracy: 97.55%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1100/60000 (2%)]\tLoss: 1.461249\tAccuracy: 97.65%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1150/60000 (2%)]\tLoss: 1.499264\tAccuracy: 97.58%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1200/60000 (2%)]\tLoss: 1.465266\tAccuracy: 97.68%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1250/60000 (2%)]\tLoss: 1.461325\tAccuracy: 97.77%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1300/60000 (2%)]\tLoss: 1.503374\tAccuracy: 97.70%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1350/60000 (2%)]\tLoss: 1.501152\tAccuracy: 97.64%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1400/60000 (2%)]\tLoss: 1.461542\tAccuracy: 97.72%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1450/60000 (2%)]\tLoss: 1.490762\tAccuracy: 97.67%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1500/60000 (2%)]\tLoss: 1.461371\tAccuracy: 97.74%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1550/60000 (3%)]\tLoss: 1.481619\tAccuracy: 97.75%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1600/60000 (3%)]\tLoss: 1.489789\tAccuracy: 97.76%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1650/60000 (3%)]\tLoss: 1.461158\tAccuracy: 97.82%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1700/60000 (3%)]\tLoss: 1.481152\tAccuracy: 97.83%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1750/60000 (3%)]\tLoss: 1.473576\tAccuracy: 97.83%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1800/60000 (3%)]\tLoss: 1.478221\tAccuracy: 97.84%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1850/60000 (3%)]\tLoss: 1.482099\tAccuracy: 97.84%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1900/60000 (3%)]\tLoss: 1.480872\tAccuracy: 97.85%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [1950/60000 (3%)]\tLoss: 1.461158\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2000/60000 (3%)]\tLoss: 1.481541\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2050/60000 (3%)]\tLoss: 1.481104\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2100/60000 (4%)]\tLoss: 1.494409\tAccuracy: 97.86%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2150/60000 (4%)]\tLoss: 1.480768\tAccuracy: 97.86%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2200/60000 (4%)]\tLoss: 1.494499\tAccuracy: 97.82%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2250/60000 (4%)]\tLoss: 1.473393\tAccuracy: 97.83%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2300/60000 (4%)]\tLoss: 1.467393\tAccuracy: 97.87%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2350/60000 (4%)]\tLoss: 1.480395\tAccuracy: 97.88%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2400/60000 (4%)]\tLoss: 1.461157\tAccuracy: 97.92%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2450/60000 (4%)]\tLoss: 1.470184\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2500/60000 (4%)]\tLoss: 1.461866\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2550/60000 (4%)]\tLoss: 1.461154\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2600/60000 (4%)]\tLoss: 1.517103\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2650/60000 (4%)]\tLoss: 1.485811\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2700/60000 (4%)]\tLoss: 1.481684\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2750/60000 (5%)]\tLoss: 1.482185\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2800/60000 (5%)]\tLoss: 1.469587\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2850/60000 (5%)]\tLoss: 1.462325\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2900/60000 (5%)]\tLoss: 1.517039\tAccuracy: 97.97%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [2950/60000 (5%)]\tLoss: 1.489294\tAccuracy: 97.97%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3000/60000 (5%)]\tLoss: 1.480993\tAccuracy: 97.97%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3050/60000 (5%)]\tLoss: 1.487772\tAccuracy: 97.97%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3100/60000 (5%)]\tLoss: 1.461669\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3150/60000 (5%)]\tLoss: 1.462781\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3200/60000 (5%)]\tLoss: 1.478448\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3250/60000 (5%)]\tLoss: 1.519580\tAccuracy: 97.97%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3300/60000 (6%)]\tLoss: 1.461156\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3350/60000 (6%)]\tLoss: 1.467414\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3400/60000 (6%)]\tLoss: 1.481755\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3450/60000 (6%)]\tLoss: 1.475093\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3500/60000 (6%)]\tLoss: 1.461251\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3550/60000 (6%)]\tLoss: 1.461153\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3600/60000 (6%)]\tLoss: 1.462682\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3650/60000 (6%)]\tLoss: 1.524210\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3700/60000 (6%)]\tLoss: 1.492886\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3750/60000 (6%)]\tLoss: 1.479685\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3800/60000 (6%)]\tLoss: 1.461304\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3850/60000 (6%)]\tLoss: 1.461230\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3900/60000 (6%)]\tLoss: 1.499776\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [3950/60000 (7%)]\tLoss: 1.498758\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4000/60000 (7%)]\tLoss: 1.461175\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4050/60000 (7%)]\tLoss: 1.461151\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4100/60000 (7%)]\tLoss: 1.461151\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4150/60000 (7%)]\tLoss: 1.489455\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4200/60000 (7%)]\tLoss: 1.463671\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4250/60000 (7%)]\tLoss: 1.461164\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4300/60000 (7%)]\tLoss: 1.461151\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4350/60000 (7%)]\tLoss: 1.480855\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4400/60000 (7%)]\tLoss: 1.480921\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4450/60000 (7%)]\tLoss: 1.461153\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4500/60000 (8%)]\tLoss: 1.470532\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4550/60000 (8%)]\tLoss: 1.461167\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4600/60000 (8%)]\tLoss: 1.481120\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4650/60000 (8%)]\tLoss: 1.480187\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4700/60000 (8%)]\tLoss: 1.491377\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4750/60000 (8%)]\tLoss: 1.462238\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4800/60000 (8%)]\tLoss: 1.486302\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4850/60000 (8%)]\tLoss: 1.462463\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4900/60000 (8%)]\tLoss: 1.462647\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [4950/60000 (8%)]\tLoss: 1.482680\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5000/60000 (8%)]\tLoss: 1.496139\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5050/60000 (8%)]\tLoss: 1.461330\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5100/60000 (8%)]\tLoss: 1.461727\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5150/60000 (9%)]\tLoss: 1.461151\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5200/60000 (9%)]\tLoss: 1.461290\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5250/60000 (9%)]\tLoss: 1.471683\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5300/60000 (9%)]\tLoss: 1.461865\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5350/60000 (9%)]\tLoss: 1.464027\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5400/60000 (9%)]\tLoss: 1.519776\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5450/60000 (9%)]\tLoss: 1.501137\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5500/60000 (9%)]\tLoss: 1.462447\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5550/60000 (9%)]\tLoss: 1.494582\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5600/60000 (9%)]\tLoss: 1.489732\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5650/60000 (9%)]\tLoss: 1.461240\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5700/60000 (10%)]\tLoss: 1.479082\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5750/60000 (10%)]\tLoss: 1.461415\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5800/60000 (10%)]\tLoss: 1.482618\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5850/60000 (10%)]\tLoss: 1.481061\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5900/60000 (10%)]\tLoss: 1.475584\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [5950/60000 (10%)]\tLoss: 1.461151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6000/60000 (10%)]\tLoss: 1.481146\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6050/60000 (10%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6100/60000 (10%)]\tLoss: 1.558615\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6150/60000 (10%)]\tLoss: 1.482813\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6200/60000 (10%)]\tLoss: 1.520590\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6250/60000 (10%)]\tLoss: 1.466021\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6300/60000 (10%)]\tLoss: 1.480246\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6350/60000 (11%)]\tLoss: 1.467415\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6400/60000 (11%)]\tLoss: 1.481228\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6450/60000 (11%)]\tLoss: 1.481063\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6500/60000 (11%)]\tLoss: 1.481825\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6550/60000 (11%)]\tLoss: 1.461169\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6600/60000 (11%)]\tLoss: 1.500116\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6650/60000 (11%)]\tLoss: 1.461169\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6700/60000 (11%)]\tLoss: 1.506407\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6750/60000 (11%)]\tLoss: 1.461178\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6800/60000 (11%)]\tLoss: 1.496413\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6850/60000 (11%)]\tLoss: 1.481188\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6900/60000 (12%)]\tLoss: 1.501117\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [6950/60000 (12%)]\tLoss: 1.481155\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7000/60000 (12%)]\tLoss: 1.461151\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7050/60000 (12%)]\tLoss: 1.483785\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7100/60000 (12%)]\tLoss: 1.461795\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7150/60000 (12%)]\tLoss: 1.481502\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7200/60000 (12%)]\tLoss: 1.481540\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7250/60000 (12%)]\tLoss: 1.481149\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7300/60000 (12%)]\tLoss: 1.500560\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7350/60000 (12%)]\tLoss: 1.461383\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7400/60000 (12%)]\tLoss: 1.465627\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7450/60000 (12%)]\tLoss: 1.461151\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7500/60000 (12%)]\tLoss: 1.486174\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7550/60000 (13%)]\tLoss: 1.485091\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7600/60000 (13%)]\tLoss: 1.480873\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7650/60000 (13%)]\tLoss: 1.491258\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7700/60000 (13%)]\tLoss: 1.461152\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7750/60000 (13%)]\tLoss: 1.489630\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7800/60000 (13%)]\tLoss: 1.461171\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7850/60000 (13%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7900/60000 (13%)]\tLoss: 1.461152\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [7950/60000 (13%)]\tLoss: 1.474877\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8000/60000 (13%)]\tLoss: 1.481154\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8050/60000 (13%)]\tLoss: 1.508100\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8100/60000 (14%)]\tLoss: 1.519508\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8150/60000 (14%)]\tLoss: 1.514365\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8200/60000 (14%)]\tLoss: 1.483256\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8250/60000 (14%)]\tLoss: 1.481149\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8300/60000 (14%)]\tLoss: 1.461151\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8350/60000 (14%)]\tLoss: 1.469199\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8400/60000 (14%)]\tLoss: 1.461151\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8450/60000 (14%)]\tLoss: 1.461154\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8500/60000 (14%)]\tLoss: 1.500730\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8550/60000 (14%)]\tLoss: 1.480985\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8600/60000 (14%)]\tLoss: 1.481937\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8650/60000 (14%)]\tLoss: 1.482928\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8700/60000 (14%)]\tLoss: 1.500834\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8750/60000 (15%)]\tLoss: 1.461540\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8800/60000 (15%)]\tLoss: 1.481073\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8850/60000 (15%)]\tLoss: 1.481301\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8900/60000 (15%)]\tLoss: 1.475645\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [8950/60000 (15%)]\tLoss: 1.480796\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9000/60000 (15%)]\tLoss: 1.466418\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9050/60000 (15%)]\tLoss: 1.520219\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9100/60000 (15%)]\tLoss: 1.474714\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9150/60000 (15%)]\tLoss: 1.461155\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9200/60000 (15%)]\tLoss: 1.481705\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9250/60000 (15%)]\tLoss: 1.461151\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9300/60000 (16%)]\tLoss: 1.461153\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9350/60000 (16%)]\tLoss: 1.501894\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9400/60000 (16%)]\tLoss: 1.499405\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9450/60000 (16%)]\tLoss: 1.481942\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9500/60000 (16%)]\tLoss: 1.461171\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9550/60000 (16%)]\tLoss: 1.461620\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9600/60000 (16%)]\tLoss: 1.461213\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9650/60000 (16%)]\tLoss: 1.501246\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9700/60000 (16%)]\tLoss: 1.461154\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9750/60000 (16%)]\tLoss: 1.461502\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9800/60000 (16%)]\tLoss: 1.481203\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9850/60000 (16%)]\tLoss: 1.481124\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9900/60000 (16%)]\tLoss: 1.481159\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [9950/60000 (17%)]\tLoss: 1.497537\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10000/60000 (17%)]\tLoss: 1.481080\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10050/60000 (17%)]\tLoss: 1.467929\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10100/60000 (17%)]\tLoss: 1.461151\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10150/60000 (17%)]\tLoss: 1.482453\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10200/60000 (17%)]\tLoss: 1.481156\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10250/60000 (17%)]\tLoss: 1.493796\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10300/60000 (17%)]\tLoss: 1.538953\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10350/60000 (17%)]\tLoss: 1.501149\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10400/60000 (17%)]\tLoss: 1.461151\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10450/60000 (17%)]\tLoss: 1.501158\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10500/60000 (18%)]\tLoss: 1.481444\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10550/60000 (18%)]\tLoss: 1.461153\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10600/60000 (18%)]\tLoss: 1.461167\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10650/60000 (18%)]\tLoss: 1.461166\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10700/60000 (18%)]\tLoss: 1.502086\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10750/60000 (18%)]\tLoss: 1.481616\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10800/60000 (18%)]\tLoss: 1.461446\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10850/60000 (18%)]\tLoss: 1.483969\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10900/60000 (18%)]\tLoss: 1.461183\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [10950/60000 (18%)]\tLoss: 1.522896\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11000/60000 (18%)]\tLoss: 1.479552\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11050/60000 (18%)]\tLoss: 1.513768\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11100/60000 (18%)]\tLoss: 1.467027\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11150/60000 (19%)]\tLoss: 1.499552\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11200/60000 (19%)]\tLoss: 1.461198\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11250/60000 (19%)]\tLoss: 1.461153\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11300/60000 (19%)]\tLoss: 1.486760\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11350/60000 (19%)]\tLoss: 1.466981\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11400/60000 (19%)]\tLoss: 1.480936\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11450/60000 (19%)]\tLoss: 1.461239\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11500/60000 (19%)]\tLoss: 1.490623\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11550/60000 (19%)]\tLoss: 1.461156\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11600/60000 (19%)]\tLoss: 1.500883\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11650/60000 (19%)]\tLoss: 1.497367\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11700/60000 (20%)]\tLoss: 1.482336\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11750/60000 (20%)]\tLoss: 1.481129\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11800/60000 (20%)]\tLoss: 1.461151\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11850/60000 (20%)]\tLoss: 1.491566\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11900/60000 (20%)]\tLoss: 1.479300\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [11950/60000 (20%)]\tLoss: 1.461160\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12000/60000 (20%)]\tLoss: 1.461166\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12050/60000 (20%)]\tLoss: 1.475206\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12100/60000 (20%)]\tLoss: 1.512122\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12150/60000 (20%)]\tLoss: 1.467722\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12200/60000 (20%)]\tLoss: 1.498619\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12250/60000 (20%)]\tLoss: 1.463927\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12300/60000 (20%)]\tLoss: 1.472535\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12350/60000 (21%)]\tLoss: 1.481161\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12400/60000 (21%)]\tLoss: 1.461430\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12450/60000 (21%)]\tLoss: 1.461177\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12500/60000 (21%)]\tLoss: 1.485562\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12550/60000 (21%)]\tLoss: 1.549079\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12600/60000 (21%)]\tLoss: 1.466475\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12650/60000 (21%)]\tLoss: 1.540051\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12700/60000 (21%)]\tLoss: 1.498311\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12750/60000 (21%)]\tLoss: 1.461265\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12800/60000 (21%)]\tLoss: 1.461192\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12850/60000 (21%)]\tLoss: 1.465379\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12900/60000 (22%)]\tLoss: 1.481920\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [12950/60000 (22%)]\tLoss: 1.499493\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13000/60000 (22%)]\tLoss: 1.461713\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13050/60000 (22%)]\tLoss: 1.461720\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13100/60000 (22%)]\tLoss: 1.462824\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13150/60000 (22%)]\tLoss: 1.465389\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13200/60000 (22%)]\tLoss: 1.461260\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13250/60000 (22%)]\tLoss: 1.477502\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13300/60000 (22%)]\tLoss: 1.481189\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13350/60000 (22%)]\tLoss: 1.481226\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13400/60000 (22%)]\tLoss: 1.461505\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13450/60000 (22%)]\tLoss: 1.531533\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13500/60000 (22%)]\tLoss: 1.490023\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13550/60000 (23%)]\tLoss: 1.521112\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13600/60000 (23%)]\tLoss: 1.461162\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13650/60000 (23%)]\tLoss: 1.462242\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13700/60000 (23%)]\tLoss: 1.532210\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13750/60000 (23%)]\tLoss: 1.510213\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13800/60000 (23%)]\tLoss: 1.490829\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13850/60000 (23%)]\tLoss: 1.477229\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13900/60000 (23%)]\tLoss: 1.515770\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [13950/60000 (23%)]\tLoss: 1.461157\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14000/60000 (23%)]\tLoss: 1.490128\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14050/60000 (23%)]\tLoss: 1.481177\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14100/60000 (24%)]\tLoss: 1.467324\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14150/60000 (24%)]\tLoss: 1.502542\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14200/60000 (24%)]\tLoss: 1.492496\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14250/60000 (24%)]\tLoss: 1.498969\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14300/60000 (24%)]\tLoss: 1.463770\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14350/60000 (24%)]\tLoss: 1.515589\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14400/60000 (24%)]\tLoss: 1.491017\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14450/60000 (24%)]\tLoss: 1.489938\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14500/60000 (24%)]\tLoss: 1.481500\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14550/60000 (24%)]\tLoss: 1.501432\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14600/60000 (24%)]\tLoss: 1.481150\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14650/60000 (24%)]\tLoss: 1.482364\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14700/60000 (24%)]\tLoss: 1.461158\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14750/60000 (25%)]\tLoss: 1.520825\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14800/60000 (25%)]\tLoss: 1.493119\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14850/60000 (25%)]\tLoss: 1.480627\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14900/60000 (25%)]\tLoss: 1.461933\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [14950/60000 (25%)]\tLoss: 1.495412\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15000/60000 (25%)]\tLoss: 1.480339\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15050/60000 (25%)]\tLoss: 1.497972\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15100/60000 (25%)]\tLoss: 1.501432\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15150/60000 (25%)]\tLoss: 1.465192\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15200/60000 (25%)]\tLoss: 1.465413\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15250/60000 (25%)]\tLoss: 1.502471\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15300/60000 (26%)]\tLoss: 1.504989\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15350/60000 (26%)]\tLoss: 1.465759\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15400/60000 (26%)]\tLoss: 1.461154\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15450/60000 (26%)]\tLoss: 1.461151\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15500/60000 (26%)]\tLoss: 1.463775\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15550/60000 (26%)]\tLoss: 1.478378\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15600/60000 (26%)]\tLoss: 1.491551\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15650/60000 (26%)]\tLoss: 1.478022\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15700/60000 (26%)]\tLoss: 1.478122\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15750/60000 (26%)]\tLoss: 1.497564\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15800/60000 (26%)]\tLoss: 1.462334\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15850/60000 (26%)]\tLoss: 1.461183\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15900/60000 (26%)]\tLoss: 1.461184\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [15950/60000 (27%)]\tLoss: 1.466547\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16000/60000 (27%)]\tLoss: 1.509835\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16050/60000 (27%)]\tLoss: 1.462083\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16100/60000 (27%)]\tLoss: 1.492232\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16150/60000 (27%)]\tLoss: 1.471949\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16200/60000 (27%)]\tLoss: 1.461586\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16250/60000 (27%)]\tLoss: 1.512222\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16300/60000 (27%)]\tLoss: 1.470870\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16350/60000 (27%)]\tLoss: 1.462510\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16400/60000 (27%)]\tLoss: 1.473475\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16450/60000 (27%)]\tLoss: 1.488119\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16500/60000 (28%)]\tLoss: 1.516167\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16550/60000 (28%)]\tLoss: 1.519655\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16600/60000 (28%)]\tLoss: 1.483342\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16650/60000 (28%)]\tLoss: 1.473376\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16700/60000 (28%)]\tLoss: 1.481023\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16750/60000 (28%)]\tLoss: 1.468407\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16800/60000 (28%)]\tLoss: 1.478356\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16850/60000 (28%)]\tLoss: 1.461352\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16900/60000 (28%)]\tLoss: 1.486182\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [16950/60000 (28%)]\tLoss: 1.481125\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17000/60000 (28%)]\tLoss: 1.482751\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17050/60000 (28%)]\tLoss: 1.481092\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17100/60000 (28%)]\tLoss: 1.481524\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17150/60000 (29%)]\tLoss: 1.468954\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17200/60000 (29%)]\tLoss: 1.462201\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17250/60000 (29%)]\tLoss: 1.461723\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17300/60000 (29%)]\tLoss: 1.519913\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17350/60000 (29%)]\tLoss: 1.461202\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17400/60000 (29%)]\tLoss: 1.501104\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17450/60000 (29%)]\tLoss: 1.461556\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17500/60000 (29%)]\tLoss: 1.481657\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17550/60000 (29%)]\tLoss: 1.504090\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17600/60000 (29%)]\tLoss: 1.486240\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17650/60000 (29%)]\tLoss: 1.463898\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17700/60000 (30%)]\tLoss: 1.462485\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17750/60000 (30%)]\tLoss: 1.461151\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17800/60000 (30%)]\tLoss: 1.466056\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17850/60000 (30%)]\tLoss: 1.461686\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17900/60000 (30%)]\tLoss: 1.461151\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [17950/60000 (30%)]\tLoss: 1.539895\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18000/60000 (30%)]\tLoss: 1.461153\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18050/60000 (30%)]\tLoss: 1.462240\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18100/60000 (30%)]\tLoss: 1.481374\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18150/60000 (30%)]\tLoss: 1.461457\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18200/60000 (30%)]\tLoss: 1.469600\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18250/60000 (30%)]\tLoss: 1.473035\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18300/60000 (30%)]\tLoss: 1.461168\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18350/60000 (31%)]\tLoss: 1.508088\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18400/60000 (31%)]\tLoss: 1.468005\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18450/60000 (31%)]\tLoss: 1.501036\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18500/60000 (31%)]\tLoss: 1.498070\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18550/60000 (31%)]\tLoss: 1.492353\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18600/60000 (31%)]\tLoss: 1.480523\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18650/60000 (31%)]\tLoss: 1.462429\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18700/60000 (31%)]\tLoss: 1.513953\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18750/60000 (31%)]\tLoss: 1.471895\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18800/60000 (31%)]\tLoss: 1.461223\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18850/60000 (31%)]\tLoss: 1.462442\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18900/60000 (32%)]\tLoss: 1.461219\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [18950/60000 (32%)]\tLoss: 1.461151\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19000/60000 (32%)]\tLoss: 1.499285\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19050/60000 (32%)]\tLoss: 1.481631\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19100/60000 (32%)]\tLoss: 1.475813\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19150/60000 (32%)]\tLoss: 1.499636\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19200/60000 (32%)]\tLoss: 1.481051\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19250/60000 (32%)]\tLoss: 1.503131\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19300/60000 (32%)]\tLoss: 1.507195\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19350/60000 (32%)]\tLoss: 1.518495\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19400/60000 (32%)]\tLoss: 1.461180\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19450/60000 (32%)]\tLoss: 1.520392\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19500/60000 (32%)]\tLoss: 1.505131\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19550/60000 (33%)]\tLoss: 1.469279\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19600/60000 (33%)]\tLoss: 1.481151\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19650/60000 (33%)]\tLoss: 1.501148\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19700/60000 (33%)]\tLoss: 1.472412\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19750/60000 (33%)]\tLoss: 1.531290\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19800/60000 (33%)]\tLoss: 1.461721\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19850/60000 (33%)]\tLoss: 1.472370\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19900/60000 (33%)]\tLoss: 1.493052\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [19950/60000 (33%)]\tLoss: 1.461187\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20000/60000 (33%)]\tLoss: 1.501149\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20050/60000 (33%)]\tLoss: 1.461206\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20100/60000 (34%)]\tLoss: 1.461183\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20150/60000 (34%)]\tLoss: 1.469833\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20200/60000 (34%)]\tLoss: 1.461560\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20250/60000 (34%)]\tLoss: 1.463164\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20300/60000 (34%)]\tLoss: 1.461916\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20350/60000 (34%)]\tLoss: 1.461156\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20400/60000 (34%)]\tLoss: 1.481382\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20450/60000 (34%)]\tLoss: 1.480270\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20500/60000 (34%)]\tLoss: 1.461203\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20550/60000 (34%)]\tLoss: 1.521135\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20600/60000 (34%)]\tLoss: 1.481247\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20650/60000 (34%)]\tLoss: 1.492306\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20700/60000 (34%)]\tLoss: 1.474316\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20750/60000 (35%)]\tLoss: 1.479893\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20800/60000 (35%)]\tLoss: 1.483302\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20850/60000 (35%)]\tLoss: 1.520012\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20900/60000 (35%)]\tLoss: 1.462668\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [20950/60000 (35%)]\tLoss: 1.461151\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21000/60000 (35%)]\tLoss: 1.461411\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21050/60000 (35%)]\tLoss: 1.536867\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21100/60000 (35%)]\tLoss: 1.463607\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21150/60000 (35%)]\tLoss: 1.462256\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21200/60000 (35%)]\tLoss: 1.461598\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21250/60000 (35%)]\tLoss: 1.481110\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21300/60000 (36%)]\tLoss: 1.461303\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21350/60000 (36%)]\tLoss: 1.518813\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21400/60000 (36%)]\tLoss: 1.516186\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21450/60000 (36%)]\tLoss: 1.472224\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21500/60000 (36%)]\tLoss: 1.472695\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21550/60000 (36%)]\tLoss: 1.489166\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21600/60000 (36%)]\tLoss: 1.474618\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21650/60000 (36%)]\tLoss: 1.497679\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21700/60000 (36%)]\tLoss: 1.461184\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21750/60000 (36%)]\tLoss: 1.480005\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21800/60000 (36%)]\tLoss: 1.550417\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21850/60000 (36%)]\tLoss: 1.486786\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21900/60000 (36%)]\tLoss: 1.464876\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [21950/60000 (37%)]\tLoss: 1.501123\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22000/60000 (37%)]\tLoss: 1.507209\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22050/60000 (37%)]\tLoss: 1.500957\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22100/60000 (37%)]\tLoss: 1.481800\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22150/60000 (37%)]\tLoss: 1.476067\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22200/60000 (37%)]\tLoss: 1.461179\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22250/60000 (37%)]\tLoss: 1.461982\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22300/60000 (37%)]\tLoss: 1.478535\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22350/60000 (37%)]\tLoss: 1.481017\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22400/60000 (37%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22450/60000 (37%)]\tLoss: 1.501027\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22500/60000 (38%)]\tLoss: 1.520372\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22550/60000 (38%)]\tLoss: 1.466385\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22600/60000 (38%)]\tLoss: 1.512817\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22650/60000 (38%)]\tLoss: 1.483019\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22700/60000 (38%)]\tLoss: 1.487728\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22750/60000 (38%)]\tLoss: 1.463006\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22800/60000 (38%)]\tLoss: 1.497164\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22850/60000 (38%)]\tLoss: 1.479434\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22900/60000 (38%)]\tLoss: 1.461227\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [22950/60000 (38%)]\tLoss: 1.491670\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23000/60000 (38%)]\tLoss: 1.481151\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23050/60000 (38%)]\tLoss: 1.461151\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23100/60000 (38%)]\tLoss: 1.480631\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23150/60000 (39%)]\tLoss: 1.500756\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23200/60000 (39%)]\tLoss: 1.481198\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23250/60000 (39%)]\tLoss: 1.481302\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23300/60000 (39%)]\tLoss: 1.463902\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23350/60000 (39%)]\tLoss: 1.488435\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23400/60000 (39%)]\tLoss: 1.481168\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23450/60000 (39%)]\tLoss: 1.461155\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23500/60000 (39%)]\tLoss: 1.481355\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23550/60000 (39%)]\tLoss: 1.495360\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23600/60000 (39%)]\tLoss: 1.480517\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23650/60000 (39%)]\tLoss: 1.500857\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23700/60000 (40%)]\tLoss: 1.486165\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23750/60000 (40%)]\tLoss: 1.481283\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23800/60000 (40%)]\tLoss: 1.486740\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23850/60000 (40%)]\tLoss: 1.489108\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23900/60000 (40%)]\tLoss: 1.464338\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [23950/60000 (40%)]\tLoss: 1.461154\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24000/60000 (40%)]\tLoss: 1.464345\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24050/60000 (40%)]\tLoss: 1.461152\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24100/60000 (40%)]\tLoss: 1.479903\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24150/60000 (40%)]\tLoss: 1.463540\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24200/60000 (40%)]\tLoss: 1.481413\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24250/60000 (40%)]\tLoss: 1.481141\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24300/60000 (40%)]\tLoss: 1.461152\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24350/60000 (41%)]\tLoss: 1.510639\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24400/60000 (41%)]\tLoss: 1.501409\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24450/60000 (41%)]\tLoss: 1.465004\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24500/60000 (41%)]\tLoss: 1.489368\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24550/60000 (41%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24600/60000 (41%)]\tLoss: 1.461332\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24650/60000 (41%)]\tLoss: 1.481354\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24700/60000 (41%)]\tLoss: 1.461326\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24750/60000 (41%)]\tLoss: 1.486531\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24800/60000 (41%)]\tLoss: 1.495663\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24850/60000 (41%)]\tLoss: 1.464574\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24900/60000 (42%)]\tLoss: 1.481575\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [24950/60000 (42%)]\tLoss: 1.462677\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25000/60000 (42%)]\tLoss: 1.515544\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25050/60000 (42%)]\tLoss: 1.464768\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25100/60000 (42%)]\tLoss: 1.492876\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25150/60000 (42%)]\tLoss: 1.464501\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25200/60000 (42%)]\tLoss: 1.481896\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25250/60000 (42%)]\tLoss: 1.462288\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25300/60000 (42%)]\tLoss: 1.463672\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25350/60000 (42%)]\tLoss: 1.481516\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25400/60000 (42%)]\tLoss: 1.498402\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25450/60000 (42%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25500/60000 (42%)]\tLoss: 1.480280\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25550/60000 (43%)]\tLoss: 1.498862\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25600/60000 (43%)]\tLoss: 1.496551\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25650/60000 (43%)]\tLoss: 1.482132\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25700/60000 (43%)]\tLoss: 1.495028\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25750/60000 (43%)]\tLoss: 1.480731\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25800/60000 (43%)]\tLoss: 1.519734\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25850/60000 (43%)]\tLoss: 1.482154\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25900/60000 (43%)]\tLoss: 1.501992\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [25950/60000 (43%)]\tLoss: 1.484159\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26000/60000 (43%)]\tLoss: 1.497731\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26050/60000 (43%)]\tLoss: 1.501363\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26100/60000 (44%)]\tLoss: 1.481694\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26150/60000 (44%)]\tLoss: 1.486226\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26200/60000 (44%)]\tLoss: 1.520631\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26250/60000 (44%)]\tLoss: 1.496533\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26300/60000 (44%)]\tLoss: 1.481156\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26350/60000 (44%)]\tLoss: 1.500577\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26400/60000 (44%)]\tLoss: 1.473220\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26450/60000 (44%)]\tLoss: 1.489731\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26500/60000 (44%)]\tLoss: 1.461151\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26550/60000 (44%)]\tLoss: 1.481126\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26600/60000 (44%)]\tLoss: 1.504169\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26650/60000 (44%)]\tLoss: 1.481074\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26700/60000 (44%)]\tLoss: 1.498542\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26750/60000 (45%)]\tLoss: 1.500453\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26800/60000 (45%)]\tLoss: 1.501236\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26850/60000 (45%)]\tLoss: 1.469895\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26900/60000 (45%)]\tLoss: 1.461153\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [26950/60000 (45%)]\tLoss: 1.482758\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27000/60000 (45%)]\tLoss: 1.501951\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27050/60000 (45%)]\tLoss: 1.493864\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27100/60000 (45%)]\tLoss: 1.482506\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27150/60000 (45%)]\tLoss: 1.462054\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27200/60000 (45%)]\tLoss: 1.461241\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27250/60000 (45%)]\tLoss: 1.469007\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27300/60000 (46%)]\tLoss: 1.485567\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27350/60000 (46%)]\tLoss: 1.481156\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27400/60000 (46%)]\tLoss: 1.469708\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27450/60000 (46%)]\tLoss: 1.479109\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27500/60000 (46%)]\tLoss: 1.461294\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27550/60000 (46%)]\tLoss: 1.462546\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27600/60000 (46%)]\tLoss: 1.481183\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27650/60000 (46%)]\tLoss: 1.462207\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27700/60000 (46%)]\tLoss: 1.481154\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27750/60000 (46%)]\tLoss: 1.495627\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27800/60000 (46%)]\tLoss: 1.480782\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27850/60000 (46%)]\tLoss: 1.521133\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27900/60000 (46%)]\tLoss: 1.502102\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [27950/60000 (47%)]\tLoss: 1.461234\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28000/60000 (47%)]\tLoss: 1.461153\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28050/60000 (47%)]\tLoss: 1.478910\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28100/60000 (47%)]\tLoss: 1.461258\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28150/60000 (47%)]\tLoss: 1.461151\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28200/60000 (47%)]\tLoss: 1.480676\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28250/60000 (47%)]\tLoss: 1.511193\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28300/60000 (47%)]\tLoss: 1.481171\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28350/60000 (47%)]\tLoss: 1.461278\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28400/60000 (47%)]\tLoss: 1.477572\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28450/60000 (47%)]\tLoss: 1.482206\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28500/60000 (48%)]\tLoss: 1.481091\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28550/60000 (48%)]\tLoss: 1.517080\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28600/60000 (48%)]\tLoss: 1.501511\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28650/60000 (48%)]\tLoss: 1.501153\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28700/60000 (48%)]\tLoss: 1.461207\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28750/60000 (48%)]\tLoss: 1.461151\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28800/60000 (48%)]\tLoss: 1.475990\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28850/60000 (48%)]\tLoss: 1.461579\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28900/60000 (48%)]\tLoss: 1.475633\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [28950/60000 (48%)]\tLoss: 1.500529\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29000/60000 (48%)]\tLoss: 1.502524\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29050/60000 (48%)]\tLoss: 1.461805\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29100/60000 (48%)]\tLoss: 1.461162\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29150/60000 (49%)]\tLoss: 1.463526\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29200/60000 (49%)]\tLoss: 1.509042\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29250/60000 (49%)]\tLoss: 1.500233\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29300/60000 (49%)]\tLoss: 1.500322\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29350/60000 (49%)]\tLoss: 1.471795\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29400/60000 (49%)]\tLoss: 1.490183\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29450/60000 (49%)]\tLoss: 1.502161\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29500/60000 (49%)]\tLoss: 1.464170\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29550/60000 (49%)]\tLoss: 1.469408\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29600/60000 (49%)]\tLoss: 1.479110\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29650/60000 (49%)]\tLoss: 1.461151\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29700/60000 (50%)]\tLoss: 1.461220\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29750/60000 (50%)]\tLoss: 1.497544\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29800/60000 (50%)]\tLoss: 1.514622\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29850/60000 (50%)]\tLoss: 1.461153\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29900/60000 (50%)]\tLoss: 1.467882\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [29950/60000 (50%)]\tLoss: 1.514157\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30000/60000 (50%)]\tLoss: 1.487615\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30050/60000 (50%)]\tLoss: 1.461161\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30100/60000 (50%)]\tLoss: 1.464410\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30150/60000 (50%)]\tLoss: 1.481386\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30200/60000 (50%)]\tLoss: 1.470091\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30250/60000 (50%)]\tLoss: 1.462852\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30300/60000 (50%)]\tLoss: 1.509481\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30350/60000 (51%)]\tLoss: 1.522114\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30400/60000 (51%)]\tLoss: 1.520783\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30450/60000 (51%)]\tLoss: 1.471039\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30500/60000 (51%)]\tLoss: 1.461184\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30550/60000 (51%)]\tLoss: 1.480902\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30600/60000 (51%)]\tLoss: 1.495125\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30650/60000 (51%)]\tLoss: 1.461908\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30700/60000 (51%)]\tLoss: 1.481178\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30750/60000 (51%)]\tLoss: 1.480997\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30800/60000 (51%)]\tLoss: 1.481162\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30850/60000 (51%)]\tLoss: 1.517566\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30900/60000 (52%)]\tLoss: 1.480902\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [30950/60000 (52%)]\tLoss: 1.498789\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31000/60000 (52%)]\tLoss: 1.482582\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31050/60000 (52%)]\tLoss: 1.461161\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31100/60000 (52%)]\tLoss: 1.489429\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31150/60000 (52%)]\tLoss: 1.488635\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31200/60000 (52%)]\tLoss: 1.461154\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31250/60000 (52%)]\tLoss: 1.470913\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31300/60000 (52%)]\tLoss: 1.497435\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31350/60000 (52%)]\tLoss: 1.462214\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31400/60000 (52%)]\tLoss: 1.462206\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31450/60000 (52%)]\tLoss: 1.481188\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31500/60000 (52%)]\tLoss: 1.501152\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31550/60000 (53%)]\tLoss: 1.482753\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31600/60000 (53%)]\tLoss: 1.461160\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31650/60000 (53%)]\tLoss: 1.461266\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31700/60000 (53%)]\tLoss: 1.504764\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31750/60000 (53%)]\tLoss: 1.501175\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31800/60000 (53%)]\tLoss: 1.482409\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31850/60000 (53%)]\tLoss: 1.461740\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31900/60000 (53%)]\tLoss: 1.506704\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [31950/60000 (53%)]\tLoss: 1.483524\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32000/60000 (53%)]\tLoss: 1.481259\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32050/60000 (53%)]\tLoss: 1.464820\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32100/60000 (54%)]\tLoss: 1.481186\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32150/60000 (54%)]\tLoss: 1.461274\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32200/60000 (54%)]\tLoss: 1.462410\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32250/60000 (54%)]\tLoss: 1.501161\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32300/60000 (54%)]\tLoss: 1.461152\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32350/60000 (54%)]\tLoss: 1.479648\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32400/60000 (54%)]\tLoss: 1.496059\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32450/60000 (54%)]\tLoss: 1.480489\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32500/60000 (54%)]\tLoss: 1.461189\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32550/60000 (54%)]\tLoss: 1.474145\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32600/60000 (54%)]\tLoss: 1.479048\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32650/60000 (54%)]\tLoss: 1.477068\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32700/60000 (54%)]\tLoss: 1.461151\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32750/60000 (55%)]\tLoss: 1.466096\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32800/60000 (55%)]\tLoss: 1.465201\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32850/60000 (55%)]\tLoss: 1.481193\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32900/60000 (55%)]\tLoss: 1.481141\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [32950/60000 (55%)]\tLoss: 1.481204\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33000/60000 (55%)]\tLoss: 1.461551\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33050/60000 (55%)]\tLoss: 1.462217\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33100/60000 (55%)]\tLoss: 1.482406\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33150/60000 (55%)]\tLoss: 1.467823\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33200/60000 (55%)]\tLoss: 1.481170\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33250/60000 (55%)]\tLoss: 1.461151\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33300/60000 (56%)]\tLoss: 1.480806\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33350/60000 (56%)]\tLoss: 1.461172\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33400/60000 (56%)]\tLoss: 1.482152\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33450/60000 (56%)]\tLoss: 1.487191\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33500/60000 (56%)]\tLoss: 1.482325\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33550/60000 (56%)]\tLoss: 1.502711\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33600/60000 (56%)]\tLoss: 1.481575\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33650/60000 (56%)]\tLoss: 1.481643\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33700/60000 (56%)]\tLoss: 1.499367\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33750/60000 (56%)]\tLoss: 1.481153\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33800/60000 (56%)]\tLoss: 1.481278\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33850/60000 (56%)]\tLoss: 1.476896\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33900/60000 (56%)]\tLoss: 1.480732\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [33950/60000 (57%)]\tLoss: 1.481161\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34000/60000 (57%)]\tLoss: 1.481262\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34050/60000 (57%)]\tLoss: 1.480503\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34100/60000 (57%)]\tLoss: 1.461175\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34150/60000 (57%)]\tLoss: 1.499878\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34200/60000 (57%)]\tLoss: 1.461153\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34250/60000 (57%)]\tLoss: 1.508215\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34300/60000 (57%)]\tLoss: 1.461151\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34350/60000 (57%)]\tLoss: 1.481141\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34400/60000 (57%)]\tLoss: 1.480533\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34450/60000 (57%)]\tLoss: 1.470777\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34500/60000 (58%)]\tLoss: 1.486765\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34550/60000 (58%)]\tLoss: 1.465240\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34600/60000 (58%)]\tLoss: 1.501167\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34650/60000 (58%)]\tLoss: 1.481157\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34700/60000 (58%)]\tLoss: 1.462523\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34750/60000 (58%)]\tLoss: 1.472820\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34800/60000 (58%)]\tLoss: 1.490502\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34850/60000 (58%)]\tLoss: 1.463839\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34900/60000 (58%)]\tLoss: 1.487308\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [34950/60000 (58%)]\tLoss: 1.484504\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35000/60000 (58%)]\tLoss: 1.461178\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35050/60000 (58%)]\tLoss: 1.499984\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35100/60000 (58%)]\tLoss: 1.481224\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35150/60000 (59%)]\tLoss: 1.489328\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35200/60000 (59%)]\tLoss: 1.492387\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35250/60000 (59%)]\tLoss: 1.466270\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35300/60000 (59%)]\tLoss: 1.462832\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35350/60000 (59%)]\tLoss: 1.498993\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35400/60000 (59%)]\tLoss: 1.462896\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35450/60000 (59%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35500/60000 (59%)]\tLoss: 1.481296\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35550/60000 (59%)]\tLoss: 1.481158\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35600/60000 (59%)]\tLoss: 1.481499\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35650/60000 (59%)]\tLoss: 1.461371\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35700/60000 (60%)]\tLoss: 1.470642\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35750/60000 (60%)]\tLoss: 1.481317\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35800/60000 (60%)]\tLoss: 1.481056\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35850/60000 (60%)]\tLoss: 1.496936\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35900/60000 (60%)]\tLoss: 1.461223\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [35950/60000 (60%)]\tLoss: 1.462693\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36000/60000 (60%)]\tLoss: 1.461472\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36050/60000 (60%)]\tLoss: 1.462615\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36100/60000 (60%)]\tLoss: 1.482062\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36150/60000 (60%)]\tLoss: 1.461161\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36200/60000 (60%)]\tLoss: 1.501037\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36250/60000 (60%)]\tLoss: 1.490033\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36300/60000 (60%)]\tLoss: 1.481184\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36350/60000 (61%)]\tLoss: 1.480792\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36400/60000 (61%)]\tLoss: 1.485788\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36450/60000 (61%)]\tLoss: 1.481150\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36500/60000 (61%)]\tLoss: 1.501151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36550/60000 (61%)]\tLoss: 1.461177\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36600/60000 (61%)]\tLoss: 1.518209\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36650/60000 (61%)]\tLoss: 1.461172\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36700/60000 (61%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36750/60000 (61%)]\tLoss: 1.461154\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36800/60000 (61%)]\tLoss: 1.524505\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36850/60000 (61%)]\tLoss: 1.478063\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36900/60000 (62%)]\tLoss: 1.477591\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [36950/60000 (62%)]\tLoss: 1.481154\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37000/60000 (62%)]\tLoss: 1.481149\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37050/60000 (62%)]\tLoss: 1.463068\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37100/60000 (62%)]\tLoss: 1.501062\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37150/60000 (62%)]\tLoss: 1.500667\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37200/60000 (62%)]\tLoss: 1.499969\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37250/60000 (62%)]\tLoss: 1.462136\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37300/60000 (62%)]\tLoss: 1.481132\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37350/60000 (62%)]\tLoss: 1.485728\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37400/60000 (62%)]\tLoss: 1.467230\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37450/60000 (62%)]\tLoss: 1.481342\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37500/60000 (62%)]\tLoss: 1.461289\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37550/60000 (63%)]\tLoss: 1.481158\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37600/60000 (63%)]\tLoss: 1.496200\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37650/60000 (63%)]\tLoss: 1.500531\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37700/60000 (63%)]\tLoss: 1.479026\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37750/60000 (63%)]\tLoss: 1.461177\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37800/60000 (63%)]\tLoss: 1.464979\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37850/60000 (63%)]\tLoss: 1.463763\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37900/60000 (63%)]\tLoss: 1.461993\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [37950/60000 (63%)]\tLoss: 1.481181\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38000/60000 (63%)]\tLoss: 1.480145\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38050/60000 (63%)]\tLoss: 1.481108\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38100/60000 (64%)]\tLoss: 1.461697\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38150/60000 (64%)]\tLoss: 1.481239\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38200/60000 (64%)]\tLoss: 1.520150\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38250/60000 (64%)]\tLoss: 1.501153\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38300/60000 (64%)]\tLoss: 1.481245\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38350/60000 (64%)]\tLoss: 1.484276\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38400/60000 (64%)]\tLoss: 1.509837\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38450/60000 (64%)]\tLoss: 1.521008\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38500/60000 (64%)]\tLoss: 1.520993\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38550/60000 (64%)]\tLoss: 1.504180\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38600/60000 (64%)]\tLoss: 1.501304\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38650/60000 (64%)]\tLoss: 1.474182\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38700/60000 (64%)]\tLoss: 1.463580\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38750/60000 (65%)]\tLoss: 1.473972\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38800/60000 (65%)]\tLoss: 1.472901\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38850/60000 (65%)]\tLoss: 1.461220\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38900/60000 (65%)]\tLoss: 1.506988\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [38950/60000 (65%)]\tLoss: 1.534326\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39000/60000 (65%)]\tLoss: 1.495352\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39050/60000 (65%)]\tLoss: 1.476391\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39100/60000 (65%)]\tLoss: 1.461151\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39150/60000 (65%)]\tLoss: 1.481198\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39200/60000 (65%)]\tLoss: 1.461622\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39250/60000 (65%)]\tLoss: 1.496295\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39300/60000 (66%)]\tLoss: 1.461155\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39350/60000 (66%)]\tLoss: 1.461397\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39400/60000 (66%)]\tLoss: 1.481203\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39450/60000 (66%)]\tLoss: 1.501575\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39500/60000 (66%)]\tLoss: 1.461283\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39550/60000 (66%)]\tLoss: 1.497624\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39600/60000 (66%)]\tLoss: 1.500316\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39650/60000 (66%)]\tLoss: 1.462018\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39700/60000 (66%)]\tLoss: 1.496901\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39750/60000 (66%)]\tLoss: 1.520602\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39800/60000 (66%)]\tLoss: 1.516658\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39850/60000 (66%)]\tLoss: 1.526913\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39900/60000 (66%)]\tLoss: 1.461215\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [39950/60000 (67%)]\tLoss: 1.510415\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40000/60000 (67%)]\tLoss: 1.461961\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40050/60000 (67%)]\tLoss: 1.495975\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40100/60000 (67%)]\tLoss: 1.461213\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40150/60000 (67%)]\tLoss: 1.480526\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40200/60000 (67%)]\tLoss: 1.461162\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40250/60000 (67%)]\tLoss: 1.461167\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40300/60000 (67%)]\tLoss: 1.482632\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40350/60000 (67%)]\tLoss: 1.461154\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40400/60000 (67%)]\tLoss: 1.497159\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40450/60000 (67%)]\tLoss: 1.461276\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40500/60000 (68%)]\tLoss: 1.461160\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40550/60000 (68%)]\tLoss: 1.461152\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40600/60000 (68%)]\tLoss: 1.500839\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40650/60000 (68%)]\tLoss: 1.504328\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40700/60000 (68%)]\tLoss: 1.464251\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40750/60000 (68%)]\tLoss: 1.470486\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40800/60000 (68%)]\tLoss: 1.466785\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40850/60000 (68%)]\tLoss: 1.497015\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40900/60000 (68%)]\tLoss: 1.479319\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [40950/60000 (68%)]\tLoss: 1.478686\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41000/60000 (68%)]\tLoss: 1.461151\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41050/60000 (68%)]\tLoss: 1.500779\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41100/60000 (68%)]\tLoss: 1.508772\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41150/60000 (69%)]\tLoss: 1.470113\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41200/60000 (69%)]\tLoss: 1.518924\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41250/60000 (69%)]\tLoss: 1.512943\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41300/60000 (69%)]\tLoss: 1.501295\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41350/60000 (69%)]\tLoss: 1.481147\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41400/60000 (69%)]\tLoss: 1.501076\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41450/60000 (69%)]\tLoss: 1.531470\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41500/60000 (69%)]\tLoss: 1.489927\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41550/60000 (69%)]\tLoss: 1.480790\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41600/60000 (69%)]\tLoss: 1.461238\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41650/60000 (69%)]\tLoss: 1.461151\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41700/60000 (70%)]\tLoss: 1.500237\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41750/60000 (70%)]\tLoss: 1.501093\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41800/60000 (70%)]\tLoss: 1.480576\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41850/60000 (70%)]\tLoss: 1.485672\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41900/60000 (70%)]\tLoss: 1.481157\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [41950/60000 (70%)]\tLoss: 1.467524\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42000/60000 (70%)]\tLoss: 1.469754\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42050/60000 (70%)]\tLoss: 1.483732\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42100/60000 (70%)]\tLoss: 1.484859\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42150/60000 (70%)]\tLoss: 1.521007\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42200/60000 (70%)]\tLoss: 1.481112\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42250/60000 (70%)]\tLoss: 1.461375\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42300/60000 (70%)]\tLoss: 1.480615\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42350/60000 (71%)]\tLoss: 1.481696\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42400/60000 (71%)]\tLoss: 1.499815\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42450/60000 (71%)]\tLoss: 1.540134\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42500/60000 (71%)]\tLoss: 1.537239\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42550/60000 (71%)]\tLoss: 1.463654\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42600/60000 (71%)]\tLoss: 1.493305\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42650/60000 (71%)]\tLoss: 1.463367\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42700/60000 (71%)]\tLoss: 1.461589\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42750/60000 (71%)]\tLoss: 1.500818\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42800/60000 (71%)]\tLoss: 1.491219\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42850/60000 (71%)]\tLoss: 1.481186\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42900/60000 (72%)]\tLoss: 1.493944\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [42950/60000 (72%)]\tLoss: 1.462895\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43000/60000 (72%)]\tLoss: 1.488548\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43050/60000 (72%)]\tLoss: 1.479293\tAccuracy: 97.99%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43100/60000 (72%)]\tLoss: 1.461152\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43150/60000 (72%)]\tLoss: 1.483558\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43200/60000 (72%)]\tLoss: 1.461183\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43250/60000 (72%)]\tLoss: 1.461151\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43300/60000 (72%)]\tLoss: 1.461272\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43350/60000 (72%)]\tLoss: 1.464395\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43400/60000 (72%)]\tLoss: 1.461204\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43450/60000 (72%)]\tLoss: 1.494349\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43500/60000 (72%)]\tLoss: 1.481151\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43550/60000 (73%)]\tLoss: 1.464095\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43600/60000 (73%)]\tLoss: 1.491669\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43650/60000 (73%)]\tLoss: 1.461154\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43700/60000 (73%)]\tLoss: 1.483701\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43750/60000 (73%)]\tLoss: 1.461458\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43800/60000 (73%)]\tLoss: 1.486499\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43850/60000 (73%)]\tLoss: 1.498823\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43900/60000 (73%)]\tLoss: 1.463447\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [43950/60000 (73%)]\tLoss: 1.481151\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44000/60000 (73%)]\tLoss: 1.484674\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44050/60000 (73%)]\tLoss: 1.481174\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44100/60000 (74%)]\tLoss: 1.484221\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44150/60000 (74%)]\tLoss: 1.461156\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44200/60000 (74%)]\tLoss: 1.462096\tAccuracy: 98.01%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44250/60000 (74%)]\tLoss: 1.461157\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44300/60000 (74%)]\tLoss: 1.494310\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44350/60000 (74%)]\tLoss: 1.461886\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44400/60000 (74%)]\tLoss: 1.481191\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44450/60000 (74%)]\tLoss: 1.461306\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44500/60000 (74%)]\tLoss: 1.461961\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44550/60000 (74%)]\tLoss: 1.500939\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44600/60000 (74%)]\tLoss: 1.461152\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44650/60000 (74%)]\tLoss: 1.487032\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44700/60000 (74%)]\tLoss: 1.501376\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44750/60000 (75%)]\tLoss: 1.461620\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44800/60000 (75%)]\tLoss: 1.461154\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44850/60000 (75%)]\tLoss: 1.481301\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44900/60000 (75%)]\tLoss: 1.479302\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [44950/60000 (75%)]\tLoss: 1.461553\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45000/60000 (75%)]\tLoss: 1.464658\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45050/60000 (75%)]\tLoss: 1.461178\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45100/60000 (75%)]\tLoss: 1.461478\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45150/60000 (75%)]\tLoss: 1.461663\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45200/60000 (75%)]\tLoss: 1.461167\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45250/60000 (75%)]\tLoss: 1.461152\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45300/60000 (76%)]\tLoss: 1.480707\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45350/60000 (76%)]\tLoss: 1.481151\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45400/60000 (76%)]\tLoss: 1.512673\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45450/60000 (76%)]\tLoss: 1.513010\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45500/60000 (76%)]\tLoss: 1.481059\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45550/60000 (76%)]\tLoss: 1.461328\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45600/60000 (76%)]\tLoss: 1.463018\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45650/60000 (76%)]\tLoss: 1.462667\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45700/60000 (76%)]\tLoss: 1.495499\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45750/60000 (76%)]\tLoss: 1.462806\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45800/60000 (76%)]\tLoss: 1.486829\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45850/60000 (76%)]\tLoss: 1.472835\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45900/60000 (76%)]\tLoss: 1.461160\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [45950/60000 (77%)]\tLoss: 1.461163\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46000/60000 (77%)]\tLoss: 1.481143\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46050/60000 (77%)]\tLoss: 1.481146\tAccuracy: 98.04%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46100/60000 (77%)]\tLoss: 1.465031\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46150/60000 (77%)]\tLoss: 1.461151\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46200/60000 (77%)]\tLoss: 1.481273\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46250/60000 (77%)]\tLoss: 1.481201\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46300/60000 (77%)]\tLoss: 1.461151\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46350/60000 (77%)]\tLoss: 1.480786\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46400/60000 (77%)]\tLoss: 1.461233\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46450/60000 (77%)]\tLoss: 1.463580\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46500/60000 (78%)]\tLoss: 1.533486\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46550/60000 (78%)]\tLoss: 1.461208\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46600/60000 (78%)]\tLoss: 1.501074\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46650/60000 (78%)]\tLoss: 1.461163\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46700/60000 (78%)]\tLoss: 1.474460\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46750/60000 (78%)]\tLoss: 1.484451\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46800/60000 (78%)]\tLoss: 1.472916\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46850/60000 (78%)]\tLoss: 1.481032\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46900/60000 (78%)]\tLoss: 1.485829\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [46950/60000 (78%)]\tLoss: 1.461190\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47000/60000 (78%)]\tLoss: 1.481147\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47050/60000 (78%)]\tLoss: 1.462547\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47100/60000 (78%)]\tLoss: 1.481877\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47150/60000 (79%)]\tLoss: 1.481144\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47200/60000 (79%)]\tLoss: 1.476399\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47250/60000 (79%)]\tLoss: 1.506779\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47300/60000 (79%)]\tLoss: 1.484782\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47350/60000 (79%)]\tLoss: 1.482199\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47400/60000 (79%)]\tLoss: 1.483166\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47450/60000 (79%)]\tLoss: 1.461151\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47500/60000 (79%)]\tLoss: 1.461165\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47550/60000 (79%)]\tLoss: 1.461211\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47600/60000 (79%)]\tLoss: 1.467025\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47650/60000 (79%)]\tLoss: 1.461153\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47700/60000 (80%)]\tLoss: 1.500515\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47750/60000 (80%)]\tLoss: 1.464267\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47800/60000 (80%)]\tLoss: 1.465113\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47850/60000 (80%)]\tLoss: 1.479112\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47900/60000 (80%)]\tLoss: 1.472171\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [47950/60000 (80%)]\tLoss: 1.461275\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48000/60000 (80%)]\tLoss: 1.461155\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48050/60000 (80%)]\tLoss: 1.535315\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48100/60000 (80%)]\tLoss: 1.484792\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48150/60000 (80%)]\tLoss: 1.481322\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48200/60000 (80%)]\tLoss: 1.466539\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48250/60000 (80%)]\tLoss: 1.461175\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48300/60000 (80%)]\tLoss: 1.481110\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48350/60000 (81%)]\tLoss: 1.479384\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48400/60000 (81%)]\tLoss: 1.474027\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48450/60000 (81%)]\tLoss: 1.504220\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48500/60000 (81%)]\tLoss: 1.481169\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48550/60000 (81%)]\tLoss: 1.518512\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48600/60000 (81%)]\tLoss: 1.481242\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48650/60000 (81%)]\tLoss: 1.482301\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48700/60000 (81%)]\tLoss: 1.514762\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48750/60000 (81%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48800/60000 (81%)]\tLoss: 1.480157\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48850/60000 (81%)]\tLoss: 1.463099\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48900/60000 (82%)]\tLoss: 1.500108\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [48950/60000 (82%)]\tLoss: 1.481155\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49000/60000 (82%)]\tLoss: 1.480357\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49050/60000 (82%)]\tLoss: 1.461152\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49100/60000 (82%)]\tLoss: 1.481198\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49150/60000 (82%)]\tLoss: 1.463086\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49200/60000 (82%)]\tLoss: 1.462478\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49250/60000 (82%)]\tLoss: 1.463732\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49300/60000 (82%)]\tLoss: 1.501149\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49350/60000 (82%)]\tLoss: 1.461166\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49400/60000 (82%)]\tLoss: 1.481144\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49450/60000 (82%)]\tLoss: 1.494066\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49500/60000 (82%)]\tLoss: 1.461868\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49550/60000 (83%)]\tLoss: 1.479495\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49600/60000 (83%)]\tLoss: 1.463425\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49650/60000 (83%)]\tLoss: 1.468705\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49700/60000 (83%)]\tLoss: 1.481492\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49750/60000 (83%)]\tLoss: 1.482153\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49800/60000 (83%)]\tLoss: 1.484517\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49850/60000 (83%)]\tLoss: 1.491378\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49900/60000 (83%)]\tLoss: 1.505586\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [49950/60000 (83%)]\tLoss: 1.480828\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50000/60000 (83%)]\tLoss: 1.495788\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50050/60000 (83%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50100/60000 (84%)]\tLoss: 1.461879\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50150/60000 (84%)]\tLoss: 1.476433\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50200/60000 (84%)]\tLoss: 1.481156\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50250/60000 (84%)]\tLoss: 1.482432\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50300/60000 (84%)]\tLoss: 1.479761\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50350/60000 (84%)]\tLoss: 1.461152\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50400/60000 (84%)]\tLoss: 1.501093\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50450/60000 (84%)]\tLoss: 1.461221\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50500/60000 (84%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50550/60000 (84%)]\tLoss: 1.465182\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50600/60000 (84%)]\tLoss: 1.461158\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50650/60000 (84%)]\tLoss: 1.495134\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50700/60000 (84%)]\tLoss: 1.510284\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50750/60000 (85%)]\tLoss: 1.461228\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50800/60000 (85%)]\tLoss: 1.501628\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50850/60000 (85%)]\tLoss: 1.461176\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50900/60000 (85%)]\tLoss: 1.478684\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [50950/60000 (85%)]\tLoss: 1.461389\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51000/60000 (85%)]\tLoss: 1.462461\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51050/60000 (85%)]\tLoss: 1.481187\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51100/60000 (85%)]\tLoss: 1.481235\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51150/60000 (85%)]\tLoss: 1.461166\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51200/60000 (85%)]\tLoss: 1.461151\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51250/60000 (85%)]\tLoss: 1.462502\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51300/60000 (86%)]\tLoss: 1.474039\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51350/60000 (86%)]\tLoss: 1.461151\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51400/60000 (86%)]\tLoss: 1.491558\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51450/60000 (86%)]\tLoss: 1.461828\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51500/60000 (86%)]\tLoss: 1.521534\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51550/60000 (86%)]\tLoss: 1.501077\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51600/60000 (86%)]\tLoss: 1.461151\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51650/60000 (86%)]\tLoss: 1.481246\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51700/60000 (86%)]\tLoss: 1.481313\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51750/60000 (86%)]\tLoss: 1.497640\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51800/60000 (86%)]\tLoss: 1.464402\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51850/60000 (86%)]\tLoss: 1.478972\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51900/60000 (86%)]\tLoss: 1.503715\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [51950/60000 (87%)]\tLoss: 1.495342\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52000/60000 (87%)]\tLoss: 1.500980\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52050/60000 (87%)]\tLoss: 1.461182\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52100/60000 (87%)]\tLoss: 1.481302\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52150/60000 (87%)]\tLoss: 1.481153\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52200/60000 (87%)]\tLoss: 1.462994\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52250/60000 (87%)]\tLoss: 1.481149\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52300/60000 (87%)]\tLoss: 1.475062\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52350/60000 (87%)]\tLoss: 1.478863\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52400/60000 (87%)]\tLoss: 1.499226\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52450/60000 (87%)]\tLoss: 1.481156\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52500/60000 (88%)]\tLoss: 1.512015\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52550/60000 (88%)]\tLoss: 1.481514\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52600/60000 (88%)]\tLoss: 1.461970\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52650/60000 (88%)]\tLoss: 1.472810\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52700/60000 (88%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52750/60000 (88%)]\tLoss: 1.481187\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52800/60000 (88%)]\tLoss: 1.465614\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52850/60000 (88%)]\tLoss: 1.481805\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52900/60000 (88%)]\tLoss: 1.462469\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [52950/60000 (88%)]\tLoss: 1.467684\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53000/60000 (88%)]\tLoss: 1.471935\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53050/60000 (88%)]\tLoss: 1.503069\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53100/60000 (88%)]\tLoss: 1.492298\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53150/60000 (89%)]\tLoss: 1.501253\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53200/60000 (89%)]\tLoss: 1.501217\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53250/60000 (89%)]\tLoss: 1.461152\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53300/60000 (89%)]\tLoss: 1.461611\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53350/60000 (89%)]\tLoss: 1.491158\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53400/60000 (89%)]\tLoss: 1.461156\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53450/60000 (89%)]\tLoss: 1.501576\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53500/60000 (89%)]\tLoss: 1.501416\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53550/60000 (89%)]\tLoss: 1.490531\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53600/60000 (89%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53650/60000 (89%)]\tLoss: 1.461169\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53700/60000 (90%)]\tLoss: 1.482622\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53750/60000 (90%)]\tLoss: 1.519835\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53800/60000 (90%)]\tLoss: 1.488897\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53850/60000 (90%)]\tLoss: 1.461160\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53900/60000 (90%)]\tLoss: 1.461162\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [53950/60000 (90%)]\tLoss: 1.461214\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54000/60000 (90%)]\tLoss: 1.466604\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54050/60000 (90%)]\tLoss: 1.521313\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54100/60000 (90%)]\tLoss: 1.461363\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54150/60000 (90%)]\tLoss: 1.481152\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54200/60000 (90%)]\tLoss: 1.463627\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54250/60000 (90%)]\tLoss: 1.461451\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54300/60000 (90%)]\tLoss: 1.461209\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54350/60000 (91%)]\tLoss: 1.461312\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54400/60000 (91%)]\tLoss: 1.481521\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54450/60000 (91%)]\tLoss: 1.464913\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54500/60000 (91%)]\tLoss: 1.466701\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54550/60000 (91%)]\tLoss: 1.481633\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54600/60000 (91%)]\tLoss: 1.500052\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54650/60000 (91%)]\tLoss: 1.461151\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54700/60000 (91%)]\tLoss: 1.502408\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54750/60000 (91%)]\tLoss: 1.471377\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54800/60000 (91%)]\tLoss: 1.502383\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54850/60000 (91%)]\tLoss: 1.501099\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54900/60000 (92%)]\tLoss: 1.490589\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [54950/60000 (92%)]\tLoss: 1.528412\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55000/60000 (92%)]\tLoss: 1.494231\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55050/60000 (92%)]\tLoss: 1.481972\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55100/60000 (92%)]\tLoss: 1.478440\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55150/60000 (92%)]\tLoss: 1.476273\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55200/60000 (92%)]\tLoss: 1.480521\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55250/60000 (92%)]\tLoss: 1.478350\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55300/60000 (92%)]\tLoss: 1.461268\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55350/60000 (92%)]\tLoss: 1.463140\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55400/60000 (92%)]\tLoss: 1.482369\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55450/60000 (92%)]\tLoss: 1.499619\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55500/60000 (92%)]\tLoss: 1.480121\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55550/60000 (93%)]\tLoss: 1.469646\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55600/60000 (93%)]\tLoss: 1.501148\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55650/60000 (93%)]\tLoss: 1.483430\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55700/60000 (93%)]\tLoss: 1.461783\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55750/60000 (93%)]\tLoss: 1.531931\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55800/60000 (93%)]\tLoss: 1.500949\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55850/60000 (93%)]\tLoss: 1.483760\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55900/60000 (93%)]\tLoss: 1.461159\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [55950/60000 (93%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56000/60000 (93%)]\tLoss: 1.480898\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56050/60000 (93%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56100/60000 (94%)]\tLoss: 1.461151\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56150/60000 (94%)]\tLoss: 1.481045\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56200/60000 (94%)]\tLoss: 1.477428\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56250/60000 (94%)]\tLoss: 1.501122\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56300/60000 (94%)]\tLoss: 1.461159\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56350/60000 (94%)]\tLoss: 1.502951\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56400/60000 (94%)]\tLoss: 1.520329\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56450/60000 (94%)]\tLoss: 1.461155\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56500/60000 (94%)]\tLoss: 1.481158\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56550/60000 (94%)]\tLoss: 1.461153\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56600/60000 (94%)]\tLoss: 1.465235\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56650/60000 (94%)]\tLoss: 1.480967\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56700/60000 (94%)]\tLoss: 1.501164\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56750/60000 (95%)]\tLoss: 1.476534\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56800/60000 (95%)]\tLoss: 1.493061\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56850/60000 (95%)]\tLoss: 1.495045\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56900/60000 (95%)]\tLoss: 1.522621\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [56950/60000 (95%)]\tLoss: 1.514910\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57000/60000 (95%)]\tLoss: 1.496237\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57050/60000 (95%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57100/60000 (95%)]\tLoss: 1.481147\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57150/60000 (95%)]\tLoss: 1.479913\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57200/60000 (95%)]\tLoss: 1.481048\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57250/60000 (95%)]\tLoss: 1.466720\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57300/60000 (96%)]\tLoss: 1.477500\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57350/60000 (96%)]\tLoss: 1.513145\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57400/60000 (96%)]\tLoss: 1.464042\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57450/60000 (96%)]\tLoss: 1.523873\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57500/60000 (96%)]\tLoss: 1.461632\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57550/60000 (96%)]\tLoss: 1.484203\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57600/60000 (96%)]\tLoss: 1.501048\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57650/60000 (96%)]\tLoss: 1.481119\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57700/60000 (96%)]\tLoss: 1.481176\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57750/60000 (96%)]\tLoss: 1.481299\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57800/60000 (96%)]\tLoss: 1.481929\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57850/60000 (96%)]\tLoss: 1.478391\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57900/60000 (96%)]\tLoss: 1.482179\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [57950/60000 (97%)]\tLoss: 1.526065\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58000/60000 (97%)]\tLoss: 1.461224\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58050/60000 (97%)]\tLoss: 1.461451\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58100/60000 (97%)]\tLoss: 1.521440\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58150/60000 (97%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58200/60000 (97%)]\tLoss: 1.461153\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58250/60000 (97%)]\tLoss: 1.461151\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58300/60000 (97%)]\tLoss: 1.482660\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58350/60000 (97%)]\tLoss: 1.461152\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58400/60000 (97%)]\tLoss: 1.481071\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58450/60000 (97%)]\tLoss: 1.474212\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58500/60000 (98%)]\tLoss: 1.481172\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58550/60000 (98%)]\tLoss: 1.461290\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58600/60000 (98%)]\tLoss: 1.470174\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58650/60000 (98%)]\tLoss: 1.493555\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58700/60000 (98%)]\tLoss: 1.479480\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58750/60000 (98%)]\tLoss: 1.463153\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58800/60000 (98%)]\tLoss: 1.519941\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58850/60000 (98%)]\tLoss: 1.461255\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58900/60000 (98%)]\tLoss: 1.461563\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [58950/60000 (98%)]\tLoss: 1.462439\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59000/60000 (98%)]\tLoss: 1.516150\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59050/60000 (98%)]\tLoss: 1.461162\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59100/60000 (98%)]\tLoss: 1.461186\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59150/60000 (99%)]\tLoss: 1.493667\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59200/60000 (99%)]\tLoss: 1.461226\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59250/60000 (99%)]\tLoss: 1.477910\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59300/60000 (99%)]\tLoss: 1.519904\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59350/60000 (99%)]\tLoss: 1.481156\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59400/60000 (99%)]\tLoss: 1.483248\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59450/60000 (99%)]\tLoss: 1.461295\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59500/60000 (99%)]\tLoss: 1.461242\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59550/60000 (99%)]\tLoss: 1.481570\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59600/60000 (99%)]\tLoss: 1.461160\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59650/60000 (99%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59700/60000 (100%)]\tLoss: 1.481130\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59750/60000 (100%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59800/60000 (100%)]\tLoss: 1.461671\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59850/60000 (100%)]\tLoss: 1.461155\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59900/60000 (100%)]\tLoss: 1.461151\tAccuracy: 98.08%\tGradient Norm: 0.000000\n",
      "Epoch: 2 [59950/60000 (100%)]\tLoss: 1.494627\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [0/60000 (0%)]\tLoss: 1.461153\tAccuracy: 100.00%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50/60000 (0%)]\tLoss: 1.468373\tAccuracy: 100.00%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [100/60000 (0%)]\tLoss: 1.489695\tAccuracy: 98.67%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [150/60000 (0%)]\tLoss: 1.461334\tAccuracy: 99.00%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [200/60000 (0%)]\tLoss: 1.469405\tAccuracy: 99.20%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [250/60000 (0%)]\tLoss: 1.466482\tAccuracy: 99.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [300/60000 (0%)]\tLoss: 1.461281\tAccuracy: 99.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [350/60000 (1%)]\tLoss: 1.475684\tAccuracy: 99.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [400/60000 (1%)]\tLoss: 1.461151\tAccuracy: 99.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [450/60000 (1%)]\tLoss: 1.461248\tAccuracy: 99.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [500/60000 (1%)]\tLoss: 1.480680\tAccuracy: 99.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [550/60000 (1%)]\tLoss: 1.480133\tAccuracy: 99.17%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [600/60000 (1%)]\tLoss: 1.473692\tAccuracy: 99.08%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [650/60000 (1%)]\tLoss: 1.477425\tAccuracy: 99.00%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [700/60000 (1%)]\tLoss: 1.482516\tAccuracy: 98.93%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [750/60000 (1%)]\tLoss: 1.461215\tAccuracy: 99.00%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [800/60000 (1%)]\tLoss: 1.463246\tAccuracy: 99.06%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [850/60000 (1%)]\tLoss: 1.461157\tAccuracy: 99.11%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [900/60000 (2%)]\tLoss: 1.514387\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [950/60000 (2%)]\tLoss: 1.477476\tAccuracy: 98.80%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1000/60000 (2%)]\tLoss: 1.463476\tAccuracy: 98.86%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1050/60000 (2%)]\tLoss: 1.500667\tAccuracy: 98.73%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1100/60000 (2%)]\tLoss: 1.461157\tAccuracy: 98.78%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1150/60000 (2%)]\tLoss: 1.480075\tAccuracy: 98.75%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1200/60000 (2%)]\tLoss: 1.500039\tAccuracy: 98.64%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1250/60000 (2%)]\tLoss: 1.461152\tAccuracy: 98.69%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1300/60000 (2%)]\tLoss: 1.463199\tAccuracy: 98.74%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1350/60000 (2%)]\tLoss: 1.461498\tAccuracy: 98.79%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1400/60000 (2%)]\tLoss: 1.461171\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1450/60000 (2%)]\tLoss: 1.461257\tAccuracy: 98.87%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1500/60000 (2%)]\tLoss: 1.481211\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1550/60000 (3%)]\tLoss: 1.481781\tAccuracy: 98.81%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1600/60000 (3%)]\tLoss: 1.485926\tAccuracy: 98.79%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1650/60000 (3%)]\tLoss: 1.461195\tAccuracy: 98.82%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1700/60000 (3%)]\tLoss: 1.481156\tAccuracy: 98.80%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1750/60000 (3%)]\tLoss: 1.461705\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1800/60000 (3%)]\tLoss: 1.481392\tAccuracy: 98.81%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1850/60000 (3%)]\tLoss: 1.481195\tAccuracy: 98.79%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1900/60000 (3%)]\tLoss: 1.461151\tAccuracy: 98.82%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [1950/60000 (3%)]\tLoss: 1.491190\tAccuracy: 98.75%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2000/60000 (3%)]\tLoss: 1.481153\tAccuracy: 98.73%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2050/60000 (3%)]\tLoss: 1.461195\tAccuracy: 98.76%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2100/60000 (4%)]\tLoss: 1.467841\tAccuracy: 98.79%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2150/60000 (4%)]\tLoss: 1.461166\tAccuracy: 98.82%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2200/60000 (4%)]\tLoss: 1.461627\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2250/60000 (4%)]\tLoss: 1.461151\tAccuracy: 98.87%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2300/60000 (4%)]\tLoss: 1.467907\tAccuracy: 98.89%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2350/60000 (4%)]\tLoss: 1.461157\tAccuracy: 98.92%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2400/60000 (4%)]\tLoss: 1.461529\tAccuracy: 98.94%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2450/60000 (4%)]\tLoss: 1.481148\tAccuracy: 98.92%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2500/60000 (4%)]\tLoss: 1.461151\tAccuracy: 98.94%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2550/60000 (4%)]\tLoss: 1.488658\tAccuracy: 98.92%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2600/60000 (4%)]\tLoss: 1.481192\tAccuracy: 98.91%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2650/60000 (4%)]\tLoss: 1.467688\tAccuracy: 98.93%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2700/60000 (4%)]\tLoss: 1.461151\tAccuracy: 98.95%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2750/60000 (5%)]\tLoss: 1.516029\tAccuracy: 98.86%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2800/60000 (5%)]\tLoss: 1.481553\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2850/60000 (5%)]\tLoss: 1.482829\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2900/60000 (5%)]\tLoss: 1.461162\tAccuracy: 98.85%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [2950/60000 (5%)]\tLoss: 1.461227\tAccuracy: 98.87%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3000/60000 (5%)]\tLoss: 1.462045\tAccuracy: 98.89%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3050/60000 (5%)]\tLoss: 1.481378\tAccuracy: 98.87%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3100/60000 (5%)]\tLoss: 1.493657\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3150/60000 (5%)]\tLoss: 1.463791\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3200/60000 (5%)]\tLoss: 1.477251\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3250/60000 (5%)]\tLoss: 1.481143\tAccuracy: 98.82%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3300/60000 (6%)]\tLoss: 1.461151\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3350/60000 (6%)]\tLoss: 1.481151\tAccuracy: 98.82%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3400/60000 (6%)]\tLoss: 1.475110\tAccuracy: 98.81%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3450/60000 (6%)]\tLoss: 1.461151\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3500/60000 (6%)]\tLoss: 1.461294\tAccuracy: 98.85%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3550/60000 (6%)]\tLoss: 1.480634\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3600/60000 (6%)]\tLoss: 1.471304\tAccuracy: 98.85%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3650/60000 (6%)]\tLoss: 1.500748\tAccuracy: 98.81%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3700/60000 (6%)]\tLoss: 1.461243\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3750/60000 (6%)]\tLoss: 1.463620\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3800/60000 (6%)]\tLoss: 1.481168\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3850/60000 (6%)]\tLoss: 1.481151\tAccuracy: 98.82%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3900/60000 (6%)]\tLoss: 1.489384\tAccuracy: 98.81%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [3950/60000 (7%)]\tLoss: 1.461151\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4000/60000 (7%)]\tLoss: 1.469820\tAccuracy: 98.84%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4050/60000 (7%)]\tLoss: 1.481706\tAccuracy: 98.83%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4100/60000 (7%)]\tLoss: 1.498620\tAccuracy: 98.80%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4150/60000 (7%)]\tLoss: 1.481042\tAccuracy: 98.79%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4200/60000 (7%)]\tLoss: 1.504310\tAccuracy: 98.75%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4250/60000 (7%)]\tLoss: 1.469544\tAccuracy: 98.77%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4300/60000 (7%)]\tLoss: 1.501437\tAccuracy: 98.74%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4350/60000 (7%)]\tLoss: 1.504110\tAccuracy: 98.70%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4400/60000 (7%)]\tLoss: 1.499997\tAccuracy: 98.67%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4450/60000 (7%)]\tLoss: 1.513864\tAccuracy: 98.62%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4500/60000 (8%)]\tLoss: 1.465897\tAccuracy: 98.64%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4550/60000 (8%)]\tLoss: 1.461153\tAccuracy: 98.65%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4600/60000 (8%)]\tLoss: 1.479772\tAccuracy: 98.65%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4650/60000 (8%)]\tLoss: 1.485471\tAccuracy: 98.64%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4700/60000 (8%)]\tLoss: 1.482294\tAccuracy: 98.63%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4750/60000 (8%)]\tLoss: 1.463024\tAccuracy: 98.65%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4800/60000 (8%)]\tLoss: 1.466032\tAccuracy: 98.66%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4850/60000 (8%)]\tLoss: 1.461151\tAccuracy: 98.67%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4900/60000 (8%)]\tLoss: 1.509346\tAccuracy: 98.63%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [4950/60000 (8%)]\tLoss: 1.464266\tAccuracy: 98.64%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5000/60000 (8%)]\tLoss: 1.506365\tAccuracy: 98.61%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5050/60000 (8%)]\tLoss: 1.481284\tAccuracy: 98.61%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5100/60000 (8%)]\tLoss: 1.481151\tAccuracy: 98.60%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5150/60000 (9%)]\tLoss: 1.487852\tAccuracy: 98.60%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5200/60000 (9%)]\tLoss: 1.500094\tAccuracy: 98.57%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5250/60000 (9%)]\tLoss: 1.464926\tAccuracy: 98.58%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5300/60000 (9%)]\tLoss: 1.461158\tAccuracy: 98.60%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5350/60000 (9%)]\tLoss: 1.506101\tAccuracy: 98.57%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5400/60000 (9%)]\tLoss: 1.479216\tAccuracy: 98.57%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5450/60000 (9%)]\tLoss: 1.461174\tAccuracy: 98.58%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5500/60000 (9%)]\tLoss: 1.467990\tAccuracy: 98.59%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5550/60000 (9%)]\tLoss: 1.480407\tAccuracy: 98.59%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5600/60000 (9%)]\tLoss: 1.521093\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5650/60000 (9%)]\tLoss: 1.461151\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5700/60000 (10%)]\tLoss: 1.481018\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5750/60000 (10%)]\tLoss: 1.520790\tAccuracy: 98.52%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5800/60000 (10%)]\tLoss: 1.483457\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5850/60000 (10%)]\tLoss: 1.470588\tAccuracy: 98.53%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5900/60000 (10%)]\tLoss: 1.480970\tAccuracy: 98.52%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [5950/60000 (10%)]\tLoss: 1.461151\tAccuracy: 98.53%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6000/60000 (10%)]\tLoss: 1.461171\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6050/60000 (10%)]\tLoss: 1.461151\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6100/60000 (10%)]\tLoss: 1.461262\tAccuracy: 98.57%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6150/60000 (10%)]\tLoss: 1.471917\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6200/60000 (10%)]\tLoss: 1.461313\tAccuracy: 98.58%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6250/60000 (10%)]\tLoss: 1.461392\tAccuracy: 98.59%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6300/60000 (10%)]\tLoss: 1.461184\tAccuracy: 98.60%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6350/60000 (11%)]\tLoss: 1.461153\tAccuracy: 98.61%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6400/60000 (11%)]\tLoss: 1.461494\tAccuracy: 98.62%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6450/60000 (11%)]\tLoss: 1.522540\tAccuracy: 98.58%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6500/60000 (11%)]\tLoss: 1.461151\tAccuracy: 98.60%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6550/60000 (11%)]\tLoss: 1.533100\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6600/60000 (11%)]\tLoss: 1.466808\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6650/60000 (11%)]\tLoss: 1.481144\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6700/60000 (11%)]\tLoss: 1.479765\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6750/60000 (11%)]\tLoss: 1.482314\tAccuracy: 98.54%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6800/60000 (11%)]\tLoss: 1.461157\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6850/60000 (11%)]\tLoss: 1.462750\tAccuracy: 98.57%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6900/60000 (12%)]\tLoss: 1.461152\tAccuracy: 98.58%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [6950/60000 (12%)]\tLoss: 1.481160\tAccuracy: 98.57%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7000/60000 (12%)]\tLoss: 1.469829\tAccuracy: 98.58%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7050/60000 (12%)]\tLoss: 1.503480\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7100/60000 (12%)]\tLoss: 1.500947\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7150/60000 (12%)]\tLoss: 1.481169\tAccuracy: 98.54%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7200/60000 (12%)]\tLoss: 1.482492\tAccuracy: 98.54%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7250/60000 (12%)]\tLoss: 1.462634\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7300/60000 (12%)]\tLoss: 1.461184\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7350/60000 (12%)]\tLoss: 1.522574\tAccuracy: 98.53%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7400/60000 (12%)]\tLoss: 1.471224\tAccuracy: 98.52%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7450/60000 (12%)]\tLoss: 1.462109\tAccuracy: 98.53%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7500/60000 (12%)]\tLoss: 1.461528\tAccuracy: 98.54%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7550/60000 (13%)]\tLoss: 1.461187\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7600/60000 (13%)]\tLoss: 1.469050\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7650/60000 (13%)]\tLoss: 1.481177\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7700/60000 (13%)]\tLoss: 1.481182\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7750/60000 (13%)]\tLoss: 1.480930\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7800/60000 (13%)]\tLoss: 1.477850\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7850/60000 (13%)]\tLoss: 1.461163\tAccuracy: 98.56%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7900/60000 (13%)]\tLoss: 1.481558\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [7950/60000 (13%)]\tLoss: 1.481187\tAccuracy: 98.55%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8000/60000 (13%)]\tLoss: 1.523465\tAccuracy: 98.52%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8050/60000 (13%)]\tLoss: 1.500778\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8100/60000 (14%)]\tLoss: 1.481178\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8150/60000 (14%)]\tLoss: 1.461197\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8200/60000 (14%)]\tLoss: 1.505361\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8250/60000 (14%)]\tLoss: 1.481192\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8300/60000 (14%)]\tLoss: 1.465175\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8350/60000 (14%)]\tLoss: 1.526254\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8400/60000 (14%)]\tLoss: 1.461207\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8450/60000 (14%)]\tLoss: 1.470518\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8500/60000 (14%)]\tLoss: 1.461947\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8550/60000 (14%)]\tLoss: 1.461192\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8600/60000 (14%)]\tLoss: 1.461216\tAccuracy: 98.52%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8650/60000 (14%)]\tLoss: 1.461621\tAccuracy: 98.53%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8700/60000 (14%)]\tLoss: 1.481727\tAccuracy: 98.53%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8750/60000 (15%)]\tLoss: 1.474960\tAccuracy: 98.53%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8800/60000 (15%)]\tLoss: 1.510420\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8850/60000 (15%)]\tLoss: 1.554617\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8900/60000 (15%)]\tLoss: 1.481119\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [8950/60000 (15%)]\tLoss: 1.461159\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9000/60000 (15%)]\tLoss: 1.461151\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9050/60000 (15%)]\tLoss: 1.499813\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9100/60000 (15%)]\tLoss: 1.462489\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9150/60000 (15%)]\tLoss: 1.461155\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9200/60000 (15%)]\tLoss: 1.500714\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9250/60000 (15%)]\tLoss: 1.462691\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9300/60000 (16%)]\tLoss: 1.479568\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9350/60000 (16%)]\tLoss: 1.540640\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9400/60000 (16%)]\tLoss: 1.461152\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9450/60000 (16%)]\tLoss: 1.531365\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9500/60000 (16%)]\tLoss: 1.520174\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9550/60000 (16%)]\tLoss: 1.536277\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9600/60000 (16%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9650/60000 (16%)]\tLoss: 1.461152\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9700/60000 (16%)]\tLoss: 1.481146\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9750/60000 (16%)]\tLoss: 1.500368\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9800/60000 (16%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9850/60000 (16%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9900/60000 (16%)]\tLoss: 1.500642\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [9950/60000 (17%)]\tLoss: 1.482028\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10000/60000 (17%)]\tLoss: 1.480797\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10050/60000 (17%)]\tLoss: 1.468560\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10100/60000 (17%)]\tLoss: 1.484052\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10150/60000 (17%)]\tLoss: 1.501120\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10200/60000 (17%)]\tLoss: 1.481257\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10250/60000 (17%)]\tLoss: 1.461271\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10300/60000 (17%)]\tLoss: 1.461872\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10350/60000 (17%)]\tLoss: 1.481151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10400/60000 (17%)]\tLoss: 1.461232\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10450/60000 (17%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10500/60000 (18%)]\tLoss: 1.461164\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10550/60000 (18%)]\tLoss: 1.481162\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10600/60000 (18%)]\tLoss: 1.515387\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10650/60000 (18%)]\tLoss: 1.470713\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10700/60000 (18%)]\tLoss: 1.480541\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10750/60000 (18%)]\tLoss: 1.461163\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10800/60000 (18%)]\tLoss: 1.461233\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10850/60000 (18%)]\tLoss: 1.481174\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10900/60000 (18%)]\tLoss: 1.480752\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [10950/60000 (18%)]\tLoss: 1.475675\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11000/60000 (18%)]\tLoss: 1.498589\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11050/60000 (18%)]\tLoss: 1.480137\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11100/60000 (18%)]\tLoss: 1.484414\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11150/60000 (19%)]\tLoss: 1.519814\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11200/60000 (19%)]\tLoss: 1.497470\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11250/60000 (19%)]\tLoss: 1.464325\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11300/60000 (19%)]\tLoss: 1.487800\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11350/60000 (19%)]\tLoss: 1.474076\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11400/60000 (19%)]\tLoss: 1.501100\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11450/60000 (19%)]\tLoss: 1.480346\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11500/60000 (19%)]\tLoss: 1.461155\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11550/60000 (19%)]\tLoss: 1.480575\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11600/60000 (19%)]\tLoss: 1.463779\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11650/60000 (19%)]\tLoss: 1.489802\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11700/60000 (20%)]\tLoss: 1.480433\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11750/60000 (20%)]\tLoss: 1.461205\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11800/60000 (20%)]\tLoss: 1.465260\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11850/60000 (20%)]\tLoss: 1.481048\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11900/60000 (20%)]\tLoss: 1.479332\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [11950/60000 (20%)]\tLoss: 1.483231\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12000/60000 (20%)]\tLoss: 1.462418\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12050/60000 (20%)]\tLoss: 1.480909\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12100/60000 (20%)]\tLoss: 1.464012\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12150/60000 (20%)]\tLoss: 1.462980\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12200/60000 (20%)]\tLoss: 1.493978\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12250/60000 (20%)]\tLoss: 1.488642\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12300/60000 (20%)]\tLoss: 1.501151\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12350/60000 (21%)]\tLoss: 1.461173\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12400/60000 (21%)]\tLoss: 1.461153\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12450/60000 (21%)]\tLoss: 1.482361\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12500/60000 (21%)]\tLoss: 1.461260\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12550/60000 (21%)]\tLoss: 1.500574\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12600/60000 (21%)]\tLoss: 1.498667\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12650/60000 (21%)]\tLoss: 1.463405\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12700/60000 (21%)]\tLoss: 1.482761\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12750/60000 (21%)]\tLoss: 1.482685\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12800/60000 (21%)]\tLoss: 1.462338\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12850/60000 (21%)]\tLoss: 1.482597\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12900/60000 (22%)]\tLoss: 1.481794\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [12950/60000 (22%)]\tLoss: 1.461497\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13000/60000 (22%)]\tLoss: 1.463617\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13050/60000 (22%)]\tLoss: 1.495387\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13100/60000 (22%)]\tLoss: 1.461152\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13150/60000 (22%)]\tLoss: 1.461161\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13200/60000 (22%)]\tLoss: 1.481826\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13250/60000 (22%)]\tLoss: 1.461268\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13300/60000 (22%)]\tLoss: 1.461531\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13350/60000 (22%)]\tLoss: 1.501400\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13400/60000 (22%)]\tLoss: 1.461157\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13450/60000 (22%)]\tLoss: 1.481387\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13500/60000 (22%)]\tLoss: 1.461242\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13550/60000 (23%)]\tLoss: 1.498579\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13600/60000 (23%)]\tLoss: 1.461152\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13650/60000 (23%)]\tLoss: 1.500830\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13700/60000 (23%)]\tLoss: 1.481150\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13750/60000 (23%)]\tLoss: 1.503921\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13800/60000 (23%)]\tLoss: 1.461177\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13850/60000 (23%)]\tLoss: 1.485885\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13900/60000 (23%)]\tLoss: 1.490605\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [13950/60000 (23%)]\tLoss: 1.462510\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14000/60000 (23%)]\tLoss: 1.462077\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14050/60000 (23%)]\tLoss: 1.481163\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14100/60000 (24%)]\tLoss: 1.461154\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14150/60000 (24%)]\tLoss: 1.481193\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14200/60000 (24%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14250/60000 (24%)]\tLoss: 1.461151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14300/60000 (24%)]\tLoss: 1.499598\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14350/60000 (24%)]\tLoss: 1.465576\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14400/60000 (24%)]\tLoss: 1.481165\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14450/60000 (24%)]\tLoss: 1.461610\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14500/60000 (24%)]\tLoss: 1.491938\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14550/60000 (24%)]\tLoss: 1.481167\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14600/60000 (24%)]\tLoss: 1.461304\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14650/60000 (24%)]\tLoss: 1.461915\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14700/60000 (24%)]\tLoss: 1.461564\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14750/60000 (25%)]\tLoss: 1.461195\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14800/60000 (25%)]\tLoss: 1.461151\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14850/60000 (25%)]\tLoss: 1.480931\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14900/60000 (25%)]\tLoss: 1.480483\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [14950/60000 (25%)]\tLoss: 1.500283\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15000/60000 (25%)]\tLoss: 1.466525\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15050/60000 (25%)]\tLoss: 1.461516\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15100/60000 (25%)]\tLoss: 1.500585\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15150/60000 (25%)]\tLoss: 1.461154\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15200/60000 (25%)]\tLoss: 1.481152\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15250/60000 (25%)]\tLoss: 1.461237\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15300/60000 (26%)]\tLoss: 1.462320\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15350/60000 (26%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15400/60000 (26%)]\tLoss: 1.472397\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15450/60000 (26%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15500/60000 (26%)]\tLoss: 1.461174\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15550/60000 (26%)]\tLoss: 1.461208\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15600/60000 (26%)]\tLoss: 1.494987\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15650/60000 (26%)]\tLoss: 1.461785\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15700/60000 (26%)]\tLoss: 1.461154\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15750/60000 (26%)]\tLoss: 1.461151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15800/60000 (26%)]\tLoss: 1.480333\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15850/60000 (26%)]\tLoss: 1.461262\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15900/60000 (26%)]\tLoss: 1.500870\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [15950/60000 (27%)]\tLoss: 1.478135\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16000/60000 (27%)]\tLoss: 1.481109\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16050/60000 (27%)]\tLoss: 1.480935\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16100/60000 (27%)]\tLoss: 1.464130\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16150/60000 (27%)]\tLoss: 1.476058\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16200/60000 (27%)]\tLoss: 1.481177\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16250/60000 (27%)]\tLoss: 1.469688\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16300/60000 (27%)]\tLoss: 1.501091\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16350/60000 (27%)]\tLoss: 1.461151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16400/60000 (27%)]\tLoss: 1.464949\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16450/60000 (27%)]\tLoss: 1.461232\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16500/60000 (28%)]\tLoss: 1.479592\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16550/60000 (28%)]\tLoss: 1.484237\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16600/60000 (28%)]\tLoss: 1.480662\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16650/60000 (28%)]\tLoss: 1.481446\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16700/60000 (28%)]\tLoss: 1.482159\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16750/60000 (28%)]\tLoss: 1.481151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16800/60000 (28%)]\tLoss: 1.486547\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16850/60000 (28%)]\tLoss: 1.461151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16900/60000 (28%)]\tLoss: 1.461151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [16950/60000 (28%)]\tLoss: 1.481210\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17000/60000 (28%)]\tLoss: 1.463476\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17050/60000 (28%)]\tLoss: 1.481179\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17100/60000 (28%)]\tLoss: 1.480420\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17150/60000 (29%)]\tLoss: 1.461722\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17200/60000 (29%)]\tLoss: 1.515328\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17250/60000 (29%)]\tLoss: 1.461156\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17300/60000 (29%)]\tLoss: 1.480020\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17350/60000 (29%)]\tLoss: 1.466376\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17400/60000 (29%)]\tLoss: 1.461156\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17450/60000 (29%)]\tLoss: 1.461210\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17500/60000 (29%)]\tLoss: 1.461171\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17550/60000 (29%)]\tLoss: 1.483033\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17600/60000 (29%)]\tLoss: 1.461157\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17650/60000 (29%)]\tLoss: 1.469737\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17700/60000 (30%)]\tLoss: 1.501285\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17750/60000 (30%)]\tLoss: 1.494728\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17800/60000 (30%)]\tLoss: 1.501168\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17850/60000 (30%)]\tLoss: 1.461702\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17900/60000 (30%)]\tLoss: 1.461151\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [17950/60000 (30%)]\tLoss: 1.461155\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18000/60000 (30%)]\tLoss: 1.473110\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18050/60000 (30%)]\tLoss: 1.499940\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18100/60000 (30%)]\tLoss: 1.461151\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18150/60000 (30%)]\tLoss: 1.467367\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18200/60000 (30%)]\tLoss: 1.462627\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18250/60000 (30%)]\tLoss: 1.461551\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18300/60000 (30%)]\tLoss: 1.461152\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18350/60000 (31%)]\tLoss: 1.501089\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18400/60000 (31%)]\tLoss: 1.462807\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18450/60000 (31%)]\tLoss: 1.461163\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18500/60000 (31%)]\tLoss: 1.461153\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18550/60000 (31%)]\tLoss: 1.499686\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18600/60000 (31%)]\tLoss: 1.461155\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18650/60000 (31%)]\tLoss: 1.461249\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18700/60000 (31%)]\tLoss: 1.461988\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18750/60000 (31%)]\tLoss: 1.481316\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18800/60000 (31%)]\tLoss: 1.461455\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18850/60000 (31%)]\tLoss: 1.480167\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18900/60000 (32%)]\tLoss: 1.461159\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [18950/60000 (32%)]\tLoss: 1.485394\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19000/60000 (32%)]\tLoss: 1.500412\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19050/60000 (32%)]\tLoss: 1.522411\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19100/60000 (32%)]\tLoss: 1.481161\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19150/60000 (32%)]\tLoss: 1.463648\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19200/60000 (32%)]\tLoss: 1.461174\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19250/60000 (32%)]\tLoss: 1.518730\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19300/60000 (32%)]\tLoss: 1.497196\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19350/60000 (32%)]\tLoss: 1.485886\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19400/60000 (32%)]\tLoss: 1.502289\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19450/60000 (32%)]\tLoss: 1.481541\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19500/60000 (32%)]\tLoss: 1.463866\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19550/60000 (33%)]\tLoss: 1.507382\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19600/60000 (33%)]\tLoss: 1.461406\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19650/60000 (33%)]\tLoss: 1.463975\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19700/60000 (33%)]\tLoss: 1.480308\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19750/60000 (33%)]\tLoss: 1.463158\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19800/60000 (33%)]\tLoss: 1.500469\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19850/60000 (33%)]\tLoss: 1.461214\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19900/60000 (33%)]\tLoss: 1.478513\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [19950/60000 (33%)]\tLoss: 1.469436\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20000/60000 (33%)]\tLoss: 1.503927\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20050/60000 (33%)]\tLoss: 1.461151\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20100/60000 (34%)]\tLoss: 1.481064\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20150/60000 (34%)]\tLoss: 1.461156\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20200/60000 (34%)]\tLoss: 1.519138\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20250/60000 (34%)]\tLoss: 1.461168\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20300/60000 (34%)]\tLoss: 1.481171\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20350/60000 (34%)]\tLoss: 1.461168\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20400/60000 (34%)]\tLoss: 1.461295\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20450/60000 (34%)]\tLoss: 1.500972\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20500/60000 (34%)]\tLoss: 1.477495\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20550/60000 (34%)]\tLoss: 1.500458\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20600/60000 (34%)]\tLoss: 1.461154\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20650/60000 (34%)]\tLoss: 1.495919\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20700/60000 (34%)]\tLoss: 1.471702\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20750/60000 (35%)]\tLoss: 1.461187\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20800/60000 (35%)]\tLoss: 1.461238\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20850/60000 (35%)]\tLoss: 1.461171\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20900/60000 (35%)]\tLoss: 1.466679\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [20950/60000 (35%)]\tLoss: 1.461152\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21000/60000 (35%)]\tLoss: 1.461155\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21050/60000 (35%)]\tLoss: 1.501802\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21100/60000 (35%)]\tLoss: 1.461158\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21150/60000 (35%)]\tLoss: 1.481221\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21200/60000 (35%)]\tLoss: 1.461887\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21250/60000 (35%)]\tLoss: 1.461613\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21300/60000 (36%)]\tLoss: 1.487155\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21350/60000 (36%)]\tLoss: 1.461167\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21400/60000 (36%)]\tLoss: 1.461286\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21450/60000 (36%)]\tLoss: 1.480946\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21500/60000 (36%)]\tLoss: 1.486050\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21550/60000 (36%)]\tLoss: 1.481515\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21600/60000 (36%)]\tLoss: 1.461784\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21650/60000 (36%)]\tLoss: 1.468269\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21700/60000 (36%)]\tLoss: 1.461186\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21750/60000 (36%)]\tLoss: 1.461151\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21800/60000 (36%)]\tLoss: 1.478868\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21850/60000 (36%)]\tLoss: 1.461818\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21900/60000 (36%)]\tLoss: 1.481732\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [21950/60000 (37%)]\tLoss: 1.481167\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22000/60000 (37%)]\tLoss: 1.461154\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22050/60000 (37%)]\tLoss: 1.480970\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22100/60000 (37%)]\tLoss: 1.461151\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22150/60000 (37%)]\tLoss: 1.480875\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22200/60000 (37%)]\tLoss: 1.461151\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22250/60000 (37%)]\tLoss: 1.481151\tAccuracy: 98.51%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22300/60000 (37%)]\tLoss: 1.484238\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22350/60000 (37%)]\tLoss: 1.462016\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22400/60000 (37%)]\tLoss: 1.485466\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22450/60000 (37%)]\tLoss: 1.496214\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22500/60000 (38%)]\tLoss: 1.461273\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22550/60000 (38%)]\tLoss: 1.461253\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22600/60000 (38%)]\tLoss: 1.481036\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22650/60000 (38%)]\tLoss: 1.480500\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22700/60000 (38%)]\tLoss: 1.488244\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22750/60000 (38%)]\tLoss: 1.461152\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22800/60000 (38%)]\tLoss: 1.494250\tAccuracy: 98.50%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22850/60000 (38%)]\tLoss: 1.496025\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22900/60000 (38%)]\tLoss: 1.472117\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [22950/60000 (38%)]\tLoss: 1.478415\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23000/60000 (38%)]\tLoss: 1.481152\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23050/60000 (38%)]\tLoss: 1.481340\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23100/60000 (38%)]\tLoss: 1.480733\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23150/60000 (39%)]\tLoss: 1.461182\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23200/60000 (39%)]\tLoss: 1.500722\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23250/60000 (39%)]\tLoss: 1.481149\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23300/60000 (39%)]\tLoss: 1.481198\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23350/60000 (39%)]\tLoss: 1.461151\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23400/60000 (39%)]\tLoss: 1.480307\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23450/60000 (39%)]\tLoss: 1.461156\tAccuracy: 98.49%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23500/60000 (39%)]\tLoss: 1.527589\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23550/60000 (39%)]\tLoss: 1.515933\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23600/60000 (39%)]\tLoss: 1.480288\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23650/60000 (39%)]\tLoss: 1.461155\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23700/60000 (40%)]\tLoss: 1.501430\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23750/60000 (40%)]\tLoss: 1.464995\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23800/60000 (40%)]\tLoss: 1.481431\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23850/60000 (40%)]\tLoss: 1.461153\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23900/60000 (40%)]\tLoss: 1.479787\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [23950/60000 (40%)]\tLoss: 1.481074\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24000/60000 (40%)]\tLoss: 1.468553\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24050/60000 (40%)]\tLoss: 1.463283\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24100/60000 (40%)]\tLoss: 1.494736\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24150/60000 (40%)]\tLoss: 1.491903\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24200/60000 (40%)]\tLoss: 1.480938\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24250/60000 (40%)]\tLoss: 1.481634\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24300/60000 (40%)]\tLoss: 1.501040\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24350/60000 (41%)]\tLoss: 1.465812\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24400/60000 (41%)]\tLoss: 1.530213\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24450/60000 (41%)]\tLoss: 1.481151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24500/60000 (41%)]\tLoss: 1.501029\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24550/60000 (41%)]\tLoss: 1.461195\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24600/60000 (41%)]\tLoss: 1.498865\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24650/60000 (41%)]\tLoss: 1.461151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24700/60000 (41%)]\tLoss: 1.461163\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24750/60000 (41%)]\tLoss: 1.462679\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24800/60000 (41%)]\tLoss: 1.461180\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24850/60000 (41%)]\tLoss: 1.501145\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24900/60000 (42%)]\tLoss: 1.481151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [24950/60000 (42%)]\tLoss: 1.486333\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25000/60000 (42%)]\tLoss: 1.485656\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25050/60000 (42%)]\tLoss: 1.482792\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25100/60000 (42%)]\tLoss: 1.515753\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25150/60000 (42%)]\tLoss: 1.462499\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25200/60000 (42%)]\tLoss: 1.461153\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25250/60000 (42%)]\tLoss: 1.512869\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25300/60000 (42%)]\tLoss: 1.481164\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25350/60000 (42%)]\tLoss: 1.461247\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25400/60000 (42%)]\tLoss: 1.501154\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25450/60000 (42%)]\tLoss: 1.502977\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25500/60000 (42%)]\tLoss: 1.483007\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25550/60000 (43%)]\tLoss: 1.500866\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25600/60000 (43%)]\tLoss: 1.481071\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25650/60000 (43%)]\tLoss: 1.467626\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25700/60000 (43%)]\tLoss: 1.488155\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25750/60000 (43%)]\tLoss: 1.461461\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25800/60000 (43%)]\tLoss: 1.501965\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25850/60000 (43%)]\tLoss: 1.508900\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25900/60000 (43%)]\tLoss: 1.463783\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [25950/60000 (43%)]\tLoss: 1.461165\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26000/60000 (43%)]\tLoss: 1.481622\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26050/60000 (43%)]\tLoss: 1.480976\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26100/60000 (44%)]\tLoss: 1.506634\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26150/60000 (44%)]\tLoss: 1.481153\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26200/60000 (44%)]\tLoss: 1.461964\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26250/60000 (44%)]\tLoss: 1.481480\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26300/60000 (44%)]\tLoss: 1.464036\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26350/60000 (44%)]\tLoss: 1.474937\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26400/60000 (44%)]\tLoss: 1.481151\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26450/60000 (44%)]\tLoss: 1.501599\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26500/60000 (44%)]\tLoss: 1.461254\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26550/60000 (44%)]\tLoss: 1.486277\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26600/60000 (44%)]\tLoss: 1.565399\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26650/60000 (44%)]\tLoss: 1.482675\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26700/60000 (44%)]\tLoss: 1.495212\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26750/60000 (45%)]\tLoss: 1.501159\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26800/60000 (45%)]\tLoss: 1.481436\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26850/60000 (45%)]\tLoss: 1.508923\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26900/60000 (45%)]\tLoss: 1.469483\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [26950/60000 (45%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27000/60000 (45%)]\tLoss: 1.481141\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27050/60000 (45%)]\tLoss: 1.480388\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27100/60000 (45%)]\tLoss: 1.478609\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27150/60000 (45%)]\tLoss: 1.492672\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27200/60000 (45%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27250/60000 (45%)]\tLoss: 1.501463\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27300/60000 (46%)]\tLoss: 1.511636\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27350/60000 (46%)]\tLoss: 1.467861\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27400/60000 (46%)]\tLoss: 1.501146\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27450/60000 (46%)]\tLoss: 1.481218\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27500/60000 (46%)]\tLoss: 1.481143\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27550/60000 (46%)]\tLoss: 1.499699\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27600/60000 (46%)]\tLoss: 1.472336\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27650/60000 (46%)]\tLoss: 1.472177\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27700/60000 (46%)]\tLoss: 1.491844\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27750/60000 (46%)]\tLoss: 1.519773\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27800/60000 (46%)]\tLoss: 1.481862\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27850/60000 (46%)]\tLoss: 1.475611\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27900/60000 (46%)]\tLoss: 1.500737\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [27950/60000 (47%)]\tLoss: 1.461151\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28000/60000 (47%)]\tLoss: 1.481129\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28050/60000 (47%)]\tLoss: 1.501148\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28100/60000 (47%)]\tLoss: 1.463361\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28150/60000 (47%)]\tLoss: 1.501151\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28200/60000 (47%)]\tLoss: 1.501236\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28250/60000 (47%)]\tLoss: 1.481161\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28300/60000 (47%)]\tLoss: 1.499378\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28350/60000 (47%)]\tLoss: 1.467748\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28400/60000 (47%)]\tLoss: 1.462434\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28450/60000 (47%)]\tLoss: 1.481151\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28500/60000 (48%)]\tLoss: 1.481606\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28550/60000 (48%)]\tLoss: 1.497490\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28600/60000 (48%)]\tLoss: 1.483321\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28650/60000 (48%)]\tLoss: 1.490829\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28700/60000 (48%)]\tLoss: 1.461395\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28750/60000 (48%)]\tLoss: 1.471950\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28800/60000 (48%)]\tLoss: 1.481156\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28850/60000 (48%)]\tLoss: 1.481153\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28900/60000 (48%)]\tLoss: 1.461152\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [28950/60000 (48%)]\tLoss: 1.480189\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29000/60000 (48%)]\tLoss: 1.480477\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29050/60000 (48%)]\tLoss: 1.481151\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29100/60000 (48%)]\tLoss: 1.481307\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29150/60000 (49%)]\tLoss: 1.480483\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29200/60000 (49%)]\tLoss: 1.476214\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29250/60000 (49%)]\tLoss: 1.461588\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29300/60000 (49%)]\tLoss: 1.485765\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29350/60000 (49%)]\tLoss: 1.497375\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29400/60000 (49%)]\tLoss: 1.500435\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29450/60000 (49%)]\tLoss: 1.461157\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29500/60000 (49%)]\tLoss: 1.481147\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29550/60000 (49%)]\tLoss: 1.479252\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29600/60000 (49%)]\tLoss: 1.472902\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29650/60000 (49%)]\tLoss: 1.461158\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29700/60000 (50%)]\tLoss: 1.480698\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29750/60000 (50%)]\tLoss: 1.481183\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29800/60000 (50%)]\tLoss: 1.462198\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29850/60000 (50%)]\tLoss: 1.482003\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29900/60000 (50%)]\tLoss: 1.481151\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [29950/60000 (50%)]\tLoss: 1.512241\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30000/60000 (50%)]\tLoss: 1.461336\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30050/60000 (50%)]\tLoss: 1.515464\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30100/60000 (50%)]\tLoss: 1.461220\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30150/60000 (50%)]\tLoss: 1.481474\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30200/60000 (50%)]\tLoss: 1.461151\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30250/60000 (50%)]\tLoss: 1.481146\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30300/60000 (50%)]\tLoss: 1.479964\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30350/60000 (51%)]\tLoss: 1.461151\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30400/60000 (51%)]\tLoss: 1.465603\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30450/60000 (51%)]\tLoss: 1.493614\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30500/60000 (51%)]\tLoss: 1.475855\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30550/60000 (51%)]\tLoss: 1.481494\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30600/60000 (51%)]\tLoss: 1.484688\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30650/60000 (51%)]\tLoss: 1.480849\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30700/60000 (51%)]\tLoss: 1.461851\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30750/60000 (51%)]\tLoss: 1.480991\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30800/60000 (51%)]\tLoss: 1.500756\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30850/60000 (51%)]\tLoss: 1.463019\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30900/60000 (52%)]\tLoss: 1.480944\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [30950/60000 (52%)]\tLoss: 1.471874\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31000/60000 (52%)]\tLoss: 1.476169\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31050/60000 (52%)]\tLoss: 1.482567\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31100/60000 (52%)]\tLoss: 1.461472\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31150/60000 (52%)]\tLoss: 1.461233\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31200/60000 (52%)]\tLoss: 1.480572\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31250/60000 (52%)]\tLoss: 1.499858\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31300/60000 (52%)]\tLoss: 1.498360\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31350/60000 (52%)]\tLoss: 1.557086\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31400/60000 (52%)]\tLoss: 1.500644\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31450/60000 (52%)]\tLoss: 1.481151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31500/60000 (52%)]\tLoss: 1.502096\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31550/60000 (53%)]\tLoss: 1.474146\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31600/60000 (53%)]\tLoss: 1.474980\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31650/60000 (53%)]\tLoss: 1.501498\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31700/60000 (53%)]\tLoss: 1.463270\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31750/60000 (53%)]\tLoss: 1.481666\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31800/60000 (53%)]\tLoss: 1.479148\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31850/60000 (53%)]\tLoss: 1.461208\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31900/60000 (53%)]\tLoss: 1.461249\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [31950/60000 (53%)]\tLoss: 1.475152\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32000/60000 (53%)]\tLoss: 1.500288\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32050/60000 (53%)]\tLoss: 1.467218\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32100/60000 (54%)]\tLoss: 1.473050\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32150/60000 (54%)]\tLoss: 1.477384\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32200/60000 (54%)]\tLoss: 1.461189\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32250/60000 (54%)]\tLoss: 1.478569\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32300/60000 (54%)]\tLoss: 1.461162\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32350/60000 (54%)]\tLoss: 1.477703\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32400/60000 (54%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32450/60000 (54%)]\tLoss: 1.473351\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32500/60000 (54%)]\tLoss: 1.498538\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32550/60000 (54%)]\tLoss: 1.482511\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32600/60000 (54%)]\tLoss: 1.461210\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32650/60000 (54%)]\tLoss: 1.481420\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32700/60000 (54%)]\tLoss: 1.500361\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32750/60000 (55%)]\tLoss: 1.474299\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32800/60000 (55%)]\tLoss: 1.461163\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32850/60000 (55%)]\tLoss: 1.480909\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32900/60000 (55%)]\tLoss: 1.461155\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [32950/60000 (55%)]\tLoss: 1.481200\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33000/60000 (55%)]\tLoss: 1.461359\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33050/60000 (55%)]\tLoss: 1.480998\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33100/60000 (55%)]\tLoss: 1.525137\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33150/60000 (55%)]\tLoss: 1.463802\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33200/60000 (55%)]\tLoss: 1.461335\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33250/60000 (55%)]\tLoss: 1.461177\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33300/60000 (56%)]\tLoss: 1.471720\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33350/60000 (56%)]\tLoss: 1.481218\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33400/60000 (56%)]\tLoss: 1.501081\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33450/60000 (56%)]\tLoss: 1.461160\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33500/60000 (56%)]\tLoss: 1.506027\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33550/60000 (56%)]\tLoss: 1.461165\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33600/60000 (56%)]\tLoss: 1.489536\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33650/60000 (56%)]\tLoss: 1.461153\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33700/60000 (56%)]\tLoss: 1.480412\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33750/60000 (56%)]\tLoss: 1.461328\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33800/60000 (56%)]\tLoss: 1.481171\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33850/60000 (56%)]\tLoss: 1.480713\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33900/60000 (56%)]\tLoss: 1.467426\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [33950/60000 (57%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34000/60000 (57%)]\tLoss: 1.473266\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34050/60000 (57%)]\tLoss: 1.481139\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34100/60000 (57%)]\tLoss: 1.491857\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34150/60000 (57%)]\tLoss: 1.461402\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34200/60000 (57%)]\tLoss: 1.480138\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34250/60000 (57%)]\tLoss: 1.461179\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34300/60000 (57%)]\tLoss: 1.523181\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34350/60000 (57%)]\tLoss: 1.487805\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34400/60000 (57%)]\tLoss: 1.461363\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34450/60000 (57%)]\tLoss: 1.481426\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34500/60000 (58%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34550/60000 (58%)]\tLoss: 1.491133\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34600/60000 (58%)]\tLoss: 1.481094\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34650/60000 (58%)]\tLoss: 1.499683\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34700/60000 (58%)]\tLoss: 1.500417\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34750/60000 (58%)]\tLoss: 1.478462\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34800/60000 (58%)]\tLoss: 1.483371\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34850/60000 (58%)]\tLoss: 1.508427\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34900/60000 (58%)]\tLoss: 1.461188\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [34950/60000 (58%)]\tLoss: 1.495332\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35000/60000 (58%)]\tLoss: 1.489324\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35050/60000 (58%)]\tLoss: 1.461152\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35100/60000 (58%)]\tLoss: 1.461151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35150/60000 (59%)]\tLoss: 1.461774\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35200/60000 (59%)]\tLoss: 1.481158\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35250/60000 (59%)]\tLoss: 1.461393\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35300/60000 (59%)]\tLoss: 1.461377\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35350/60000 (59%)]\tLoss: 1.481460\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35400/60000 (59%)]\tLoss: 1.461152\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35450/60000 (59%)]\tLoss: 1.481151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35500/60000 (59%)]\tLoss: 1.472612\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35550/60000 (59%)]\tLoss: 1.481186\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35600/60000 (59%)]\tLoss: 1.463497\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35650/60000 (59%)]\tLoss: 1.483236\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35700/60000 (60%)]\tLoss: 1.461442\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35750/60000 (60%)]\tLoss: 1.461552\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35800/60000 (60%)]\tLoss: 1.480792\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35850/60000 (60%)]\tLoss: 1.490385\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35900/60000 (60%)]\tLoss: 1.462387\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [35950/60000 (60%)]\tLoss: 1.481134\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36000/60000 (60%)]\tLoss: 1.480336\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36050/60000 (60%)]\tLoss: 1.468988\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36100/60000 (60%)]\tLoss: 1.491515\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36150/60000 (60%)]\tLoss: 1.545062\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36200/60000 (60%)]\tLoss: 1.461250\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36250/60000 (60%)]\tLoss: 1.461247\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36300/60000 (60%)]\tLoss: 1.465064\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36350/60000 (61%)]\tLoss: 1.461305\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36400/60000 (61%)]\tLoss: 1.480971\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36450/60000 (61%)]\tLoss: 1.484750\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36500/60000 (61%)]\tLoss: 1.478352\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36550/60000 (61%)]\tLoss: 1.472025\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36600/60000 (61%)]\tLoss: 1.503465\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36650/60000 (61%)]\tLoss: 1.490210\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36700/60000 (61%)]\tLoss: 1.552220\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36750/60000 (61%)]\tLoss: 1.461152\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36800/60000 (61%)]\tLoss: 1.464932\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36850/60000 (61%)]\tLoss: 1.461244\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36900/60000 (62%)]\tLoss: 1.461167\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [36950/60000 (62%)]\tLoss: 1.461157\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37000/60000 (62%)]\tLoss: 1.481075\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37050/60000 (62%)]\tLoss: 1.500711\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37100/60000 (62%)]\tLoss: 1.465755\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37150/60000 (62%)]\tLoss: 1.481138\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37200/60000 (62%)]\tLoss: 1.501497\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37250/60000 (62%)]\tLoss: 1.464136\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37300/60000 (62%)]\tLoss: 1.461153\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37350/60000 (62%)]\tLoss: 1.462082\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37400/60000 (62%)]\tLoss: 1.475612\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37450/60000 (62%)]\tLoss: 1.481142\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37500/60000 (62%)]\tLoss: 1.461329\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37550/60000 (63%)]\tLoss: 1.461669\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37600/60000 (63%)]\tLoss: 1.481832\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37650/60000 (63%)]\tLoss: 1.493403\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37700/60000 (63%)]\tLoss: 1.472801\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37750/60000 (63%)]\tLoss: 1.501181\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37800/60000 (63%)]\tLoss: 1.461153\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37850/60000 (63%)]\tLoss: 1.480360\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37900/60000 (63%)]\tLoss: 1.481833\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [37950/60000 (63%)]\tLoss: 1.482273\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38000/60000 (63%)]\tLoss: 1.461152\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38050/60000 (63%)]\tLoss: 1.481242\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38100/60000 (64%)]\tLoss: 1.481229\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38150/60000 (64%)]\tLoss: 1.485404\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38200/60000 (64%)]\tLoss: 1.471716\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38250/60000 (64%)]\tLoss: 1.483903\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38300/60000 (64%)]\tLoss: 1.461154\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38350/60000 (64%)]\tLoss: 1.468859\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38400/60000 (64%)]\tLoss: 1.501026\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38450/60000 (64%)]\tLoss: 1.487000\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38500/60000 (64%)]\tLoss: 1.518104\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38550/60000 (64%)]\tLoss: 1.461154\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38600/60000 (64%)]\tLoss: 1.511495\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38650/60000 (64%)]\tLoss: 1.485905\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38700/60000 (64%)]\tLoss: 1.474084\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38750/60000 (65%)]\tLoss: 1.481706\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38800/60000 (65%)]\tLoss: 1.473661\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38850/60000 (65%)]\tLoss: 1.461191\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38900/60000 (65%)]\tLoss: 1.461189\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [38950/60000 (65%)]\tLoss: 1.462076\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39000/60000 (65%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39050/60000 (65%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39100/60000 (65%)]\tLoss: 1.478350\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39150/60000 (65%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39200/60000 (65%)]\tLoss: 1.481129\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39250/60000 (65%)]\tLoss: 1.492417\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39300/60000 (66%)]\tLoss: 1.466449\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39350/60000 (66%)]\tLoss: 1.484731\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39400/60000 (66%)]\tLoss: 1.461157\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39450/60000 (66%)]\tLoss: 1.461172\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39500/60000 (66%)]\tLoss: 1.461263\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39550/60000 (66%)]\tLoss: 1.461151\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39600/60000 (66%)]\tLoss: 1.572043\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39650/60000 (66%)]\tLoss: 1.461158\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39700/60000 (66%)]\tLoss: 1.520652\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39750/60000 (66%)]\tLoss: 1.481151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39800/60000 (66%)]\tLoss: 1.498606\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39850/60000 (66%)]\tLoss: 1.461152\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39900/60000 (66%)]\tLoss: 1.470593\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [39950/60000 (67%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40000/60000 (67%)]\tLoss: 1.481294\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40050/60000 (67%)]\tLoss: 1.481884\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40100/60000 (67%)]\tLoss: 1.461153\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40150/60000 (67%)]\tLoss: 1.481180\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40200/60000 (67%)]\tLoss: 1.478449\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40250/60000 (67%)]\tLoss: 1.484882\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40300/60000 (67%)]\tLoss: 1.477144\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40350/60000 (67%)]\tLoss: 1.487411\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40400/60000 (67%)]\tLoss: 1.492482\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40450/60000 (67%)]\tLoss: 1.462043\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40500/60000 (68%)]\tLoss: 1.461161\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40550/60000 (68%)]\tLoss: 1.482368\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40600/60000 (68%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40650/60000 (68%)]\tLoss: 1.461380\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40700/60000 (68%)]\tLoss: 1.461210\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40750/60000 (68%)]\tLoss: 1.501037\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40800/60000 (68%)]\tLoss: 1.482491\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40850/60000 (68%)]\tLoss: 1.519607\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40900/60000 (68%)]\tLoss: 1.461152\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [40950/60000 (68%)]\tLoss: 1.485849\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41000/60000 (68%)]\tLoss: 1.480149\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41050/60000 (68%)]\tLoss: 1.481133\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41100/60000 (68%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41150/60000 (69%)]\tLoss: 1.471593\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41200/60000 (69%)]\tLoss: 1.461154\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41250/60000 (69%)]\tLoss: 1.461157\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41300/60000 (69%)]\tLoss: 1.461267\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41350/60000 (69%)]\tLoss: 1.485719\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41400/60000 (69%)]\tLoss: 1.502443\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41450/60000 (69%)]\tLoss: 1.471223\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41500/60000 (69%)]\tLoss: 1.484265\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41550/60000 (69%)]\tLoss: 1.461152\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41600/60000 (69%)]\tLoss: 1.475115\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41650/60000 (69%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41700/60000 (70%)]\tLoss: 1.480689\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41750/60000 (70%)]\tLoss: 1.481142\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41800/60000 (70%)]\tLoss: 1.520640\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41850/60000 (70%)]\tLoss: 1.481137\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41900/60000 (70%)]\tLoss: 1.461233\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [41950/60000 (70%)]\tLoss: 1.494968\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42000/60000 (70%)]\tLoss: 1.474765\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42050/60000 (70%)]\tLoss: 1.481082\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42100/60000 (70%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42150/60000 (70%)]\tLoss: 1.465699\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42200/60000 (70%)]\tLoss: 1.475848\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42250/60000 (70%)]\tLoss: 1.474645\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42300/60000 (70%)]\tLoss: 1.479630\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42350/60000 (71%)]\tLoss: 1.504435\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42400/60000 (71%)]\tLoss: 1.462304\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42450/60000 (71%)]\tLoss: 1.480994\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42500/60000 (71%)]\tLoss: 1.461168\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42550/60000 (71%)]\tLoss: 1.461160\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42600/60000 (71%)]\tLoss: 1.512368\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42650/60000 (71%)]\tLoss: 1.499714\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42700/60000 (71%)]\tLoss: 1.499939\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42750/60000 (71%)]\tLoss: 1.461155\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42800/60000 (71%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42850/60000 (71%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42900/60000 (72%)]\tLoss: 1.509934\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [42950/60000 (72%)]\tLoss: 1.481157\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43000/60000 (72%)]\tLoss: 1.484765\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43050/60000 (72%)]\tLoss: 1.500442\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43100/60000 (72%)]\tLoss: 1.461151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43150/60000 (72%)]\tLoss: 1.502850\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43200/60000 (72%)]\tLoss: 1.461646\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43250/60000 (72%)]\tLoss: 1.476687\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43300/60000 (72%)]\tLoss: 1.486719\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43350/60000 (72%)]\tLoss: 1.464377\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43400/60000 (72%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43450/60000 (72%)]\tLoss: 1.501031\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43500/60000 (72%)]\tLoss: 1.499949\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43550/60000 (73%)]\tLoss: 1.461981\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43600/60000 (73%)]\tLoss: 1.462290\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43650/60000 (73%)]\tLoss: 1.480915\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43700/60000 (73%)]\tLoss: 1.496198\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43750/60000 (73%)]\tLoss: 1.468426\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43800/60000 (73%)]\tLoss: 1.462424\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43850/60000 (73%)]\tLoss: 1.519663\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43900/60000 (73%)]\tLoss: 1.480771\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [43950/60000 (73%)]\tLoss: 1.461419\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44000/60000 (73%)]\tLoss: 1.471616\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44050/60000 (73%)]\tLoss: 1.512173\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44100/60000 (74%)]\tLoss: 1.461236\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44150/60000 (74%)]\tLoss: 1.497399\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44200/60000 (74%)]\tLoss: 1.469113\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44250/60000 (74%)]\tLoss: 1.518987\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44300/60000 (74%)]\tLoss: 1.483279\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44350/60000 (74%)]\tLoss: 1.501374\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44400/60000 (74%)]\tLoss: 1.461151\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44450/60000 (74%)]\tLoss: 1.487911\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44500/60000 (74%)]\tLoss: 1.481839\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44550/60000 (74%)]\tLoss: 1.489644\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44600/60000 (74%)]\tLoss: 1.489228\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44650/60000 (74%)]\tLoss: 1.487348\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44700/60000 (74%)]\tLoss: 1.498306\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44750/60000 (75%)]\tLoss: 1.481363\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44800/60000 (75%)]\tLoss: 1.461250\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44850/60000 (75%)]\tLoss: 1.461213\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44900/60000 (75%)]\tLoss: 1.471755\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [44950/60000 (75%)]\tLoss: 1.497879\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45000/60000 (75%)]\tLoss: 1.461152\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45050/60000 (75%)]\tLoss: 1.467561\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45100/60000 (75%)]\tLoss: 1.461151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45150/60000 (75%)]\tLoss: 1.510480\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45200/60000 (75%)]\tLoss: 1.461884\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45250/60000 (75%)]\tLoss: 1.489599\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45300/60000 (76%)]\tLoss: 1.465914\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45350/60000 (76%)]\tLoss: 1.465300\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45400/60000 (76%)]\tLoss: 1.461765\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45450/60000 (76%)]\tLoss: 1.463030\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45500/60000 (76%)]\tLoss: 1.462236\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45550/60000 (76%)]\tLoss: 1.480377\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45600/60000 (76%)]\tLoss: 1.481179\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45650/60000 (76%)]\tLoss: 1.512652\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45700/60000 (76%)]\tLoss: 1.461576\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45750/60000 (76%)]\tLoss: 1.480479\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45800/60000 (76%)]\tLoss: 1.481591\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45850/60000 (76%)]\tLoss: 1.490272\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45900/60000 (76%)]\tLoss: 1.497799\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [45950/60000 (77%)]\tLoss: 1.494099\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46000/60000 (77%)]\tLoss: 1.481159\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46050/60000 (77%)]\tLoss: 1.463186\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46100/60000 (77%)]\tLoss: 1.493702\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46150/60000 (77%)]\tLoss: 1.478288\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46200/60000 (77%)]\tLoss: 1.537552\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46250/60000 (77%)]\tLoss: 1.521739\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46300/60000 (77%)]\tLoss: 1.501127\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46350/60000 (77%)]\tLoss: 1.481513\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46400/60000 (77%)]\tLoss: 1.495990\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46450/60000 (77%)]\tLoss: 1.483954\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46500/60000 (78%)]\tLoss: 1.483465\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46550/60000 (78%)]\tLoss: 1.463137\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46600/60000 (78%)]\tLoss: 1.497658\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46650/60000 (78%)]\tLoss: 1.461156\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46700/60000 (78%)]\tLoss: 1.470693\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46750/60000 (78%)]\tLoss: 1.461152\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46800/60000 (78%)]\tLoss: 1.536007\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46850/60000 (78%)]\tLoss: 1.501924\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46900/60000 (78%)]\tLoss: 1.475318\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [46950/60000 (78%)]\tLoss: 1.461151\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47000/60000 (78%)]\tLoss: 1.464343\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47050/60000 (78%)]\tLoss: 1.461173\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47100/60000 (78%)]\tLoss: 1.471614\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47150/60000 (79%)]\tLoss: 1.481537\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47200/60000 (79%)]\tLoss: 1.480907\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47250/60000 (79%)]\tLoss: 1.479599\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47300/60000 (79%)]\tLoss: 1.483060\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47350/60000 (79%)]\tLoss: 1.461162\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47400/60000 (79%)]\tLoss: 1.481104\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47450/60000 (79%)]\tLoss: 1.478449\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47500/60000 (79%)]\tLoss: 1.519866\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47550/60000 (79%)]\tLoss: 1.488145\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47600/60000 (79%)]\tLoss: 1.481079\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47650/60000 (79%)]\tLoss: 1.482160\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47700/60000 (80%)]\tLoss: 1.461453\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47750/60000 (80%)]\tLoss: 1.480695\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47800/60000 (80%)]\tLoss: 1.501177\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47850/60000 (80%)]\tLoss: 1.481317\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47900/60000 (80%)]\tLoss: 1.471115\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [47950/60000 (80%)]\tLoss: 1.481196\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48000/60000 (80%)]\tLoss: 1.481062\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48050/60000 (80%)]\tLoss: 1.481294\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48100/60000 (80%)]\tLoss: 1.467672\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48150/60000 (80%)]\tLoss: 1.461173\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48200/60000 (80%)]\tLoss: 1.467505\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48250/60000 (80%)]\tLoss: 1.502171\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48300/60000 (80%)]\tLoss: 1.461183\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48350/60000 (81%)]\tLoss: 1.478535\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48400/60000 (81%)]\tLoss: 1.479715\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48450/60000 (81%)]\tLoss: 1.481088\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48500/60000 (81%)]\tLoss: 1.481295\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48550/60000 (81%)]\tLoss: 1.482824\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48600/60000 (81%)]\tLoss: 1.461757\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48650/60000 (81%)]\tLoss: 1.473339\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48700/60000 (81%)]\tLoss: 1.530885\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48750/60000 (81%)]\tLoss: 1.479550\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48800/60000 (81%)]\tLoss: 1.461304\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48850/60000 (81%)]\tLoss: 1.462795\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48900/60000 (82%)]\tLoss: 1.463661\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [48950/60000 (82%)]\tLoss: 1.472412\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49000/60000 (82%)]\tLoss: 1.537371\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49050/60000 (82%)]\tLoss: 1.500325\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49100/60000 (82%)]\tLoss: 1.482083\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49150/60000 (82%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49200/60000 (82%)]\tLoss: 1.501402\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49250/60000 (82%)]\tLoss: 1.483876\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49300/60000 (82%)]\tLoss: 1.481422\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49350/60000 (82%)]\tLoss: 1.501158\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49400/60000 (82%)]\tLoss: 1.481864\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49450/60000 (82%)]\tLoss: 1.479965\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49500/60000 (82%)]\tLoss: 1.481743\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49550/60000 (83%)]\tLoss: 1.480202\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49600/60000 (83%)]\tLoss: 1.461203\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49650/60000 (83%)]\tLoss: 1.461299\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49700/60000 (83%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49750/60000 (83%)]\tLoss: 1.461941\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49800/60000 (83%)]\tLoss: 1.507962\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49850/60000 (83%)]\tLoss: 1.468148\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49900/60000 (83%)]\tLoss: 1.501080\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [49950/60000 (83%)]\tLoss: 1.484587\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50000/60000 (83%)]\tLoss: 1.481148\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50050/60000 (83%)]\tLoss: 1.461154\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50100/60000 (84%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50150/60000 (84%)]\tLoss: 1.461194\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50200/60000 (84%)]\tLoss: 1.477005\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50250/60000 (84%)]\tLoss: 1.481354\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50300/60000 (84%)]\tLoss: 1.461154\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50350/60000 (84%)]\tLoss: 1.474673\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50400/60000 (84%)]\tLoss: 1.515258\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50450/60000 (84%)]\tLoss: 1.481178\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50500/60000 (84%)]\tLoss: 1.501454\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50550/60000 (84%)]\tLoss: 1.514909\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50600/60000 (84%)]\tLoss: 1.481085\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50650/60000 (84%)]\tLoss: 1.481823\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50700/60000 (84%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50750/60000 (85%)]\tLoss: 1.461152\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50800/60000 (85%)]\tLoss: 1.461272\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50850/60000 (85%)]\tLoss: 1.461152\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50900/60000 (85%)]\tLoss: 1.480047\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [50950/60000 (85%)]\tLoss: 1.478881\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51000/60000 (85%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51050/60000 (85%)]\tLoss: 1.501418\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51100/60000 (85%)]\tLoss: 1.521002\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51150/60000 (85%)]\tLoss: 1.498151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51200/60000 (85%)]\tLoss: 1.470314\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51250/60000 (85%)]\tLoss: 1.500876\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51300/60000 (86%)]\tLoss: 1.477543\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51350/60000 (86%)]\tLoss: 1.461256\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51400/60000 (86%)]\tLoss: 1.461161\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51450/60000 (86%)]\tLoss: 1.465816\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51500/60000 (86%)]\tLoss: 1.465858\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51550/60000 (86%)]\tLoss: 1.467543\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51600/60000 (86%)]\tLoss: 1.479378\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51650/60000 (86%)]\tLoss: 1.461154\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51700/60000 (86%)]\tLoss: 1.499919\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51750/60000 (86%)]\tLoss: 1.481140\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51800/60000 (86%)]\tLoss: 1.500459\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51850/60000 (86%)]\tLoss: 1.481503\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51900/60000 (86%)]\tLoss: 1.480700\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [51950/60000 (87%)]\tLoss: 1.461206\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52000/60000 (87%)]\tLoss: 1.496328\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52050/60000 (87%)]\tLoss: 1.501070\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52100/60000 (87%)]\tLoss: 1.500268\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52150/60000 (87%)]\tLoss: 1.461153\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52200/60000 (87%)]\tLoss: 1.461162\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52250/60000 (87%)]\tLoss: 1.465182\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52300/60000 (87%)]\tLoss: 1.461233\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52350/60000 (87%)]\tLoss: 1.461264\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52400/60000 (87%)]\tLoss: 1.501254\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52450/60000 (87%)]\tLoss: 1.461408\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52500/60000 (88%)]\tLoss: 1.529293\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52550/60000 (88%)]\tLoss: 1.495474\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52600/60000 (88%)]\tLoss: 1.481151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52650/60000 (88%)]\tLoss: 1.501150\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52700/60000 (88%)]\tLoss: 1.481117\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52750/60000 (88%)]\tLoss: 1.467453\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52800/60000 (88%)]\tLoss: 1.500315\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52850/60000 (88%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52900/60000 (88%)]\tLoss: 1.500100\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [52950/60000 (88%)]\tLoss: 1.518692\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53000/60000 (88%)]\tLoss: 1.475334\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53050/60000 (88%)]\tLoss: 1.461797\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53100/60000 (88%)]\tLoss: 1.475203\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53150/60000 (89%)]\tLoss: 1.499483\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53200/60000 (89%)]\tLoss: 1.465815\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53250/60000 (89%)]\tLoss: 1.461176\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53300/60000 (89%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53350/60000 (89%)]\tLoss: 1.501151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53400/60000 (89%)]\tLoss: 1.462172\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53450/60000 (89%)]\tLoss: 1.481151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53500/60000 (89%)]\tLoss: 1.523653\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53550/60000 (89%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53600/60000 (89%)]\tLoss: 1.484802\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53650/60000 (89%)]\tLoss: 1.461476\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53700/60000 (90%)]\tLoss: 1.461752\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53750/60000 (90%)]\tLoss: 1.486337\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53800/60000 (90%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53850/60000 (90%)]\tLoss: 1.461454\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53900/60000 (90%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [53950/60000 (90%)]\tLoss: 1.476587\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54000/60000 (90%)]\tLoss: 1.499450\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54050/60000 (90%)]\tLoss: 1.476760\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54100/60000 (90%)]\tLoss: 1.480875\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54150/60000 (90%)]\tLoss: 1.496121\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54200/60000 (90%)]\tLoss: 1.476962\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54250/60000 (90%)]\tLoss: 1.487701\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54300/60000 (90%)]\tLoss: 1.481167\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54350/60000 (91%)]\tLoss: 1.461544\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54400/60000 (91%)]\tLoss: 1.461209\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54450/60000 (91%)]\tLoss: 1.479733\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54500/60000 (91%)]\tLoss: 1.461414\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54550/60000 (91%)]\tLoss: 1.467876\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54600/60000 (91%)]\tLoss: 1.482100\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54650/60000 (91%)]\tLoss: 1.461212\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54700/60000 (91%)]\tLoss: 1.481151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54750/60000 (91%)]\tLoss: 1.479025\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54800/60000 (91%)]\tLoss: 1.500968\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54850/60000 (91%)]\tLoss: 1.471354\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54900/60000 (92%)]\tLoss: 1.463516\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [54950/60000 (92%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55000/60000 (92%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55050/60000 (92%)]\tLoss: 1.482207\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55100/60000 (92%)]\tLoss: 1.494445\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55150/60000 (92%)]\tLoss: 1.461277\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55200/60000 (92%)]\tLoss: 1.515640\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55250/60000 (92%)]\tLoss: 1.461160\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55300/60000 (92%)]\tLoss: 1.471152\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55350/60000 (92%)]\tLoss: 1.461176\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55400/60000 (92%)]\tLoss: 1.461333\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55450/60000 (92%)]\tLoss: 1.498195\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55500/60000 (92%)]\tLoss: 1.513572\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55550/60000 (93%)]\tLoss: 1.467343\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55600/60000 (93%)]\tLoss: 1.481189\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55650/60000 (93%)]\tLoss: 1.461370\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55700/60000 (93%)]\tLoss: 1.466734\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55750/60000 (93%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55800/60000 (93%)]\tLoss: 1.481124\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55850/60000 (93%)]\tLoss: 1.484355\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55900/60000 (93%)]\tLoss: 1.501615\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [55950/60000 (93%)]\tLoss: 1.482859\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56000/60000 (93%)]\tLoss: 1.480853\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56050/60000 (93%)]\tLoss: 1.501143\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56100/60000 (94%)]\tLoss: 1.461158\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56150/60000 (94%)]\tLoss: 1.521351\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56200/60000 (94%)]\tLoss: 1.476478\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56250/60000 (94%)]\tLoss: 1.500050\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56300/60000 (94%)]\tLoss: 1.480012\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56350/60000 (94%)]\tLoss: 1.461481\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56400/60000 (94%)]\tLoss: 1.481138\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56450/60000 (94%)]\tLoss: 1.481132\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56500/60000 (94%)]\tLoss: 1.500748\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56550/60000 (94%)]\tLoss: 1.466818\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56600/60000 (94%)]\tLoss: 1.471949\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56650/60000 (94%)]\tLoss: 1.464857\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56700/60000 (94%)]\tLoss: 1.480930\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56750/60000 (95%)]\tLoss: 1.520662\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56800/60000 (95%)]\tLoss: 1.501142\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56850/60000 (95%)]\tLoss: 1.461152\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56900/60000 (95%)]\tLoss: 1.501168\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [56950/60000 (95%)]\tLoss: 1.478844\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57000/60000 (95%)]\tLoss: 1.471033\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57050/60000 (95%)]\tLoss: 1.462570\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57100/60000 (95%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57150/60000 (95%)]\tLoss: 1.501209\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57200/60000 (95%)]\tLoss: 1.506456\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57250/60000 (95%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57300/60000 (96%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57350/60000 (96%)]\tLoss: 1.481143\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57400/60000 (96%)]\tLoss: 1.462717\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57450/60000 (96%)]\tLoss: 1.541070\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57500/60000 (96%)]\tLoss: 1.461334\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57550/60000 (96%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57600/60000 (96%)]\tLoss: 1.462165\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57650/60000 (96%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57700/60000 (96%)]\tLoss: 1.481165\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57750/60000 (96%)]\tLoss: 1.509104\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57800/60000 (96%)]\tLoss: 1.461447\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57850/60000 (96%)]\tLoss: 1.481172\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57900/60000 (96%)]\tLoss: 1.463599\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [57950/60000 (97%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58000/60000 (97%)]\tLoss: 1.481316\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58050/60000 (97%)]\tLoss: 1.497831\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58100/60000 (97%)]\tLoss: 1.463385\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58150/60000 (97%)]\tLoss: 1.481122\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58200/60000 (97%)]\tLoss: 1.461170\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58250/60000 (97%)]\tLoss: 1.481133\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58300/60000 (97%)]\tLoss: 1.461692\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58350/60000 (97%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58400/60000 (97%)]\tLoss: 1.469468\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58450/60000 (97%)]\tLoss: 1.461161\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58500/60000 (98%)]\tLoss: 1.463556\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58550/60000 (98%)]\tLoss: 1.461211\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58600/60000 (98%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58650/60000 (98%)]\tLoss: 1.500964\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58700/60000 (98%)]\tLoss: 1.461152\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58750/60000 (98%)]\tLoss: 1.484330\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58800/60000 (98%)]\tLoss: 1.481151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58850/60000 (98%)]\tLoss: 1.462161\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58900/60000 (98%)]\tLoss: 1.494278\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [58950/60000 (98%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59000/60000 (98%)]\tLoss: 1.481700\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59050/60000 (98%)]\tLoss: 1.461222\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59100/60000 (98%)]\tLoss: 1.481153\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59150/60000 (99%)]\tLoss: 1.481154\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59200/60000 (99%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59250/60000 (99%)]\tLoss: 1.461223\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59300/60000 (99%)]\tLoss: 1.501001\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59350/60000 (99%)]\tLoss: 1.468204\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59400/60000 (99%)]\tLoss: 1.461159\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59450/60000 (99%)]\tLoss: 1.501064\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59500/60000 (99%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59550/60000 (99%)]\tLoss: 1.501153\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59600/60000 (99%)]\tLoss: 1.484172\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59650/60000 (99%)]\tLoss: 1.461468\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59700/60000 (100%)]\tLoss: 1.511073\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59750/60000 (100%)]\tLoss: 1.476470\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59800/60000 (100%)]\tLoss: 1.461214\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59850/60000 (100%)]\tLoss: 1.483148\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59900/60000 (100%)]\tLoss: 1.494615\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 3 [59950/60000 (100%)]\tLoss: 1.461153\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [0/60000 (0%)]\tLoss: 1.479635\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50/60000 (0%)]\tLoss: 1.487520\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [100/60000 (0%)]\tLoss: 1.501086\tAccuracy: 97.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [150/60000 (0%)]\tLoss: 1.461684\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [200/60000 (0%)]\tLoss: 1.481022\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [250/60000 (0%)]\tLoss: 1.461223\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [300/60000 (0%)]\tLoss: 1.461151\tAccuracy: 98.57%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [350/60000 (1%)]\tLoss: 1.524023\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [400/60000 (1%)]\tLoss: 1.485711\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [450/60000 (1%)]\tLoss: 1.481151\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [500/60000 (1%)]\tLoss: 1.462217\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [550/60000 (1%)]\tLoss: 1.479228\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [600/60000 (1%)]\tLoss: 1.481150\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [650/60000 (1%)]\tLoss: 1.509115\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [700/60000 (1%)]\tLoss: 1.472866\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [750/60000 (1%)]\tLoss: 1.475675\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [800/60000 (1%)]\tLoss: 1.494904\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [850/60000 (1%)]\tLoss: 1.481151\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [900/60000 (2%)]\tLoss: 1.461172\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [950/60000 (2%)]\tLoss: 1.461283\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1000/60000 (2%)]\tLoss: 1.469181\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1050/60000 (2%)]\tLoss: 1.461355\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1100/60000 (2%)]\tLoss: 1.481154\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1150/60000 (2%)]\tLoss: 1.481235\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1200/60000 (2%)]\tLoss: 1.500766\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1250/60000 (2%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1300/60000 (2%)]\tLoss: 1.463307\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1350/60000 (2%)]\tLoss: 1.540967\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1400/60000 (2%)]\tLoss: 1.461408\tAccuracy: 98.21%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1450/60000 (2%)]\tLoss: 1.482301\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1500/60000 (2%)]\tLoss: 1.466387\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1550/60000 (3%)]\tLoss: 1.521149\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1600/60000 (3%)]\tLoss: 1.504810\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1650/60000 (3%)]\tLoss: 1.474047\tAccuracy: 98.06%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1700/60000 (3%)]\tLoss: 1.510657\tAccuracy: 97.94%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1750/60000 (3%)]\tLoss: 1.472913\tAccuracy: 97.94%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1800/60000 (3%)]\tLoss: 1.510974\tAccuracy: 97.84%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1850/60000 (3%)]\tLoss: 1.482299\tAccuracy: 97.84%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1900/60000 (3%)]\tLoss: 1.478701\tAccuracy: 97.85%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [1950/60000 (3%)]\tLoss: 1.481933\tAccuracy: 97.85%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2000/60000 (3%)]\tLoss: 1.500948\tAccuracy: 97.80%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2050/60000 (3%)]\tLoss: 1.472511\tAccuracy: 97.81%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2100/60000 (4%)]\tLoss: 1.461178\tAccuracy: 97.86%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2150/60000 (4%)]\tLoss: 1.487935\tAccuracy: 97.86%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2200/60000 (4%)]\tLoss: 1.466233\tAccuracy: 97.91%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2250/60000 (4%)]\tLoss: 1.481152\tAccuracy: 97.91%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2300/60000 (4%)]\tLoss: 1.463662\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2350/60000 (4%)]\tLoss: 1.481151\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2400/60000 (4%)]\tLoss: 1.481214\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2450/60000 (4%)]\tLoss: 1.480767\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2500/60000 (4%)]\tLoss: 1.481370\tAccuracy: 97.96%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2550/60000 (4%)]\tLoss: 1.517590\tAccuracy: 97.88%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2600/60000 (4%)]\tLoss: 1.505014\tAccuracy: 97.85%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2650/60000 (4%)]\tLoss: 1.482054\tAccuracy: 97.85%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2700/60000 (4%)]\tLoss: 1.467310\tAccuracy: 97.89%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2750/60000 (5%)]\tLoss: 1.465965\tAccuracy: 97.93%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2800/60000 (5%)]\tLoss: 1.504168\tAccuracy: 97.89%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2850/60000 (5%)]\tLoss: 1.483834\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2900/60000 (5%)]\tLoss: 1.472140\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [2950/60000 (5%)]\tLoss: 1.481255\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3000/60000 (5%)]\tLoss: 1.497229\tAccuracy: 97.87%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3050/60000 (5%)]\tLoss: 1.461165\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3100/60000 (5%)]\tLoss: 1.479140\tAccuracy: 97.90%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3150/60000 (5%)]\tLoss: 1.499768\tAccuracy: 97.88%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3200/60000 (5%)]\tLoss: 1.461151\tAccuracy: 97.91%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3250/60000 (5%)]\tLoss: 1.461904\tAccuracy: 97.94%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3300/60000 (6%)]\tLoss: 1.478301\tAccuracy: 97.94%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3350/60000 (6%)]\tLoss: 1.481151\tAccuracy: 97.94%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3400/60000 (6%)]\tLoss: 1.461274\tAccuracy: 97.97%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3450/60000 (6%)]\tLoss: 1.500358\tAccuracy: 97.94%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3500/60000 (6%)]\tLoss: 1.481145\tAccuracy: 97.94%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3550/60000 (6%)]\tLoss: 1.466681\tAccuracy: 97.97%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3600/60000 (6%)]\tLoss: 1.461151\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3650/60000 (6%)]\tLoss: 1.483199\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3700/60000 (6%)]\tLoss: 1.461187\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3750/60000 (6%)]\tLoss: 1.488784\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3800/60000 (6%)]\tLoss: 1.469920\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3850/60000 (6%)]\tLoss: 1.494893\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3900/60000 (6%)]\tLoss: 1.481150\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [3950/60000 (7%)]\tLoss: 1.484555\tAccuracy: 98.03%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4000/60000 (7%)]\tLoss: 1.462642\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4050/60000 (7%)]\tLoss: 1.461195\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4100/60000 (7%)]\tLoss: 1.538879\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4150/60000 (7%)]\tLoss: 1.481154\tAccuracy: 98.00%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4200/60000 (7%)]\tLoss: 1.461675\tAccuracy: 98.02%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4250/60000 (7%)]\tLoss: 1.461649\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4300/60000 (7%)]\tLoss: 1.482815\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4350/60000 (7%)]\tLoss: 1.482057\tAccuracy: 98.05%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4400/60000 (7%)]\tLoss: 1.461151\tAccuracy: 98.07%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4450/60000 (7%)]\tLoss: 1.461151\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4500/60000 (8%)]\tLoss: 1.481447\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4550/60000 (8%)]\tLoss: 1.481151\tAccuracy: 98.09%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4600/60000 (8%)]\tLoss: 1.461183\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4650/60000 (8%)]\tLoss: 1.463485\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4700/60000 (8%)]\tLoss: 1.461151\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4750/60000 (8%)]\tLoss: 1.481498\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4800/60000 (8%)]\tLoss: 1.461151\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4850/60000 (8%)]\tLoss: 1.499471\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4900/60000 (8%)]\tLoss: 1.500993\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [4950/60000 (8%)]\tLoss: 1.461162\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5000/60000 (8%)]\tLoss: 1.481160\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5050/60000 (8%)]\tLoss: 1.461895\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5100/60000 (8%)]\tLoss: 1.508759\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5150/60000 (9%)]\tLoss: 1.474414\tAccuracy: 98.12%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5200/60000 (9%)]\tLoss: 1.491004\tAccuracy: 98.10%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5250/60000 (9%)]\tLoss: 1.461158\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5300/60000 (9%)]\tLoss: 1.483130\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5350/60000 (9%)]\tLoss: 1.470239\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5400/60000 (9%)]\tLoss: 1.501070\tAccuracy: 98.11%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5450/60000 (9%)]\tLoss: 1.461174\tAccuracy: 98.13%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5500/60000 (9%)]\tLoss: 1.461177\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5550/60000 (9%)]\tLoss: 1.461182\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5600/60000 (9%)]\tLoss: 1.461151\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5650/60000 (9%)]\tLoss: 1.510877\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5700/60000 (10%)]\tLoss: 1.461152\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5750/60000 (10%)]\tLoss: 1.480859\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5800/60000 (10%)]\tLoss: 1.491451\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5850/60000 (10%)]\tLoss: 1.480603\tAccuracy: 98.14%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5900/60000 (10%)]\tLoss: 1.461154\tAccuracy: 98.15%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [5950/60000 (10%)]\tLoss: 1.461222\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6000/60000 (10%)]\tLoss: 1.478560\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6050/60000 (10%)]\tLoss: 1.476162\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6100/60000 (10%)]\tLoss: 1.480890\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6150/60000 (10%)]\tLoss: 1.476452\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6200/60000 (10%)]\tLoss: 1.480419\tAccuracy: 98.16%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6250/60000 (10%)]\tLoss: 1.461927\tAccuracy: 98.17%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6300/60000 (10%)]\tLoss: 1.461151\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6350/60000 (11%)]\tLoss: 1.481712\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6400/60000 (11%)]\tLoss: 1.462376\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6450/60000 (11%)]\tLoss: 1.496988\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6500/60000 (11%)]\tLoss: 1.481201\tAccuracy: 98.18%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6550/60000 (11%)]\tLoss: 1.461254\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6600/60000 (11%)]\tLoss: 1.481151\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6650/60000 (11%)]\tLoss: 1.481094\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6700/60000 (11%)]\tLoss: 1.488610\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6750/60000 (11%)]\tLoss: 1.473078\tAccuracy: 98.19%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6800/60000 (11%)]\tLoss: 1.465228\tAccuracy: 98.20%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6850/60000 (11%)]\tLoss: 1.461862\tAccuracy: 98.22%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6900/60000 (12%)]\tLoss: 1.462308\tAccuracy: 98.23%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [6950/60000 (12%)]\tLoss: 1.461470\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7000/60000 (12%)]\tLoss: 1.461157\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7050/60000 (12%)]\tLoss: 1.481143\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7100/60000 (12%)]\tLoss: 1.469814\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7150/60000 (12%)]\tLoss: 1.468654\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7200/60000 (12%)]\tLoss: 1.481087\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7250/60000 (12%)]\tLoss: 1.478234\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7300/60000 (12%)]\tLoss: 1.461652\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7350/60000 (12%)]\tLoss: 1.488812\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7400/60000 (12%)]\tLoss: 1.481796\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7450/60000 (12%)]\tLoss: 1.461157\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7500/60000 (12%)]\tLoss: 1.493344\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7550/60000 (13%)]\tLoss: 1.461151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7600/60000 (13%)]\tLoss: 1.497934\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7650/60000 (13%)]\tLoss: 1.461778\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7700/60000 (13%)]\tLoss: 1.515718\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7750/60000 (13%)]\tLoss: 1.461168\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7800/60000 (13%)]\tLoss: 1.488646\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7850/60000 (13%)]\tLoss: 1.482954\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7900/60000 (13%)]\tLoss: 1.466334\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [7950/60000 (13%)]\tLoss: 1.487322\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8000/60000 (13%)]\tLoss: 1.492968\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8050/60000 (13%)]\tLoss: 1.461364\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8100/60000 (14%)]\tLoss: 1.461164\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8150/60000 (14%)]\tLoss: 1.464479\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8200/60000 (14%)]\tLoss: 1.472933\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8250/60000 (14%)]\tLoss: 1.461157\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8300/60000 (14%)]\tLoss: 1.481266\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8350/60000 (14%)]\tLoss: 1.501138\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8400/60000 (14%)]\tLoss: 1.479591\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8450/60000 (14%)]\tLoss: 1.461524\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8500/60000 (14%)]\tLoss: 1.519849\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8550/60000 (14%)]\tLoss: 1.475631\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8600/60000 (14%)]\tLoss: 1.481151\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8650/60000 (14%)]\tLoss: 1.461183\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8700/60000 (14%)]\tLoss: 1.480984\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8750/60000 (15%)]\tLoss: 1.481328\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8800/60000 (15%)]\tLoss: 1.461151\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8850/60000 (15%)]\tLoss: 1.481137\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8900/60000 (15%)]\tLoss: 1.461162\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [8950/60000 (15%)]\tLoss: 1.461379\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9000/60000 (15%)]\tLoss: 1.479402\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9050/60000 (15%)]\tLoss: 1.461179\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9100/60000 (15%)]\tLoss: 1.461151\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9150/60000 (15%)]\tLoss: 1.493135\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9200/60000 (15%)]\tLoss: 1.461157\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9250/60000 (15%)]\tLoss: 1.508025\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9300/60000 (16%)]\tLoss: 1.483691\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9350/60000 (16%)]\tLoss: 1.461152\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9400/60000 (16%)]\tLoss: 1.461156\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9450/60000 (16%)]\tLoss: 1.463403\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9500/60000 (16%)]\tLoss: 1.461402\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9550/60000 (16%)]\tLoss: 1.481149\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9600/60000 (16%)]\tLoss: 1.461167\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9650/60000 (16%)]\tLoss: 1.461245\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9700/60000 (16%)]\tLoss: 1.500502\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9750/60000 (16%)]\tLoss: 1.461153\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9800/60000 (16%)]\tLoss: 1.461264\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9850/60000 (16%)]\tLoss: 1.496873\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9900/60000 (16%)]\tLoss: 1.461159\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [9950/60000 (17%)]\tLoss: 1.474879\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10000/60000 (17%)]\tLoss: 1.528451\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10050/60000 (17%)]\tLoss: 1.501087\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10100/60000 (17%)]\tLoss: 1.462951\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10150/60000 (17%)]\tLoss: 1.481199\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10200/60000 (17%)]\tLoss: 1.466773\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10250/60000 (17%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10300/60000 (17%)]\tLoss: 1.480184\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10350/60000 (17%)]\tLoss: 1.501727\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10400/60000 (17%)]\tLoss: 1.482322\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10450/60000 (17%)]\tLoss: 1.476990\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10500/60000 (18%)]\tLoss: 1.484046\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10550/60000 (18%)]\tLoss: 1.481155\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10600/60000 (18%)]\tLoss: 1.479595\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10650/60000 (18%)]\tLoss: 1.471066\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10700/60000 (18%)]\tLoss: 1.500993\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10750/60000 (18%)]\tLoss: 1.481168\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10800/60000 (18%)]\tLoss: 1.463122\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10850/60000 (18%)]\tLoss: 1.501143\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10900/60000 (18%)]\tLoss: 1.477316\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [10950/60000 (18%)]\tLoss: 1.461366\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11000/60000 (18%)]\tLoss: 1.480954\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11050/60000 (18%)]\tLoss: 1.501183\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11100/60000 (18%)]\tLoss: 1.492002\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11150/60000 (19%)]\tLoss: 1.470585\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11200/60000 (19%)]\tLoss: 1.500658\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11250/60000 (19%)]\tLoss: 1.481151\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11300/60000 (19%)]\tLoss: 1.466819\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11350/60000 (19%)]\tLoss: 1.499707\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11400/60000 (19%)]\tLoss: 1.461208\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11450/60000 (19%)]\tLoss: 1.461191\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11500/60000 (19%)]\tLoss: 1.478868\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11550/60000 (19%)]\tLoss: 1.489505\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11600/60000 (19%)]\tLoss: 1.465135\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11650/60000 (19%)]\tLoss: 1.461151\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11700/60000 (20%)]\tLoss: 1.461151\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11750/60000 (20%)]\tLoss: 1.461151\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11800/60000 (20%)]\tLoss: 1.481152\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11850/60000 (20%)]\tLoss: 1.504624\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11900/60000 (20%)]\tLoss: 1.461271\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [11950/60000 (20%)]\tLoss: 1.484792\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12000/60000 (20%)]\tLoss: 1.492721\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12050/60000 (20%)]\tLoss: 1.466276\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12100/60000 (20%)]\tLoss: 1.481144\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12150/60000 (20%)]\tLoss: 1.481197\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12200/60000 (20%)]\tLoss: 1.461154\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12250/60000 (20%)]\tLoss: 1.461151\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12300/60000 (20%)]\tLoss: 1.480998\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12350/60000 (21%)]\tLoss: 1.461398\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12400/60000 (21%)]\tLoss: 1.481149\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12450/60000 (21%)]\tLoss: 1.461151\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12500/60000 (21%)]\tLoss: 1.473642\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12550/60000 (21%)]\tLoss: 1.501151\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12600/60000 (21%)]\tLoss: 1.461171\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12650/60000 (21%)]\tLoss: 1.501205\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12700/60000 (21%)]\tLoss: 1.520312\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12750/60000 (21%)]\tLoss: 1.461151\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12800/60000 (21%)]\tLoss: 1.476646\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12850/60000 (21%)]\tLoss: 1.461722\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12900/60000 (22%)]\tLoss: 1.461155\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [12950/60000 (22%)]\tLoss: 1.481152\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13000/60000 (22%)]\tLoss: 1.481151\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13050/60000 (22%)]\tLoss: 1.461158\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13100/60000 (22%)]\tLoss: 1.486193\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13150/60000 (22%)]\tLoss: 1.461346\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13200/60000 (22%)]\tLoss: 1.481406\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13250/60000 (22%)]\tLoss: 1.496172\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13300/60000 (22%)]\tLoss: 1.461162\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13350/60000 (22%)]\tLoss: 1.461155\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13400/60000 (22%)]\tLoss: 1.489745\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13450/60000 (22%)]\tLoss: 1.467071\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13500/60000 (22%)]\tLoss: 1.479876\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13550/60000 (23%)]\tLoss: 1.461403\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13600/60000 (23%)]\tLoss: 1.538656\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13650/60000 (23%)]\tLoss: 1.461156\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13700/60000 (23%)]\tLoss: 1.461792\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13750/60000 (23%)]\tLoss: 1.481151\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13800/60000 (23%)]\tLoss: 1.461151\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13850/60000 (23%)]\tLoss: 1.461152\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13900/60000 (23%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [13950/60000 (23%)]\tLoss: 1.481097\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14000/60000 (23%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14050/60000 (23%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14100/60000 (24%)]\tLoss: 1.461194\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14150/60000 (24%)]\tLoss: 1.481293\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14200/60000 (24%)]\tLoss: 1.481144\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14250/60000 (24%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14300/60000 (24%)]\tLoss: 1.474343\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14350/60000 (24%)]\tLoss: 1.461259\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14400/60000 (24%)]\tLoss: 1.520098\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14450/60000 (24%)]\tLoss: 1.481321\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14500/60000 (24%)]\tLoss: 1.462817\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14550/60000 (24%)]\tLoss: 1.477838\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14600/60000 (24%)]\tLoss: 1.497346\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14650/60000 (24%)]\tLoss: 1.481150\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14700/60000 (24%)]\tLoss: 1.480845\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14750/60000 (25%)]\tLoss: 1.480471\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14800/60000 (25%)]\tLoss: 1.501616\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14850/60000 (25%)]\tLoss: 1.463256\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14900/60000 (25%)]\tLoss: 1.475547\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [14950/60000 (25%)]\tLoss: 1.481576\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15000/60000 (25%)]\tLoss: 1.481145\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15050/60000 (25%)]\tLoss: 1.461756\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15100/60000 (25%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15150/60000 (25%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15200/60000 (25%)]\tLoss: 1.481321\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15250/60000 (25%)]\tLoss: 1.461152\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15300/60000 (26%)]\tLoss: 1.490816\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15350/60000 (26%)]\tLoss: 1.480850\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15400/60000 (26%)]\tLoss: 1.481115\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15450/60000 (26%)]\tLoss: 1.501143\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15500/60000 (26%)]\tLoss: 1.461157\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15550/60000 (26%)]\tLoss: 1.480549\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15600/60000 (26%)]\tLoss: 1.481154\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15650/60000 (26%)]\tLoss: 1.480779\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15700/60000 (26%)]\tLoss: 1.481546\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15750/60000 (26%)]\tLoss: 1.481260\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15800/60000 (26%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15850/60000 (26%)]\tLoss: 1.464535\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15900/60000 (26%)]\tLoss: 1.481128\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [15950/60000 (27%)]\tLoss: 1.479077\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16000/60000 (27%)]\tLoss: 1.461165\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16050/60000 (27%)]\tLoss: 1.463006\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16100/60000 (27%)]\tLoss: 1.462719\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16150/60000 (27%)]\tLoss: 1.462483\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16200/60000 (27%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16250/60000 (27%)]\tLoss: 1.501060\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16300/60000 (27%)]\tLoss: 1.465150\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16350/60000 (27%)]\tLoss: 1.498546\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16400/60000 (27%)]\tLoss: 1.462169\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16450/60000 (27%)]\tLoss: 1.501123\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16500/60000 (28%)]\tLoss: 1.463915\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16550/60000 (28%)]\tLoss: 1.500021\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16600/60000 (28%)]\tLoss: 1.461153\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16650/60000 (28%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16700/60000 (28%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16750/60000 (28%)]\tLoss: 1.481156\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16800/60000 (28%)]\tLoss: 1.461178\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16850/60000 (28%)]\tLoss: 1.461151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16900/60000 (28%)]\tLoss: 1.461152\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [16950/60000 (28%)]\tLoss: 1.500559\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17000/60000 (28%)]\tLoss: 1.481130\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17050/60000 (28%)]\tLoss: 1.461151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17100/60000 (28%)]\tLoss: 1.461861\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17150/60000 (29%)]\tLoss: 1.461151\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17200/60000 (29%)]\tLoss: 1.461342\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17250/60000 (29%)]\tLoss: 1.481153\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17300/60000 (29%)]\tLoss: 1.492007\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17350/60000 (29%)]\tLoss: 1.461630\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17400/60000 (29%)]\tLoss: 1.501151\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17450/60000 (29%)]\tLoss: 1.480452\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17500/60000 (29%)]\tLoss: 1.518031\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17550/60000 (29%)]\tLoss: 1.462716\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17600/60000 (29%)]\tLoss: 1.502489\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17650/60000 (29%)]\tLoss: 1.481151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17700/60000 (30%)]\tLoss: 1.461167\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17750/60000 (30%)]\tLoss: 1.482024\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17800/60000 (30%)]\tLoss: 1.463697\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17850/60000 (30%)]\tLoss: 1.461151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17900/60000 (30%)]\tLoss: 1.461178\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [17950/60000 (30%)]\tLoss: 1.461151\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18000/60000 (30%)]\tLoss: 1.484488\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18050/60000 (30%)]\tLoss: 1.461165\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18100/60000 (30%)]\tLoss: 1.461154\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18150/60000 (30%)]\tLoss: 1.461152\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18200/60000 (30%)]\tLoss: 1.479572\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18250/60000 (30%)]\tLoss: 1.461160\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18300/60000 (30%)]\tLoss: 1.461181\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18350/60000 (31%)]\tLoss: 1.481150\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18400/60000 (31%)]\tLoss: 1.481150\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18450/60000 (31%)]\tLoss: 1.480489\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18500/60000 (31%)]\tLoss: 1.461155\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18550/60000 (31%)]\tLoss: 1.482255\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18600/60000 (31%)]\tLoss: 1.481151\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18650/60000 (31%)]\tLoss: 1.468846\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18700/60000 (31%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18750/60000 (31%)]\tLoss: 1.461195\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18800/60000 (31%)]\tLoss: 1.481119\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18850/60000 (31%)]\tLoss: 1.464263\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18900/60000 (32%)]\tLoss: 1.499163\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [18950/60000 (32%)]\tLoss: 1.481225\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19000/60000 (32%)]\tLoss: 1.461151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19050/60000 (32%)]\tLoss: 1.481092\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19100/60000 (32%)]\tLoss: 1.464230\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19150/60000 (32%)]\tLoss: 1.461473\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19200/60000 (32%)]\tLoss: 1.463201\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19250/60000 (32%)]\tLoss: 1.461151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19300/60000 (32%)]\tLoss: 1.503630\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19350/60000 (32%)]\tLoss: 1.493823\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19400/60000 (32%)]\tLoss: 1.461164\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19450/60000 (32%)]\tLoss: 1.461506\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19500/60000 (32%)]\tLoss: 1.461173\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19550/60000 (33%)]\tLoss: 1.480184\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19600/60000 (33%)]\tLoss: 1.480313\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19650/60000 (33%)]\tLoss: 1.499571\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19700/60000 (33%)]\tLoss: 1.501151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19750/60000 (33%)]\tLoss: 1.461151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19800/60000 (33%)]\tLoss: 1.463104\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19850/60000 (33%)]\tLoss: 1.461222\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19900/60000 (33%)]\tLoss: 1.461153\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [19950/60000 (33%)]\tLoss: 1.461151\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20000/60000 (33%)]\tLoss: 1.461174\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20050/60000 (33%)]\tLoss: 1.499988\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20100/60000 (34%)]\tLoss: 1.490138\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20150/60000 (34%)]\tLoss: 1.495293\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20200/60000 (34%)]\tLoss: 1.481146\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20250/60000 (34%)]\tLoss: 1.461651\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20300/60000 (34%)]\tLoss: 1.462872\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20350/60000 (34%)]\tLoss: 1.469996\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20400/60000 (34%)]\tLoss: 1.481962\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20450/60000 (34%)]\tLoss: 1.463378\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20500/60000 (34%)]\tLoss: 1.461151\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20550/60000 (34%)]\tLoss: 1.495574\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20600/60000 (34%)]\tLoss: 1.465060\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20650/60000 (34%)]\tLoss: 1.465993\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20700/60000 (34%)]\tLoss: 1.491052\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20750/60000 (35%)]\tLoss: 1.481140\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20800/60000 (35%)]\tLoss: 1.461220\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20850/60000 (35%)]\tLoss: 1.461187\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20900/60000 (35%)]\tLoss: 1.481127\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [20950/60000 (35%)]\tLoss: 1.461151\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21000/60000 (35%)]\tLoss: 1.461153\tAccuracy: 98.48%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21050/60000 (35%)]\tLoss: 1.500214\tAccuracy: 98.47%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21100/60000 (35%)]\tLoss: 1.501145\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21150/60000 (35%)]\tLoss: 1.472518\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21200/60000 (35%)]\tLoss: 1.482671\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21250/60000 (35%)]\tLoss: 1.474854\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21300/60000 (36%)]\tLoss: 1.500104\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21350/60000 (36%)]\tLoss: 1.481165\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21400/60000 (36%)]\tLoss: 1.470014\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21450/60000 (36%)]\tLoss: 1.481182\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21500/60000 (36%)]\tLoss: 1.481151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21550/60000 (36%)]\tLoss: 1.461151\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21600/60000 (36%)]\tLoss: 1.481152\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21650/60000 (36%)]\tLoss: 1.462098\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21700/60000 (36%)]\tLoss: 1.501646\tAccuracy: 98.46%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21750/60000 (36%)]\tLoss: 1.482243\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21800/60000 (36%)]\tLoss: 1.506497\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21850/60000 (36%)]\tLoss: 1.461151\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21900/60000 (36%)]\tLoss: 1.501868\tAccuracy: 98.45%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [21950/60000 (37%)]\tLoss: 1.501045\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22000/60000 (37%)]\tLoss: 1.480894\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22050/60000 (37%)]\tLoss: 1.500797\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22100/60000 (37%)]\tLoss: 1.481125\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22150/60000 (37%)]\tLoss: 1.481269\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22200/60000 (37%)]\tLoss: 1.461151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22250/60000 (37%)]\tLoss: 1.462458\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22300/60000 (37%)]\tLoss: 1.481635\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22350/60000 (37%)]\tLoss: 1.481079\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22400/60000 (37%)]\tLoss: 1.502254\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22450/60000 (37%)]\tLoss: 1.461151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22500/60000 (38%)]\tLoss: 1.461769\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22550/60000 (38%)]\tLoss: 1.481154\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22600/60000 (38%)]\tLoss: 1.511675\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22650/60000 (38%)]\tLoss: 1.481000\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22700/60000 (38%)]\tLoss: 1.483544\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22750/60000 (38%)]\tLoss: 1.471647\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22800/60000 (38%)]\tLoss: 1.461153\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22850/60000 (38%)]\tLoss: 1.461151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22900/60000 (38%)]\tLoss: 1.510338\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [22950/60000 (38%)]\tLoss: 1.481151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23000/60000 (38%)]\tLoss: 1.461153\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23050/60000 (38%)]\tLoss: 1.513511\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23100/60000 (38%)]\tLoss: 1.481585\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23150/60000 (39%)]\tLoss: 1.461151\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23200/60000 (39%)]\tLoss: 1.481106\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23250/60000 (39%)]\tLoss: 1.478436\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23300/60000 (39%)]\tLoss: 1.501149\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23350/60000 (39%)]\tLoss: 1.461154\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23400/60000 (39%)]\tLoss: 1.461178\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23450/60000 (39%)]\tLoss: 1.470998\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23500/60000 (39%)]\tLoss: 1.461214\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23550/60000 (39%)]\tLoss: 1.461151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23600/60000 (39%)]\tLoss: 1.504625\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23650/60000 (39%)]\tLoss: 1.480404\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23700/60000 (40%)]\tLoss: 1.480219\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23750/60000 (40%)]\tLoss: 1.481579\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23800/60000 (40%)]\tLoss: 1.479925\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23850/60000 (40%)]\tLoss: 1.481356\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23900/60000 (40%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [23950/60000 (40%)]\tLoss: 1.501126\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24000/60000 (40%)]\tLoss: 1.463527\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24050/60000 (40%)]\tLoss: 1.461357\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24100/60000 (40%)]\tLoss: 1.505963\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24150/60000 (40%)]\tLoss: 1.461152\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24200/60000 (40%)]\tLoss: 1.461403\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24250/60000 (40%)]\tLoss: 1.468810\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24300/60000 (40%)]\tLoss: 1.481151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24350/60000 (41%)]\tLoss: 1.490762\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24400/60000 (41%)]\tLoss: 1.481152\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24450/60000 (41%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24500/60000 (41%)]\tLoss: 1.461160\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24550/60000 (41%)]\tLoss: 1.477116\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24600/60000 (41%)]\tLoss: 1.461181\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24650/60000 (41%)]\tLoss: 1.472762\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24700/60000 (41%)]\tLoss: 1.461164\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24750/60000 (41%)]\tLoss: 1.481315\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24800/60000 (41%)]\tLoss: 1.481152\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24850/60000 (41%)]\tLoss: 1.486144\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24900/60000 (42%)]\tLoss: 1.461151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [24950/60000 (42%)]\tLoss: 1.461375\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25000/60000 (42%)]\tLoss: 1.481135\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25050/60000 (42%)]\tLoss: 1.481385\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25100/60000 (42%)]\tLoss: 1.491441\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25150/60000 (42%)]\tLoss: 1.489539\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25200/60000 (42%)]\tLoss: 1.481151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25250/60000 (42%)]\tLoss: 1.461153\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25300/60000 (42%)]\tLoss: 1.481151\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25350/60000 (42%)]\tLoss: 1.481168\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25400/60000 (42%)]\tLoss: 1.480592\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25450/60000 (42%)]\tLoss: 1.501127\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25500/60000 (42%)]\tLoss: 1.461166\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25550/60000 (43%)]\tLoss: 1.475507\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25600/60000 (43%)]\tLoss: 1.476414\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25650/60000 (43%)]\tLoss: 1.481172\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25700/60000 (43%)]\tLoss: 1.461260\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25750/60000 (43%)]\tLoss: 1.461152\tAccuracy: 98.44%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25800/60000 (43%)]\tLoss: 1.518506\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25850/60000 (43%)]\tLoss: 1.461995\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25900/60000 (43%)]\tLoss: 1.545833\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [25950/60000 (43%)]\tLoss: 1.486903\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26000/60000 (43%)]\tLoss: 1.461151\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26050/60000 (43%)]\tLoss: 1.499754\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26100/60000 (44%)]\tLoss: 1.468905\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26150/60000 (44%)]\tLoss: 1.500213\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26200/60000 (44%)]\tLoss: 1.461545\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26250/60000 (44%)]\tLoss: 1.461312\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26300/60000 (44%)]\tLoss: 1.481171\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26350/60000 (44%)]\tLoss: 1.464637\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26400/60000 (44%)]\tLoss: 1.461223\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26450/60000 (44%)]\tLoss: 1.489241\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26500/60000 (44%)]\tLoss: 1.520425\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26550/60000 (44%)]\tLoss: 1.478630\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26600/60000 (44%)]\tLoss: 1.516934\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26650/60000 (44%)]\tLoss: 1.500812\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26700/60000 (44%)]\tLoss: 1.480657\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26750/60000 (45%)]\tLoss: 1.481215\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26800/60000 (45%)]\tLoss: 1.461211\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26850/60000 (45%)]\tLoss: 1.481114\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26900/60000 (45%)]\tLoss: 1.481036\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [26950/60000 (45%)]\tLoss: 1.461153\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27000/60000 (45%)]\tLoss: 1.479651\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27050/60000 (45%)]\tLoss: 1.462850\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27100/60000 (45%)]\tLoss: 1.462771\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27150/60000 (45%)]\tLoss: 1.461333\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27200/60000 (45%)]\tLoss: 1.461151\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27250/60000 (45%)]\tLoss: 1.461169\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27300/60000 (46%)]\tLoss: 1.498601\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27350/60000 (46%)]\tLoss: 1.461187\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27400/60000 (46%)]\tLoss: 1.461159\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27450/60000 (46%)]\tLoss: 1.518366\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27500/60000 (46%)]\tLoss: 1.500934\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27550/60000 (46%)]\tLoss: 1.481072\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27600/60000 (46%)]\tLoss: 1.473575\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27650/60000 (46%)]\tLoss: 1.500633\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27700/60000 (46%)]\tLoss: 1.478090\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27750/60000 (46%)]\tLoss: 1.461571\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27800/60000 (46%)]\tLoss: 1.462642\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27850/60000 (46%)]\tLoss: 1.461290\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27900/60000 (46%)]\tLoss: 1.492173\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [27950/60000 (47%)]\tLoss: 1.481145\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28000/60000 (47%)]\tLoss: 1.481150\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28050/60000 (47%)]\tLoss: 1.496531\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28100/60000 (47%)]\tLoss: 1.461189\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28150/60000 (47%)]\tLoss: 1.461151\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28200/60000 (47%)]\tLoss: 1.480092\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28250/60000 (47%)]\tLoss: 1.461152\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28300/60000 (47%)]\tLoss: 1.500623\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28350/60000 (47%)]\tLoss: 1.461151\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28400/60000 (47%)]\tLoss: 1.481133\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28450/60000 (47%)]\tLoss: 1.502185\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28500/60000 (48%)]\tLoss: 1.461153\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28550/60000 (48%)]\tLoss: 1.478116\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28600/60000 (48%)]\tLoss: 1.461166\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28650/60000 (48%)]\tLoss: 1.481150\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28700/60000 (48%)]\tLoss: 1.461493\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28750/60000 (48%)]\tLoss: 1.494056\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28800/60000 (48%)]\tLoss: 1.480863\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28850/60000 (48%)]\tLoss: 1.481219\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28900/60000 (48%)]\tLoss: 1.461151\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [28950/60000 (48%)]\tLoss: 1.481854\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29000/60000 (48%)]\tLoss: 1.461708\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29050/60000 (48%)]\tLoss: 1.461158\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29100/60000 (48%)]\tLoss: 1.461158\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29150/60000 (49%)]\tLoss: 1.500732\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29200/60000 (49%)]\tLoss: 1.481153\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29250/60000 (49%)]\tLoss: 1.461153\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29300/60000 (49%)]\tLoss: 1.461390\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29350/60000 (49%)]\tLoss: 1.468361\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29400/60000 (49%)]\tLoss: 1.481958\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29450/60000 (49%)]\tLoss: 1.480614\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29500/60000 (49%)]\tLoss: 1.461152\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29550/60000 (49%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29600/60000 (49%)]\tLoss: 1.469757\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29650/60000 (49%)]\tLoss: 1.481152\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29700/60000 (50%)]\tLoss: 1.482797\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29750/60000 (50%)]\tLoss: 1.493359\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29800/60000 (50%)]\tLoss: 1.492382\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29850/60000 (50%)]\tLoss: 1.464059\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29900/60000 (50%)]\tLoss: 1.463929\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [29950/60000 (50%)]\tLoss: 1.461521\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30000/60000 (50%)]\tLoss: 1.486818\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30050/60000 (50%)]\tLoss: 1.461263\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30100/60000 (50%)]\tLoss: 1.481149\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30150/60000 (50%)]\tLoss: 1.466979\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30200/60000 (50%)]\tLoss: 1.466563\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30250/60000 (50%)]\tLoss: 1.481181\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30300/60000 (50%)]\tLoss: 1.481156\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30350/60000 (51%)]\tLoss: 1.467710\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30400/60000 (51%)]\tLoss: 1.503175\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30450/60000 (51%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30500/60000 (51%)]\tLoss: 1.501219\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30550/60000 (51%)]\tLoss: 1.501391\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30600/60000 (51%)]\tLoss: 1.487738\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30650/60000 (51%)]\tLoss: 1.481169\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30700/60000 (51%)]\tLoss: 1.461162\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30750/60000 (51%)]\tLoss: 1.461151\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30800/60000 (51%)]\tLoss: 1.488040\tAccuracy: 98.43%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30850/60000 (51%)]\tLoss: 1.501183\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30900/60000 (52%)]\tLoss: 1.499891\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [30950/60000 (52%)]\tLoss: 1.481583\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31000/60000 (52%)]\tLoss: 1.461262\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31050/60000 (52%)]\tLoss: 1.503452\tAccuracy: 98.42%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31100/60000 (52%)]\tLoss: 1.501861\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31150/60000 (52%)]\tLoss: 1.523699\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31200/60000 (52%)]\tLoss: 1.479846\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31250/60000 (52%)]\tLoss: 1.481151\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31300/60000 (52%)]\tLoss: 1.480731\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31350/60000 (52%)]\tLoss: 1.519799\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31400/60000 (52%)]\tLoss: 1.461152\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31450/60000 (52%)]\tLoss: 1.461151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31500/60000 (52%)]\tLoss: 1.485855\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31550/60000 (53%)]\tLoss: 1.461158\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31600/60000 (53%)]\tLoss: 1.478589\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31650/60000 (53%)]\tLoss: 1.481142\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31700/60000 (53%)]\tLoss: 1.461166\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31750/60000 (53%)]\tLoss: 1.461155\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31800/60000 (53%)]\tLoss: 1.475940\tAccuracy: 98.41%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31850/60000 (53%)]\tLoss: 1.494465\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31900/60000 (53%)]\tLoss: 1.574389\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [31950/60000 (53%)]\tLoss: 1.505006\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32000/60000 (53%)]\tLoss: 1.481150\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32050/60000 (53%)]\tLoss: 1.461579\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32100/60000 (54%)]\tLoss: 1.481151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32150/60000 (54%)]\tLoss: 1.461156\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32200/60000 (54%)]\tLoss: 1.461170\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32250/60000 (54%)]\tLoss: 1.521113\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32300/60000 (54%)]\tLoss: 1.461697\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32350/60000 (54%)]\tLoss: 1.481250\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32400/60000 (54%)]\tLoss: 1.479134\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32450/60000 (54%)]\tLoss: 1.481158\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32500/60000 (54%)]\tLoss: 1.493533\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32550/60000 (54%)]\tLoss: 1.461440\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32600/60000 (54%)]\tLoss: 1.461575\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32650/60000 (54%)]\tLoss: 1.489191\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32700/60000 (54%)]\tLoss: 1.520803\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32750/60000 (55%)]\tLoss: 1.500841\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32800/60000 (55%)]\tLoss: 1.520620\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32850/60000 (55%)]\tLoss: 1.479155\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32900/60000 (55%)]\tLoss: 1.481918\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [32950/60000 (55%)]\tLoss: 1.471350\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33000/60000 (55%)]\tLoss: 1.481131\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33050/60000 (55%)]\tLoss: 1.461152\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33100/60000 (55%)]\tLoss: 1.481133\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33150/60000 (55%)]\tLoss: 1.471800\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33200/60000 (55%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33250/60000 (55%)]\tLoss: 1.497430\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33300/60000 (56%)]\tLoss: 1.491475\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33350/60000 (56%)]\tLoss: 1.475777\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33400/60000 (56%)]\tLoss: 1.461183\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33450/60000 (56%)]\tLoss: 1.473195\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33500/60000 (56%)]\tLoss: 1.481122\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33550/60000 (56%)]\tLoss: 1.480388\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33600/60000 (56%)]\tLoss: 1.481167\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33650/60000 (56%)]\tLoss: 1.461151\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33700/60000 (56%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33750/60000 (56%)]\tLoss: 1.491417\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33800/60000 (56%)]\tLoss: 1.481159\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33850/60000 (56%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33900/60000 (56%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [33950/60000 (57%)]\tLoss: 1.462084\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34000/60000 (57%)]\tLoss: 1.481351\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34050/60000 (57%)]\tLoss: 1.461167\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34100/60000 (57%)]\tLoss: 1.500487\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34150/60000 (57%)]\tLoss: 1.476589\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34200/60000 (57%)]\tLoss: 1.500745\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34250/60000 (57%)]\tLoss: 1.462319\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34300/60000 (57%)]\tLoss: 1.471850\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34350/60000 (57%)]\tLoss: 1.499517\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34400/60000 (57%)]\tLoss: 1.481471\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34450/60000 (57%)]\tLoss: 1.482160\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34500/60000 (58%)]\tLoss: 1.461151\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34550/60000 (58%)]\tLoss: 1.475580\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34600/60000 (58%)]\tLoss: 1.481162\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34650/60000 (58%)]\tLoss: 1.481739\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34700/60000 (58%)]\tLoss: 1.481151\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34750/60000 (58%)]\tLoss: 1.461151\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34800/60000 (58%)]\tLoss: 1.501152\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34850/60000 (58%)]\tLoss: 1.461157\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34900/60000 (58%)]\tLoss: 1.481222\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [34950/60000 (58%)]\tLoss: 1.461237\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35000/60000 (58%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35050/60000 (58%)]\tLoss: 1.479705\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35100/60000 (58%)]\tLoss: 1.513906\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35150/60000 (59%)]\tLoss: 1.512035\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35200/60000 (59%)]\tLoss: 1.480897\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35250/60000 (59%)]\tLoss: 1.461177\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35300/60000 (59%)]\tLoss: 1.461151\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35350/60000 (59%)]\tLoss: 1.480927\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35400/60000 (59%)]\tLoss: 1.464635\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35450/60000 (59%)]\tLoss: 1.481728\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35500/60000 (59%)]\tLoss: 1.461151\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35550/60000 (59%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35600/60000 (59%)]\tLoss: 1.464858\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35650/60000 (59%)]\tLoss: 1.477586\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35700/60000 (60%)]\tLoss: 1.481137\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35750/60000 (60%)]\tLoss: 1.461237\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35800/60000 (60%)]\tLoss: 1.481151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35850/60000 (60%)]\tLoss: 1.481151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35900/60000 (60%)]\tLoss: 1.461181\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [35950/60000 (60%)]\tLoss: 1.481627\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36000/60000 (60%)]\tLoss: 1.481160\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36050/60000 (60%)]\tLoss: 1.461152\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36100/60000 (60%)]\tLoss: 1.475466\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36150/60000 (60%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36200/60000 (60%)]\tLoss: 1.492180\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36250/60000 (60%)]\tLoss: 1.465862\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36300/60000 (60%)]\tLoss: 1.481789\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36350/60000 (61%)]\tLoss: 1.481598\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36400/60000 (61%)]\tLoss: 1.461520\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36450/60000 (61%)]\tLoss: 1.505041\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36500/60000 (61%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36550/60000 (61%)]\tLoss: 1.500455\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36600/60000 (61%)]\tLoss: 1.462488\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36650/60000 (61%)]\tLoss: 1.479941\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36700/60000 (61%)]\tLoss: 1.481153\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36750/60000 (61%)]\tLoss: 1.461155\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36800/60000 (61%)]\tLoss: 1.461217\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36850/60000 (61%)]\tLoss: 1.461165\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36900/60000 (62%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [36950/60000 (62%)]\tLoss: 1.461154\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37000/60000 (62%)]\tLoss: 1.481247\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37050/60000 (62%)]\tLoss: 1.501158\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37100/60000 (62%)]\tLoss: 1.461309\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37150/60000 (62%)]\tLoss: 1.461158\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37200/60000 (62%)]\tLoss: 1.481658\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37250/60000 (62%)]\tLoss: 1.481849\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37300/60000 (62%)]\tLoss: 1.501391\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37350/60000 (62%)]\tLoss: 1.483379\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37400/60000 (62%)]\tLoss: 1.481206\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37450/60000 (62%)]\tLoss: 1.480920\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37500/60000 (62%)]\tLoss: 1.461152\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37550/60000 (63%)]\tLoss: 1.501371\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37600/60000 (63%)]\tLoss: 1.462345\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37650/60000 (63%)]\tLoss: 1.461214\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37700/60000 (63%)]\tLoss: 1.464228\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37750/60000 (63%)]\tLoss: 1.461153\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37800/60000 (63%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37850/60000 (63%)]\tLoss: 1.462764\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37900/60000 (63%)]\tLoss: 1.462394\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [37950/60000 (63%)]\tLoss: 1.461155\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38000/60000 (63%)]\tLoss: 1.512301\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38050/60000 (63%)]\tLoss: 1.462257\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38100/60000 (64%)]\tLoss: 1.498527\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38150/60000 (64%)]\tLoss: 1.501404\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38200/60000 (64%)]\tLoss: 1.481774\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38250/60000 (64%)]\tLoss: 1.461151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38300/60000 (64%)]\tLoss: 1.463139\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38350/60000 (64%)]\tLoss: 1.481151\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38400/60000 (64%)]\tLoss: 1.461836\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38450/60000 (64%)]\tLoss: 1.465314\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38500/60000 (64%)]\tLoss: 1.466807\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38550/60000 (64%)]\tLoss: 1.461368\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38600/60000 (64%)]\tLoss: 1.463075\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38650/60000 (64%)]\tLoss: 1.479792\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38700/60000 (64%)]\tLoss: 1.461151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38750/60000 (65%)]\tLoss: 1.481151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38800/60000 (65%)]\tLoss: 1.461158\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38850/60000 (65%)]\tLoss: 1.482178\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38900/60000 (65%)]\tLoss: 1.464763\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [38950/60000 (65%)]\tLoss: 1.481705\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39000/60000 (65%)]\tLoss: 1.501001\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39050/60000 (65%)]\tLoss: 1.481184\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39100/60000 (65%)]\tLoss: 1.461350\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39150/60000 (65%)]\tLoss: 1.461356\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39200/60000 (65%)]\tLoss: 1.482255\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39250/60000 (65%)]\tLoss: 1.461153\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39300/60000 (66%)]\tLoss: 1.484371\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39350/60000 (66%)]\tLoss: 1.541670\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39400/60000 (66%)]\tLoss: 1.486837\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39450/60000 (66%)]\tLoss: 1.469921\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39500/60000 (66%)]\tLoss: 1.493941\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39550/60000 (66%)]\tLoss: 1.463214\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39600/60000 (66%)]\tLoss: 1.483045\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39650/60000 (66%)]\tLoss: 1.481828\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39700/60000 (66%)]\tLoss: 1.461174\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39750/60000 (66%)]\tLoss: 1.472389\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39800/60000 (66%)]\tLoss: 1.481148\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39850/60000 (66%)]\tLoss: 1.481111\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39900/60000 (66%)]\tLoss: 1.461388\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [39950/60000 (67%)]\tLoss: 1.461168\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40000/60000 (67%)]\tLoss: 1.461155\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40050/60000 (67%)]\tLoss: 1.520397\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40100/60000 (67%)]\tLoss: 1.461151\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40150/60000 (67%)]\tLoss: 1.471438\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40200/60000 (67%)]\tLoss: 1.471274\tAccuracy: 98.40%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40250/60000 (67%)]\tLoss: 1.491730\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40300/60000 (67%)]\tLoss: 1.497416\tAccuracy: 98.39%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40350/60000 (67%)]\tLoss: 1.539734\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40400/60000 (67%)]\tLoss: 1.481152\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40450/60000 (67%)]\tLoss: 1.475859\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40500/60000 (68%)]\tLoss: 1.480709\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40550/60000 (68%)]\tLoss: 1.501151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40600/60000 (68%)]\tLoss: 1.481239\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40650/60000 (68%)]\tLoss: 1.478923\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40700/60000 (68%)]\tLoss: 1.481148\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40750/60000 (68%)]\tLoss: 1.461153\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40800/60000 (68%)]\tLoss: 1.461157\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40850/60000 (68%)]\tLoss: 1.481119\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40900/60000 (68%)]\tLoss: 1.489879\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [40950/60000 (68%)]\tLoss: 1.494819\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41000/60000 (68%)]\tLoss: 1.480135\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41050/60000 (68%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41100/60000 (68%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41150/60000 (69%)]\tLoss: 1.496421\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41200/60000 (69%)]\tLoss: 1.481192\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41250/60000 (69%)]\tLoss: 1.466891\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41300/60000 (69%)]\tLoss: 1.461181\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41350/60000 (69%)]\tLoss: 1.494334\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41400/60000 (69%)]\tLoss: 1.501036\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41450/60000 (69%)]\tLoss: 1.461218\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41500/60000 (69%)]\tLoss: 1.481028\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41550/60000 (69%)]\tLoss: 1.461163\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41600/60000 (69%)]\tLoss: 1.466842\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41650/60000 (69%)]\tLoss: 1.481160\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41700/60000 (70%)]\tLoss: 1.481059\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41750/60000 (70%)]\tLoss: 1.501557\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41800/60000 (70%)]\tLoss: 1.461223\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41850/60000 (70%)]\tLoss: 1.461178\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41900/60000 (70%)]\tLoss: 1.466141\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [41950/60000 (70%)]\tLoss: 1.500951\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42000/60000 (70%)]\tLoss: 1.461163\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42050/60000 (70%)]\tLoss: 1.481153\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42100/60000 (70%)]\tLoss: 1.481149\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42150/60000 (70%)]\tLoss: 1.541929\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42200/60000 (70%)]\tLoss: 1.522349\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42250/60000 (70%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42300/60000 (70%)]\tLoss: 1.481669\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42350/60000 (71%)]\tLoss: 1.478253\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42400/60000 (71%)]\tLoss: 1.461636\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42450/60000 (71%)]\tLoss: 1.482289\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42500/60000 (71%)]\tLoss: 1.463258\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42550/60000 (71%)]\tLoss: 1.503697\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42600/60000 (71%)]\tLoss: 1.474842\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42650/60000 (71%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42700/60000 (71%)]\tLoss: 1.480975\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42750/60000 (71%)]\tLoss: 1.461152\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42800/60000 (71%)]\tLoss: 1.481062\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42850/60000 (71%)]\tLoss: 1.461158\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42900/60000 (72%)]\tLoss: 1.495798\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [42950/60000 (72%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43000/60000 (72%)]\tLoss: 1.489244\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43050/60000 (72%)]\tLoss: 1.461158\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43100/60000 (72%)]\tLoss: 1.500918\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43150/60000 (72%)]\tLoss: 1.493547\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43200/60000 (72%)]\tLoss: 1.501151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43250/60000 (72%)]\tLoss: 1.480493\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43300/60000 (72%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43350/60000 (72%)]\tLoss: 1.480945\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43400/60000 (72%)]\tLoss: 1.482790\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43450/60000 (72%)]\tLoss: 1.481590\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43500/60000 (72%)]\tLoss: 1.478449\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43550/60000 (73%)]\tLoss: 1.461157\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43600/60000 (73%)]\tLoss: 1.461151\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43650/60000 (73%)]\tLoss: 1.469605\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43700/60000 (73%)]\tLoss: 1.461485\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43750/60000 (73%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43800/60000 (73%)]\tLoss: 1.461152\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43850/60000 (73%)]\tLoss: 1.461795\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43900/60000 (73%)]\tLoss: 1.461246\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [43950/60000 (73%)]\tLoss: 1.481675\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44000/60000 (73%)]\tLoss: 1.480662\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44050/60000 (73%)]\tLoss: 1.501214\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44100/60000 (74%)]\tLoss: 1.481181\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44150/60000 (74%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44200/60000 (74%)]\tLoss: 1.474540\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44250/60000 (74%)]\tLoss: 1.481455\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44300/60000 (74%)]\tLoss: 1.461174\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44350/60000 (74%)]\tLoss: 1.495308\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44400/60000 (74%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44450/60000 (74%)]\tLoss: 1.481151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44500/60000 (74%)]\tLoss: 1.467135\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44550/60000 (74%)]\tLoss: 1.494517\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44600/60000 (74%)]\tLoss: 1.500799\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44650/60000 (74%)]\tLoss: 1.481484\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44700/60000 (74%)]\tLoss: 1.501140\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44750/60000 (75%)]\tLoss: 1.462655\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44800/60000 (75%)]\tLoss: 1.481136\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44850/60000 (75%)]\tLoss: 1.467505\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44900/60000 (75%)]\tLoss: 1.461933\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [44950/60000 (75%)]\tLoss: 1.500985\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45000/60000 (75%)]\tLoss: 1.481151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45050/60000 (75%)]\tLoss: 1.481196\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45100/60000 (75%)]\tLoss: 1.461168\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45150/60000 (75%)]\tLoss: 1.497818\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45200/60000 (75%)]\tLoss: 1.461151\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45250/60000 (75%)]\tLoss: 1.463970\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45300/60000 (76%)]\tLoss: 1.478171\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45350/60000 (76%)]\tLoss: 1.461153\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45400/60000 (76%)]\tLoss: 1.481148\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45450/60000 (76%)]\tLoss: 1.481024\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45500/60000 (76%)]\tLoss: 1.481324\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45550/60000 (76%)]\tLoss: 1.480693\tAccuracy: 98.38%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45600/60000 (76%)]\tLoss: 1.521150\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45650/60000 (76%)]\tLoss: 1.480593\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45700/60000 (76%)]\tLoss: 1.497175\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45750/60000 (76%)]\tLoss: 1.529239\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45800/60000 (76%)]\tLoss: 1.462104\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45850/60000 (76%)]\tLoss: 1.481177\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45900/60000 (76%)]\tLoss: 1.461254\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [45950/60000 (77%)]\tLoss: 1.481150\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46000/60000 (77%)]\tLoss: 1.462758\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46050/60000 (77%)]\tLoss: 1.462227\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46100/60000 (77%)]\tLoss: 1.481144\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46150/60000 (77%)]\tLoss: 1.462173\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46200/60000 (77%)]\tLoss: 1.461152\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46250/60000 (77%)]\tLoss: 1.502100\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46300/60000 (77%)]\tLoss: 1.522881\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46350/60000 (77%)]\tLoss: 1.461223\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46400/60000 (77%)]\tLoss: 1.484738\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46450/60000 (77%)]\tLoss: 1.481294\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46500/60000 (78%)]\tLoss: 1.462971\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46550/60000 (78%)]\tLoss: 1.533938\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46600/60000 (78%)]\tLoss: 1.482711\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46650/60000 (78%)]\tLoss: 1.461153\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46700/60000 (78%)]\tLoss: 1.471065\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46750/60000 (78%)]\tLoss: 1.480482\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46800/60000 (78%)]\tLoss: 1.461212\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46850/60000 (78%)]\tLoss: 1.482775\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46900/60000 (78%)]\tLoss: 1.493957\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [46950/60000 (78%)]\tLoss: 1.461628\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47000/60000 (78%)]\tLoss: 1.461300\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47050/60000 (78%)]\tLoss: 1.479736\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47100/60000 (78%)]\tLoss: 1.461655\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47150/60000 (79%)]\tLoss: 1.482046\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47200/60000 (79%)]\tLoss: 1.481282\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47250/60000 (79%)]\tLoss: 1.461756\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47300/60000 (79%)]\tLoss: 1.481022\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47350/60000 (79%)]\tLoss: 1.463307\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47400/60000 (79%)]\tLoss: 1.499074\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47450/60000 (79%)]\tLoss: 1.461521\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47500/60000 (79%)]\tLoss: 1.483185\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47550/60000 (79%)]\tLoss: 1.474645\tAccuracy: 98.37%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47600/60000 (79%)]\tLoss: 1.520268\tAccuracy: 98.36%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47650/60000 (79%)]\tLoss: 1.550842\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47700/60000 (80%)]\tLoss: 1.481127\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47750/60000 (80%)]\tLoss: 1.462113\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47800/60000 (80%)]\tLoss: 1.494865\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47850/60000 (80%)]\tLoss: 1.461183\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47900/60000 (80%)]\tLoss: 1.481165\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [47950/60000 (80%)]\tLoss: 1.501141\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48000/60000 (80%)]\tLoss: 1.502807\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48050/60000 (80%)]\tLoss: 1.461181\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48100/60000 (80%)]\tLoss: 1.492594\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48150/60000 (80%)]\tLoss: 1.461244\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48200/60000 (80%)]\tLoss: 1.497340\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48250/60000 (80%)]\tLoss: 1.481510\tAccuracy: 98.35%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48300/60000 (80%)]\tLoss: 1.571637\tAccuracy: 98.34%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48350/60000 (81%)]\tLoss: 1.530329\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48400/60000 (81%)]\tLoss: 1.481163\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48450/60000 (81%)]\tLoss: 1.461155\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48500/60000 (81%)]\tLoss: 1.522477\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48550/60000 (81%)]\tLoss: 1.480361\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48600/60000 (81%)]\tLoss: 1.461158\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48650/60000 (81%)]\tLoss: 1.496892\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48700/60000 (81%)]\tLoss: 1.484336\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48750/60000 (81%)]\tLoss: 1.471225\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48800/60000 (81%)]\tLoss: 1.478428\tAccuracy: 98.33%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48850/60000 (81%)]\tLoss: 1.505257\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48900/60000 (82%)]\tLoss: 1.481192\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [48950/60000 (82%)]\tLoss: 1.525789\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49000/60000 (82%)]\tLoss: 1.501208\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49050/60000 (82%)]\tLoss: 1.485907\tAccuracy: 98.32%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49100/60000 (82%)]\tLoss: 1.506760\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49150/60000 (82%)]\tLoss: 1.520902\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49200/60000 (82%)]\tLoss: 1.481585\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49250/60000 (82%)]\tLoss: 1.500462\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49300/60000 (82%)]\tLoss: 1.481751\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49350/60000 (82%)]\tLoss: 1.480373\tAccuracy: 98.31%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49400/60000 (82%)]\tLoss: 1.492849\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49450/60000 (82%)]\tLoss: 1.503743\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49500/60000 (82%)]\tLoss: 1.479724\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49550/60000 (83%)]\tLoss: 1.463217\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49600/60000 (83%)]\tLoss: 1.483092\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49650/60000 (83%)]\tLoss: 1.487539\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49700/60000 (83%)]\tLoss: 1.462031\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49750/60000 (83%)]\tLoss: 1.495285\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49800/60000 (83%)]\tLoss: 1.481720\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49850/60000 (83%)]\tLoss: 1.461190\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49900/60000 (83%)]\tLoss: 1.520315\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [49950/60000 (83%)]\tLoss: 1.489648\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50000/60000 (83%)]\tLoss: 1.468331\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50050/60000 (83%)]\tLoss: 1.481666\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50100/60000 (84%)]\tLoss: 1.461749\tAccuracy: 98.30%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50150/60000 (84%)]\tLoss: 1.540734\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50200/60000 (84%)]\tLoss: 1.481470\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50250/60000 (84%)]\tLoss: 1.513689\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50300/60000 (84%)]\tLoss: 1.518802\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50350/60000 (84%)]\tLoss: 1.509120\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50400/60000 (84%)]\tLoss: 1.485776\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50450/60000 (84%)]\tLoss: 1.461485\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50500/60000 (84%)]\tLoss: 1.462192\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50550/60000 (84%)]\tLoss: 1.475852\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50600/60000 (84%)]\tLoss: 1.481152\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50650/60000 (84%)]\tLoss: 1.500076\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50700/60000 (84%)]\tLoss: 1.501319\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50750/60000 (85%)]\tLoss: 1.461874\tAccuracy: 98.29%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50800/60000 (85%)]\tLoss: 1.495108\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50850/60000 (85%)]\tLoss: 1.577959\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50900/60000 (85%)]\tLoss: 1.461159\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [50950/60000 (85%)]\tLoss: 1.461183\tAccuracy: 98.28%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51000/60000 (85%)]\tLoss: 1.494580\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51050/60000 (85%)]\tLoss: 1.481151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51100/60000 (85%)]\tLoss: 1.520530\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51150/60000 (85%)]\tLoss: 1.499789\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51200/60000 (85%)]\tLoss: 1.475320\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51250/60000 (85%)]\tLoss: 1.461250\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51300/60000 (86%)]\tLoss: 1.481152\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51350/60000 (86%)]\tLoss: 1.482079\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51400/60000 (86%)]\tLoss: 1.461416\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51450/60000 (86%)]\tLoss: 1.503057\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51500/60000 (86%)]\tLoss: 1.479436\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51550/60000 (86%)]\tLoss: 1.466233\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51600/60000 (86%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51650/60000 (86%)]\tLoss: 1.497951\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51700/60000 (86%)]\tLoss: 1.461162\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51750/60000 (86%)]\tLoss: 1.481342\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51800/60000 (86%)]\tLoss: 1.500951\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51850/60000 (86%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51900/60000 (86%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [51950/60000 (87%)]\tLoss: 1.481151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52000/60000 (87%)]\tLoss: 1.500510\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52050/60000 (87%)]\tLoss: 1.461416\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52100/60000 (87%)]\tLoss: 1.481151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52150/60000 (87%)]\tLoss: 1.481151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52200/60000 (87%)]\tLoss: 1.486194\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52250/60000 (87%)]\tLoss: 1.480925\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52300/60000 (87%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52350/60000 (87%)]\tLoss: 1.501764\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52400/60000 (87%)]\tLoss: 1.490399\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52450/60000 (87%)]\tLoss: 1.461907\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52500/60000 (88%)]\tLoss: 1.500976\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52550/60000 (88%)]\tLoss: 1.461155\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52600/60000 (88%)]\tLoss: 1.481351\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52650/60000 (88%)]\tLoss: 1.493052\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52700/60000 (88%)]\tLoss: 1.472308\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52750/60000 (88%)]\tLoss: 1.465696\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52800/60000 (88%)]\tLoss: 1.461151\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52850/60000 (88%)]\tLoss: 1.465102\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52900/60000 (88%)]\tLoss: 1.507177\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [52950/60000 (88%)]\tLoss: 1.461278\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53000/60000 (88%)]\tLoss: 1.498223\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53050/60000 (88%)]\tLoss: 1.483352\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53100/60000 (88%)]\tLoss: 1.498758\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53150/60000 (89%)]\tLoss: 1.501148\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53200/60000 (89%)]\tLoss: 1.499897\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53250/60000 (89%)]\tLoss: 1.513554\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53300/60000 (89%)]\tLoss: 1.488598\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53350/60000 (89%)]\tLoss: 1.480301\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53400/60000 (89%)]\tLoss: 1.498425\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53450/60000 (89%)]\tLoss: 1.481158\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53500/60000 (89%)]\tLoss: 1.521004\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53550/60000 (89%)]\tLoss: 1.490970\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53600/60000 (89%)]\tLoss: 1.501191\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53650/60000 (89%)]\tLoss: 1.461168\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53700/60000 (90%)]\tLoss: 1.555081\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53750/60000 (90%)]\tLoss: 1.461151\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53800/60000 (90%)]\tLoss: 1.497513\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53850/60000 (90%)]\tLoss: 1.461203\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53900/60000 (90%)]\tLoss: 1.466045\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [53950/60000 (90%)]\tLoss: 1.461186\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54000/60000 (90%)]\tLoss: 1.519503\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54050/60000 (90%)]\tLoss: 1.461151\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54100/60000 (90%)]\tLoss: 1.481081\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54150/60000 (90%)]\tLoss: 1.481265\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54200/60000 (90%)]\tLoss: 1.481330\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54250/60000 (90%)]\tLoss: 1.461151\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54300/60000 (90%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54350/60000 (91%)]\tLoss: 1.462009\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54400/60000 (91%)]\tLoss: 1.463119\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54450/60000 (91%)]\tLoss: 1.490923\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54500/60000 (91%)]\tLoss: 1.461636\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54550/60000 (91%)]\tLoss: 1.475044\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54600/60000 (91%)]\tLoss: 1.480835\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54650/60000 (91%)]\tLoss: 1.463982\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54700/60000 (91%)]\tLoss: 1.461152\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54750/60000 (91%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54800/60000 (91%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54850/60000 (91%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54900/60000 (92%)]\tLoss: 1.481158\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [54950/60000 (92%)]\tLoss: 1.463953\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55000/60000 (92%)]\tLoss: 1.462299\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55050/60000 (92%)]\tLoss: 1.495113\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55100/60000 (92%)]\tLoss: 1.535239\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55150/60000 (92%)]\tLoss: 1.462462\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55200/60000 (92%)]\tLoss: 1.461228\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55250/60000 (92%)]\tLoss: 1.461156\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55300/60000 (92%)]\tLoss: 1.518949\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55350/60000 (92%)]\tLoss: 1.461618\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55400/60000 (92%)]\tLoss: 1.480752\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55450/60000 (92%)]\tLoss: 1.480879\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55500/60000 (92%)]\tLoss: 1.470119\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55550/60000 (93%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55600/60000 (93%)]\tLoss: 1.477005\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55650/60000 (93%)]\tLoss: 1.466612\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55700/60000 (93%)]\tLoss: 1.478385\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55750/60000 (93%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55800/60000 (93%)]\tLoss: 1.462014\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55850/60000 (93%)]\tLoss: 1.481125\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55900/60000 (93%)]\tLoss: 1.498043\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [55950/60000 (93%)]\tLoss: 1.492342\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56000/60000 (93%)]\tLoss: 1.462014\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56050/60000 (93%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56100/60000 (94%)]\tLoss: 1.483023\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56150/60000 (94%)]\tLoss: 1.480514\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56200/60000 (94%)]\tLoss: 1.490713\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56250/60000 (94%)]\tLoss: 1.461483\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56300/60000 (94%)]\tLoss: 1.461307\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56350/60000 (94%)]\tLoss: 1.462053\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56400/60000 (94%)]\tLoss: 1.481256\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56450/60000 (94%)]\tLoss: 1.461193\tAccuracy: 98.27%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56500/60000 (94%)]\tLoss: 1.501978\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56550/60000 (94%)]\tLoss: 1.479157\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56600/60000 (94%)]\tLoss: 1.468065\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56650/60000 (94%)]\tLoss: 1.495275\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56700/60000 (94%)]\tLoss: 1.480975\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56750/60000 (95%)]\tLoss: 1.462560\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56800/60000 (95%)]\tLoss: 1.481395\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56850/60000 (95%)]\tLoss: 1.518840\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56900/60000 (95%)]\tLoss: 1.481472\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [56950/60000 (95%)]\tLoss: 1.501095\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57000/60000 (95%)]\tLoss: 1.550162\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57050/60000 (95%)]\tLoss: 1.532733\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57100/60000 (95%)]\tLoss: 1.489937\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57150/60000 (95%)]\tLoss: 1.500964\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57200/60000 (95%)]\tLoss: 1.481153\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57250/60000 (95%)]\tLoss: 1.520577\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57300/60000 (96%)]\tLoss: 1.462957\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57350/60000 (96%)]\tLoss: 1.463015\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57400/60000 (96%)]\tLoss: 1.499620\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57450/60000 (96%)]\tLoss: 1.461151\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57500/60000 (96%)]\tLoss: 1.468197\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57550/60000 (96%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57600/60000 (96%)]\tLoss: 1.476049\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57650/60000 (96%)]\tLoss: 1.500791\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57700/60000 (96%)]\tLoss: 1.480801\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57750/60000 (96%)]\tLoss: 1.472039\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57800/60000 (96%)]\tLoss: 1.481150\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57850/60000 (96%)]\tLoss: 1.491969\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57900/60000 (96%)]\tLoss: 1.461159\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [57950/60000 (97%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58000/60000 (97%)]\tLoss: 1.461203\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58050/60000 (97%)]\tLoss: 1.468445\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58100/60000 (97%)]\tLoss: 1.501267\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58150/60000 (97%)]\tLoss: 1.481151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58200/60000 (97%)]\tLoss: 1.468720\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58250/60000 (97%)]\tLoss: 1.461152\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58300/60000 (97%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58350/60000 (97%)]\tLoss: 1.482434\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58400/60000 (97%)]\tLoss: 1.481151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58450/60000 (97%)]\tLoss: 1.462515\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58500/60000 (98%)]\tLoss: 1.480901\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58550/60000 (98%)]\tLoss: 1.522618\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58600/60000 (98%)]\tLoss: 1.499790\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58650/60000 (98%)]\tLoss: 1.485499\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58700/60000 (98%)]\tLoss: 1.487996\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58750/60000 (98%)]\tLoss: 1.462178\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58800/60000 (98%)]\tLoss: 1.471320\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58850/60000 (98%)]\tLoss: 1.499802\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58900/60000 (98%)]\tLoss: 1.518796\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [58950/60000 (98%)]\tLoss: 1.481100\tAccuracy: 98.24%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59000/60000 (98%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59050/60000 (98%)]\tLoss: 1.461152\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59100/60000 (98%)]\tLoss: 1.481138\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59150/60000 (99%)]\tLoss: 1.483047\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59200/60000 (99%)]\tLoss: 1.481151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59250/60000 (99%)]\tLoss: 1.462927\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59300/60000 (99%)]\tLoss: 1.481262\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59350/60000 (99%)]\tLoss: 1.481061\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59400/60000 (99%)]\tLoss: 1.464363\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59450/60000 (99%)]\tLoss: 1.461220\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59500/60000 (99%)]\tLoss: 1.461264\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59550/60000 (99%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59600/60000 (99%)]\tLoss: 1.470603\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59650/60000 (99%)]\tLoss: 1.461151\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59700/60000 (100%)]\tLoss: 1.487301\tAccuracy: 98.25%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59750/60000 (100%)]\tLoss: 1.466403\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59800/60000 (100%)]\tLoss: 1.480364\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59850/60000 (100%)]\tLoss: 1.461153\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59900/60000 (100%)]\tLoss: 1.484345\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Epoch: 4 [59950/60000 (100%)]\tLoss: 1.461151\tAccuracy: 98.26%\tGradient Norm: 0.000000\n",
      "Accuracy:  97.50466666666667\n"
     ]
    }
   ],
   "source": [
    "#call the function\n",
    "# source_cnn, classifier = pre_train(source_cnn, classifier,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we write a function to return accuracy, given encoder_cnn, classifier and data_loader\n",
    "def get_accuracy(encoder_cnn, classifier, data_loader):\n",
    "    #set the model to eval mode\n",
    "    encoder_cnn.eval()\n",
    "    classifier.eval()\n",
    "    #set the total and correct to zero\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    #iterate over the data\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        #send data to gpu\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #forward pass\n",
    "        output = classifier(encoder_cnn(data))\n",
    "        #get the max value from the output\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        #calculate the total number of labels\n",
    "        temp_total = target.size(0)\n",
    "        #calculate the correct predictions\n",
    "        temp_correct = (predicted == target).sum().item()\n",
    "        #add the total and correct predictions\n",
    "        total += temp_total\n",
    "        correct += temp_correct\n",
    "    #calculate the accuracy\n",
    "    total_accuracy = 100 * correct / total\n",
    "    #return the accuracy\n",
    "    #make models train mode again\n",
    "    encoder_cnn.train()\n",
    "    classifier.train()\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have the source_cnn and classifier trained\n",
    "#we will  now never train the source_cnn again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Adversarial Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will use WGAN like training for the target_cnn and discriminator\n",
    "#the REAL DATA will come from the output of source_cnn on the source data\n",
    "#the generated data will come from the output of target_cnn on the target data\n",
    "#we will minimize the WGAN loss\n",
    "#also we use gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining gradient penalty\n",
    "def gradient_penalty(critic, source, target, device=device):\n",
    "    BATCH_SIZE, C, H, W = source.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = source * alpha + target * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will define the function to train the target_cnn and discriminator\n",
    "#it will take as arguments the target_cnn, discriminator, source_cnn, train_loader, test_loader, epochs\n",
    "#we will alos plot the loss and accuracy, per batch and per epoch\n",
    "# also we will plot the gradient penalty\n",
    "def train_adapt_target(target_cnn, discriminator, source_cnn, train_loader, test_loader, epochs=NUM_EPOCHS_GAN, device=device):\n",
    "\n",
    "    #save\n",
    "    source_nn_filename = 'saved_models/ADDA/'+experiment_id + 'temp_storage' + \"source_cnn.pt\"\n",
    "    torch.save(source_cnn.state_dict(), source_nn_filename)\n",
    "    #we will use the Adam optimizer for both the target_cnn and discriminator\n",
    "    # , but seperate\n",
    "    #we will use the same learning rate for both\n",
    "    optimizer_target_cnn = optim.Adam(target_cnn.parameters(), lr=LEARNING_RATE_GAN, betas=(0.0, 0.9) )\n",
    "    optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_GAN, betas=(0.0, 0.9) )\n",
    "\n",
    "    #make both models trainable\n",
    "    target_cnn.train()\n",
    "    discriminator.train()\n",
    "    #now make source cnn non trainable\n",
    "    #use freeze_unfreeze function\n",
    "    freeze_unfreeze_model(source_cnn, False)\n",
    "    source_cnn.eval()\n",
    "\n",
    "    #define dumy variables for keeping track of accuracy, loss and iterations through the dataset\n",
    "    step = 0\n",
    "    epoch_tracker = 0\n",
    "    batch_tracker = 0\n",
    "    #define the best loss and best epoch\n",
    "    best_loss = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    #loop through epochs\n",
    "    ep=0\n",
    "    for ep in range(epochs):\n",
    "        epoch_total_loss = 0\n",
    "        epoch_critics_loss = 0\n",
    "        epoch_target_cnn_loss = 0\n",
    "        #loop through the batches\n",
    "        #loop through batches of source data and target data combined\n",
    "        for batch_idx, (source_data, target_data) in enumerate(zip(train_loader, train_loader)):\n",
    "            #get the source and target images and we do not need labels, nbut anyway we will get them\n",
    "            source_images, source_labels = source_data\n",
    "            target_images, target_labels = target_data\n",
    "            #get batch size as min of source and target batch size\n",
    "            batch_size = min(source_images.shape[0], target_images.shape[0])\n",
    "            #make the batch size of source and target equal\n",
    "            source_images = source_images[:batch_size]\n",
    "            source_labels = source_labels[:batch_size]\n",
    "            target_images = target_images[:batch_size]\n",
    "            target_labels = target_labels[:batch_size]\n",
    "\n",
    "            #move the data to gpu\n",
    "            source_images, source_labels = source_images.to(device), source_labels.to(device)\n",
    "            target_images, target_labels = target_images.to(device), target_labels.to(device)\n",
    "\n",
    "            # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "        # equivalent to minimizing the negative of that\n",
    "            for _ in range(CRITIC_ITERATIONS):\n",
    "                #generate the features of the target images\n",
    "                target_features = target_cnn(target_images)\n",
    "                #generate the features of the source images\n",
    "                #load the source_cnn\n",
    "                source_cnn.load_state_dict(torch.load(source_nn_filename))\n",
    "                source_features = source_cnn(source_images)\n",
    "                #get gradient penalty\n",
    "                gp = gradient_penalty(discriminator, source_features, target_features)\n",
    "                critic_source = discriminator(source_features).reshape(-1)\n",
    "                critic_target = discriminator(target_features).reshape(-1)\n",
    "                critic_loss = -(torch.mean(critic_source) - torch.mean(critic_target)) + LAMBDA_GP * gp\n",
    "                #zero the gradients\n",
    "                discriminator.zero_grad()\n",
    "                #backpropagate the loss\n",
    "                critic_loss.backward(retain_graph=True)\n",
    "                #update the weights\n",
    "                optimizer_discriminator.step()\n",
    "\n",
    "            # Train target_cnn: min -E[critic(gen_fake)] <-> max E[critic(gen_fake)]\n",
    "            critic_target = discriminator(target_features).reshape(-1)\n",
    "            loss_target_cnn = -torch.mean(critic_target)\n",
    "            #zero the gradients\n",
    "            target_cnn.zero_grad()\n",
    "            #backpropagate the loss\n",
    "            loss_target_cnn.backward()\n",
    "            #update the weights\n",
    "            optimizer_target_cnn.step()\n",
    "\n",
    "            #add losses to epoch losses\n",
    "            epoch_total_loss += critic_loss.item() + loss_target_cnn.item()\n",
    "            epoch_critics_loss += critic_loss.item()\n",
    "            epoch_target_cnn_loss += loss_target_cnn.item()\n",
    "\n",
    "            #we will plot the loss on tensorboard\n",
    "            #we will plot the critic loss, target_cnn loss, gradient penalty\n",
    "            writer.add_scalar('ADDA_Loss_Critic', critic_loss, global_step=batch_tracker)\n",
    "            writer.add_scalar('ADDA_Loss_Target_CNN', loss_target_cnn, global_step=batch_tracker)\n",
    "            writer.add_scalar('ADDA_Gradient_Penalty', gp, global_step=batch_tracker)\n",
    "\n",
    "            #print losses after every 100 steps\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch [{ep}/{epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
    "                      Loss D: {critic_loss:.4f}, loss G: {loss_target_cnn:.4f}, gp: {gp:.4f}\")\n",
    "            \n",
    "                \n",
    "            #increment the batch tracker\n",
    "            batch_tracker += 1\n",
    "                \n",
    "        #print the epoch loss\n",
    "        print(f\"Epoch [{ep}/{epochs}] Loss D: {epoch_critics_loss:.4f}, loss G: {epoch_target_cnn_loss:.4f}\")\n",
    "        #add the epoch loss to tensorboard\n",
    "        writer.add_scalar('ADDA_Epoch_Total_Loss', epoch_total_loss, global_step=ep)\n",
    "        #critics loss\n",
    "        writer.add_scalar('ADDA_Epoch_Loss_Critic', epoch_critics_loss, global_step=ep)\n",
    "        #target cnn loss\n",
    "        writer.add_scalar('ADDA_Epoch_Loss_Target_CNN', epoch_target_cnn_loss, global_step=ep)\n",
    "        #every epoch we will save the model\n",
    "        #save the model with name experiment_id and epoch\n",
    "        torch.save(target_cnn.state_dict(), f\"{experiment_id}_target_cnn_{ep}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"{experiment_id}_discriminator_{ep}.pth\")\n",
    "        #test the accuracy of the model on the test set\n",
    "        test_accuracy = get_accuracy(target_cnn, classifier, test_loader)\n",
    "        #make classifier non trainable\n",
    "        freeze_unfreeze_model(classifier, False)\n",
    "        classifier.eval()\n",
    "        #make model trainable\n",
    "        target_cnn.train()\n",
    "        #print the test accuracy\n",
    "        print(f\"Epoch [{ep}/{epochs}] Test Accuracy: {test_accuracy:.4f}\")\n",
    "        #add the test accuracy to tensorboard\n",
    "        writer.add_scalar('ADDA_Test_Accuracy_Target_CNN', test_accuracy, global_step=ep)  \n",
    "        \n",
    "\n",
    "\n",
    "    #return the target cnn and discriminator\n",
    "    return target_cnn, discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now train the target cnn and discriminator\n",
    "#let us create target cnn with same weights as trained source cnn\n",
    "# target_cnn = copy.deepcopy(source_cnn)\n",
    "#but both be different and changes in one will not affect the other\n",
    "#create target cnn as same weights as source cnn\n",
    "target_cnn = copy.deepcopy(source_cnn)\n",
    "#make target cnn trainable\n",
    "freeze_unfreeze_model(target_cnn, True)\n",
    "#make source cnn non trainable\n",
    "freeze_unfreeze_model(source_cnn, False)\n",
    "#make discriminator trainable\n",
    "freeze_unfreeze_model(discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2219/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source CNN Accuracy on Source Data: 8.4367\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of source cnn on source data\n",
    "source_cnn_accuracy = get_accuracy(source_cnn, classifier, train_loader)\n",
    "print(f\"Source CNN Accuracy on Source Data: {source_cnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2219/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source CNN Accuracy on Target Data: 9.3266\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of source cnn on target data\n",
    "target_cnn_accuracy = get_accuracy(source_cnn, classifier, test_loader)\n",
    "#print accuracy\n",
    "print(f\"Source CNN Accuracy on Target Data: {target_cnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2219/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target CNN Accuracy on Source Data: 8.4367\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of target cnn on source data\n",
    "target_cnn_accuracy_source = get_accuracy(target_cnn, classifier, train_loader)\n",
    "#print accuracy\n",
    "print(f\"Target CNN Accuracy on Source Data: {target_cnn_accuracy_source:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2219/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target CNN Accuracy on Target Data: 9.3266\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of target cnn on target data\n",
    "target_cnn_accuracy_target = get_accuracy(target_cnn, classifier, test_loader)\n",
    "#print accuracy\n",
    "print(f\"Target CNN Accuracy on Target Data: {target_cnn_accuracy_target:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we wont train the classifier, but we directly load target_cnn  from file\n",
    "#path to the file: mnist_adda_v2_target_cnn_26.pth\n",
    "#load the model in target_cnn\n",
    "target_cnn.load_state_dict(torch.load(\"mnist_adda_v2_target_cnn_26.pth\"))\n",
    "#make target cnn non trainable\n",
    "freeze_unfreeze_model(target_cnn, False)\n",
    "#make classifier non trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will load classifier from file\n",
    "#path to the file: saved_models/mnist_adda_v2_classifier.pth\n",
    "#load the model in classifier\n",
    "classifier.load_state_dict(torch.load(\"saved_models/mnist_adda_v2_classifier_4.pth\"))\n",
    "#make classifier non trainable\n",
    "freeze_unfreeze_model(classifier, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train the target cnn and discriminator\n",
    "# target_cnn, discriminator = train_adapt_target(target_cnn, discriminator, source_cnn, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2219/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.546015635715266"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will now test the target cnn\n",
    "#we will test the target cnn on the test data\n",
    "#the classifier will be the same as trained\n",
    "#call the function to test the target cnn accuracy\n",
    "get_accuracy(target_cnn, classifier, test_loader)\n",
    "# print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2219/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.268333333333333"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the accuracy of source cnn on source data\n",
    "get_accuracy(source_cnn, classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2219/2407848038.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.8989164723631875"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get accuracy of source cnn on target data\n",
    "get_accuracy(source_cnn, classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079402cc50f681fca3bc4b588c8594ae5b0127c6215ec7c89d21fdfb87f97274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
