{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this script we will train a classifier on mnist and then test it on usps\n",
    "#we will use the same classifier for both datasets\n",
    "#classifier is Resnet-50 based on the paper \"Deep Residual Learning for Image Recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this script we will build a classifier for mnist dataset\n",
    "#we will use pretraiined Resnet-50 model and train it on mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'mnist_classifier'\n",
    "version = 'v1'\n",
    "\n",
    "#concat experiment name and version to get experiment id\n",
    "experiment_id = experiment_name + '_' + version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "#import tenserboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use the sklearn confusion matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU name\n",
    "#\n",
    "GPU_NAME = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device\n",
    "device = torch.device(GPU_NAME if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda cache clear\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_epochs = 2\n",
    "IMAGE_SIZE = 224\n",
    "CHANNELS_IMG = 1\n",
    "NUM_CLASSES = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 1: Initialize model with the best available weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "#send weight sto gpu\n",
    "# weights = weights.to(device)\n",
    "#sending the model to GPU\n",
    "\n",
    "model = resnet50(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#print model summary\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Initialize the inference transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()\n",
    "#add on more transform to make channels 3 if there are only 1 channel\n",
    "# preprocess.transforms.append(transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the transform for the dataset\n",
    "transform_mnist_resnet = transforms.Compose(\n",
    "    [\n",
    "    #convert to pil image\n",
    "    # transforms.ToPILImage(),\n",
    "    #check if PIL Image then leave as it is, else convert to PIL Image\n",
    "    # transforms.Lambda(lambda x: x if isinstance(x, Image.Image) else transforms.functional.to_pil_image(x)),\n",
    "    # #if greyscale then convert to 3 channels using , transforms.functional.to_grayscale( num_output_channels=3)\n",
    "    # #only if the image is greyscale, then convert to 3 channels, else leave as it is\n",
    "    # transforms.Lambda(lambda x: transforms.functional.to_grayscale(x, num_output_channels=3) if x.shape[0] == 1 else x),\n",
    "\n",
    "    #print type of image\n",
    "    # transforms.Lambda(lambda x: print(type(x))),\n",
    "    # resize to 224x224\n",
    "    # transforms.Resize(IMAGE_SIZE),\n",
    "\n",
    "    #apply preprocess transform\n",
    "    # preprocess,\n",
    "    #apply : preprocess = weights.transforms()\n",
    "    #apply preprocess to input image\n",
    "    # transforms.Lambda(lambda x: preprocess(x)),\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # if torch tensor then leave as it is, else convert to tensor\n",
    "    transforms.Lambda(lambda x: x if isinstance(x, torch.Tensor) else transforms.functional.to_tensor(x)),\n",
    "    #\n",
    "\n",
    "    #resize to 224x224\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "\n",
    "    #check if channels are 1, then convert to 3 channels\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),\n",
    "\n",
    "    transforms.Lambda(lambda x: preprocess(x)),\n",
    "\n",
    "    #if channels are 3, then make them 1\n",
    "    transforms.Lambda(lambda x: x[0].unsqueeze(0) if x.shape[0] == 3 else x),\n",
    "    \n",
    "    # normalize\n",
    "    transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training we will use MNIST dataset in pytorch library\n",
    "#for testing we will use USPS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data - MNIST\n",
    "#### test data - USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "train_data = datasets.MNIST(root='./data/', download=True, transform=transform_mnist_resnet) \n",
    "#load train data\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load test data\n",
    "#USPS dataset\n",
    "test_data = datasets.USPS(root='./data/', download=True, transform=transform_mnist_resnet)\n",
    "\n",
    "\n",
    "#load test data\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([1, 224, 224])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#print the length of train and test data\n",
    "print(len(train_data))\n",
    "#print the shape of train data\n",
    "print(train_data[0][0].shape)\n",
    "#print label of train data\n",
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7291\n",
      "torch.Size([1, 224, 224])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#print length of test data\n",
    "print(len(test_data))\n",
    "#print shape of test data\n",
    "print(test_data[0][0].shape)\n",
    "#print label of test data\n",
    "print(test_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  60000\n",
      "Test size:  7291\n"
     ]
    }
   ],
   "source": [
    "#get the size of the train data and test data\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "#print\n",
    "print('Train size: ', train_size)\n",
    "print('Test size: ', test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#print shape of the data\n",
    "print('Train data shape: ', train_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the size of images in train and test data\n",
    "# train_image_size = train_data.data.shape\n",
    "# test_image_size = test_data[0][0].size()\n",
    "#print\n",
    "# print('Train image size: ', train_image_size)\n",
    "# print('Test image size: ', test_image_size)\n",
    "# train_size  = train_data.data.shape()\n",
    "# print('Train image size: ', train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #iterate through the dataset and print the dimensions of first image and then use break\n",
    "# for i, (images, labels) in enumerate(train_loader):\n",
    "#     # print('Train image size: ', images[0].size())\n",
    "#     # print(images.size())\n",
    "#     # print(labels.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we will customize resnet50 for 10 class classifier [mnist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will chnage the linear fc layer to 10 classes\n",
    "#and make the first  conv layer to 1 channel\n",
    "def  change_model(model, num_classes=10, channels=1):\n",
    "    #change the first conv layer to 1 channel\n",
    "    model.conv1 = torch.nn.Conv2d(channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    #change the last fc layer to 10 classes\n",
    "    model.fc = torch.nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now change the model\n",
    "model = change_model(model, num_classes=NUM_CLASSES, channels=CHANNELS_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send model to gpu\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will write model to tensorboard\n",
    "#we will use tensorboard to visualize the model\n",
    "#create a writer\n",
    "writer = SummaryWriter('runs/models/mnist_resnet50_classifier')\n",
    "#write the model to tensorboard\n",
    "writer.add_graph(model, torch.rand(1, 1, 224, 224).to(device))\n",
    "#close the writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the loss functoin\n",
    "#for 10 classes we will use cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will define the optimizer\n",
    "#we will use SGD optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " #function to return gradient  norm\n",
    "#write a function to calculate the gradient penalty\n",
    "def gradient_norm(dnn, current_batch):\n",
    "\n",
    "    BATCH_SIZE, C, H, W = current_batch.shape\n",
    "    #print batch size, c,h,w\n",
    "    # print(\"batch size, c, h, w\", BATCH_SIZE, C, H, W)\n",
    "    if BATCH_SIZE%2==1:\n",
    "        #remove the last element\n",
    "        current_batch = current_batch[:-1]\n",
    "    #if batch size is 0 , then just return\n",
    "    if BATCH_SIZE==0:\n",
    "        return 0\n",
    "    \n",
    "    half_batch = int(BATCH_SIZE / 2)\n",
    "    # current_batch = current_batch.to(device)\n",
    "    # current_batch = Variable(current_batch, requires_grad=True)\n",
    "    #we select the first half of the batch\n",
    "    first_half = current_batch[:half_batch]\n",
    "    #we select the second half of the batch\n",
    "    second_half = current_batch[half_batch:]\n",
    "    #we create a random number between 0 and 1\n",
    "    # alpha = torch.rand(half_batch, 1)\n",
    "    #we expand the alpha to the size of the first half of the batch\n",
    "    # alpha = alpha.expand(first_half.size())\n",
    "    #we create alpha as a random number between 0 and 1 which will allow us to interpolate between the first half and the second half\n",
    "    \n",
    "    alpha = torch.rand(half_batch, 1, 1, 1).repeat(1, C, H, W)\n",
    "    #we expand the alpha to the size of the first half of the batch\n",
    "    # alpha = alpha.expand(first_half.size())\n",
    "\n",
    "\n",
    "    #we move alpha to the device\n",
    "    alpha = alpha.to(device)\n",
    "    #we interpolate between the first half and the second half\n",
    "    interpolates = alpha * first_half + ((1 - alpha) * second_half)\n",
    "    #we move interpolates to the device\n",
    "    interpolates = interpolates.to(device)\n",
    "    # interpolates = interpolates\n",
    "    #we create a variable of interpolates\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    #we pass interpolates through the cnn\n",
    "    disc_interpolates = dnn(interpolates)\n",
    "    #we calculate the gradients\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    #we calculate the gradient penalty\n",
    "    # calculate gradient norm \n",
    "    gradients_norm = gradients.norm(2, dim=1)\n",
    "    #mean of the gradient norm without subtracting 1 or lambda\n",
    "    gradient_norm_mean = (gradients_norm **2).mean()\n",
    "    #max of sqrt of the gradient norm without subtracting 1 or lambda\n",
    "    # gradient_norm_max = (gradients_norm **2).max( dim=0, keepdim=True)[0]\n",
    "\n",
    "    #delete the variables from the memory\n",
    "    del first_half\n",
    "    del second_half\n",
    "    del alpha\n",
    "    del interpolates\n",
    "    del disc_interpolates\n",
    "    del gradients\n",
    "    del gradients_norm\n",
    "    #cache the garbage\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "    # gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()    #have to check this formula    / * LAMBDA\n",
    "    #gradient penalty  should be max(0, gradient_penalty-1)\n",
    "    #we return the gradient penalty\n",
    "    return gradient_norm_mean\n",
    "    # , gradient_norm_max\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will define the scheduler\n",
    "#we will use stepLR scheduler\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 12:06:29.349592: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "#we will use tensorboard to visualize the training\n",
    "#we will plot the loss and accuracy\n",
    "#we will also track the gradient penalty of the network\n",
    "\n",
    "#create writer for tensorboard\n",
    "writer = SummaryWriter(f'runs/'+experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will define the train function, we will also track the gradient norm of the network\n",
    "def train(model,  train_loader, optimizer, epoch=num_epochs, device = device):\n",
    "\n",
    "    #make model to train mode\n",
    "    model.train()\n",
    "    #loop for each epoch\n",
    "    epoch_tracker = 0\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batch_tracker = 0\n",
    "    #we will add th loss for each batch in the epoch and then divide by the number of batches\n",
    "    for ep in range(epoch):\n",
    "        epoch_total = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_total_loss = 0\n",
    "        #loop for each batch\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            #if first epoch and first batch then print the shape of data and label\n",
    "            if ep == 0 and batch_idx == 0:\n",
    "                print('Train data shape: ', data.shape)\n",
    "                print('Train label shape: ', target.shape)\n",
    "            #send data to gpu\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #set the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            #get the output from the model\n",
    "            output = model(data)\n",
    "            #calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            #calculate the gradients\n",
    "            loss.backward()\n",
    "            #update the weights\n",
    "\n",
    "            #we will calculate the gradient norm\n",
    "            gradient_n = gradient_norm(model, data)\n",
    "            #we will add the gradient norm to the tensorboard\n",
    "            writer.add_scalar('Gradient Norm', gradient_n, batch_tracker)\n",
    "            optimizer.step()\n",
    "            #write the loss to tensorboard\n",
    "            writer.add_scalar('Training loss', loss, global_step=batch_tracker)\n",
    "\n",
    "            #calculate the total loss\n",
    "            total_loss += loss.item()\n",
    "            #total epoch loss sum\n",
    "            epoch_total_loss += loss.item()\n",
    "\n",
    "\n",
    "            #calculate the accuracy\n",
    "            #get the max value from the output\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            #calculate the total number of labels\n",
    "            temp_total = target.size(0)\n",
    "            #calculate the correct predictions\n",
    "            temp_correct = (predicted == target).sum().item()\n",
    "            #add the total and correct predictions\n",
    "            total += temp_total\n",
    "            epoch_total += temp_total\n",
    "            correct += temp_correct\n",
    "            epoch_correct += temp_correct\n",
    "            #calculate the accuracy\n",
    "            epoch_accuracy = 100 * epoch_correct / epoch_total\n",
    "            #write the accuracy to tensorboard\n",
    "            writer.add_scalar('Training accuracy', epoch_accuracy, global_step=batch_tracker)\n",
    "            #print the loss and accuracy\n",
    "            #and\n",
    "            #print the gradient norm\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.2f}%\\tGradient Norm: {:.6f}'.format(\n",
    "                ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "                epoch_accuracy, gradient_n))\n",
    "            \n",
    "\n",
    "            \n",
    "            #print the loss\n",
    "            # if batch_idx % log_interval == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "        \n",
    "            #write the epoch loss to tensorboard\n",
    "            #first average the loss over the batches in the epoch\n",
    "            batch_tracker += 1\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "\n",
    "        #write the loss to tensorboard\n",
    "        writer.add_scalar('Training - Epoch loss', epoch_loss, global_step=ep)\n",
    "        #calculate the accuracy\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        #write the accuracy to tensorboard\n",
    "        writer.add_scalar('Training - Epoch accuracy', epoch_accuracy, global_step=ep)\n",
    "\n",
    "        # #save the model after every epoch, the name be experiment_id_epoch\n",
    "        # #wew will save in the folder saved_models\n",
    "        # torch.save(model.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'.pth')\n",
    "        # #we will also save the optimizer\n",
    "        # torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'_optimizer.pth')\n",
    "        #we will save the best model till now based on loss\n",
    "        #check if first epoch, then save the model anyway\n",
    "        if ep == 0:\n",
    "            #save the model\n",
    "            torch.save(model.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'.pth')\n",
    "            #save the optimizer\n",
    "            torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_'+str(ep)+'_optimizer.pth')\n",
    "            #save the loss\n",
    "            best_loss = epoch_total_loss\n",
    "            #save the epoch\n",
    "            best_epoch = ep\n",
    "        else:\n",
    "            #check if the loss is less than the best loss\n",
    "            if epoch_total_loss < best_loss:\n",
    "                #save the model\n",
    "                torch.save(model.state_dict(), 'saved_models/'+experiment_id+'_'+str(best_epoch)+'.pth')\n",
    "                #save the optimizer\n",
    "                torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_'+str(best_epoch)+'_optimizer.pth')\n",
    "                #save the loss\n",
    "                best_loss = epoch_loss\n",
    "                #save the epoch\n",
    "                best_epoch = ep\n",
    "       \n",
    "\n",
    "\n",
    "        epoch_tracker += 1\n",
    "    \n",
    "    #print the accuracy\n",
    "    total_accuracy = 100 * correct / total\n",
    "    print('Accuracy: ', total_accuracy)\n",
    "\n",
    "    #return the model\n",
    "    return model\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  torch.Size([64, 1, 224, 224])\n",
      "Train label shape:  torch.Size([64])\n",
      "Epoch: 0 [0/60000 (0%)]\tLoss: 2.324944\tAccuracy: 1.56%\tGradient Norm: 0.002604\n",
      "Epoch: 0 [64/60000 (0%)]\tLoss: 2.309834\tAccuracy: 7.03%\tGradient Norm: 0.002856\n",
      "Epoch: 0 [128/60000 (0%)]\tLoss: 2.323023\tAccuracy: 4.69%\tGradient Norm: 0.002425\n",
      "Epoch: 0 [192/60000 (0%)]\tLoss: 2.308043\tAccuracy: 4.30%\tGradient Norm: 0.002227\n",
      "Epoch: 0 [256/60000 (0%)]\tLoss: 2.315510\tAccuracy: 5.94%\tGradient Norm: 0.002240\n",
      "Epoch: 0 [320/60000 (1%)]\tLoss: 2.297201\tAccuracy: 7.03%\tGradient Norm: 0.002260\n",
      "Epoch: 0 [384/60000 (1%)]\tLoss: 2.299952\tAccuracy: 7.14%\tGradient Norm: 0.002525\n",
      "Epoch: 0 [448/60000 (1%)]\tLoss: 2.310394\tAccuracy: 7.03%\tGradient Norm: 0.002557\n",
      "Epoch: 0 [512/60000 (1%)]\tLoss: 2.297537\tAccuracy: 7.29%\tGradient Norm: 0.002756\n",
      "Epoch: 0 [576/60000 (1%)]\tLoss: 2.302490\tAccuracy: 7.66%\tGradient Norm: 0.002351\n",
      "Epoch: 0 [640/60000 (1%)]\tLoss: 2.293800\tAccuracy: 7.53%\tGradient Norm: 0.002608\n",
      "Epoch: 0 [704/60000 (1%)]\tLoss: 2.299428\tAccuracy: 7.81%\tGradient Norm: 0.002172\n",
      "Epoch: 0 [768/60000 (1%)]\tLoss: 2.294822\tAccuracy: 7.93%\tGradient Norm: 0.002346\n",
      "Epoch: 0 [832/60000 (1%)]\tLoss: 2.297983\tAccuracy: 7.81%\tGradient Norm: 0.002974\n",
      "Epoch: 0 [896/60000 (1%)]\tLoss: 2.271986\tAccuracy: 8.12%\tGradient Norm: 0.002784\n",
      "Epoch: 0 [960/60000 (2%)]\tLoss: 2.284151\tAccuracy: 8.11%\tGradient Norm: 0.002325\n",
      "Epoch: 0 [1024/60000 (2%)]\tLoss: 2.285505\tAccuracy: 8.46%\tGradient Norm: 0.002156\n",
      "Epoch: 0 [1088/60000 (2%)]\tLoss: 2.274391\tAccuracy: 9.03%\tGradient Norm: 0.002256\n",
      "Epoch: 0 [1152/60000 (2%)]\tLoss: 2.288818\tAccuracy: 9.29%\tGradient Norm: 0.003016\n",
      "Epoch: 0 [1216/60000 (2%)]\tLoss: 2.265971\tAccuracy: 9.92%\tGradient Norm: 0.002652\n",
      "Epoch: 0 [1280/60000 (2%)]\tLoss: 2.267157\tAccuracy: 10.19%\tGradient Norm: 0.003447\n",
      "Epoch: 0 [1344/60000 (2%)]\tLoss: 2.255059\tAccuracy: 10.80%\tGradient Norm: 0.002338\n",
      "Epoch: 0 [1408/60000 (2%)]\tLoss: 2.253317\tAccuracy: 11.01%\tGradient Norm: 0.002786\n",
      "Epoch: 0 [1472/60000 (2%)]\tLoss: 2.281285\tAccuracy: 11.07%\tGradient Norm: 0.002085\n",
      "Epoch: 0 [1536/60000 (3%)]\tLoss: 2.244317\tAccuracy: 11.44%\tGradient Norm: 0.002198\n",
      "Epoch: 0 [1600/60000 (3%)]\tLoss: 2.260010\tAccuracy: 11.48%\tGradient Norm: 0.001933\n",
      "Epoch: 0 [1664/60000 (3%)]\tLoss: 2.215524\tAccuracy: 12.21%\tGradient Norm: 0.002336\n",
      "Epoch: 0 [1728/60000 (3%)]\tLoss: 2.228319\tAccuracy: 12.50%\tGradient Norm: 0.002169\n",
      "Epoch: 0 [1792/60000 (3%)]\tLoss: 2.202352\tAccuracy: 13.04%\tGradient Norm: 0.002276\n",
      "Epoch: 0 [1856/60000 (3%)]\tLoss: 2.204820\tAccuracy: 13.54%\tGradient Norm: 0.002329\n",
      "Epoch: 0 [1920/60000 (3%)]\tLoss: 2.220939\tAccuracy: 13.96%\tGradient Norm: 0.001888\n",
      "Epoch: 0 [1984/60000 (3%)]\tLoss: 2.179078\tAccuracy: 14.60%\tGradient Norm: 0.002246\n",
      "Epoch: 0 [2048/60000 (3%)]\tLoss: 2.186388\tAccuracy: 15.39%\tGradient Norm: 0.002437\n",
      "Epoch: 0 [2112/60000 (4%)]\tLoss: 2.199046\tAccuracy: 15.85%\tGradient Norm: 0.002686\n",
      "Epoch: 0 [2176/60000 (4%)]\tLoss: 2.188041\tAccuracy: 16.43%\tGradient Norm: 0.002562\n",
      "Epoch: 0 [2240/60000 (4%)]\tLoss: 2.181338\tAccuracy: 17.14%\tGradient Norm: 0.002440\n",
      "Epoch: 0 [2304/60000 (4%)]\tLoss: 2.200711\tAccuracy: 17.65%\tGradient Norm: 0.002646\n",
      "Epoch: 0 [2368/60000 (4%)]\tLoss: 2.182112\tAccuracy: 17.89%\tGradient Norm: 0.002471\n",
      "Epoch: 0 [2432/60000 (4%)]\tLoss: 2.161359\tAccuracy: 18.47%\tGradient Norm: 0.002870\n",
      "Epoch: 0 [2496/60000 (4%)]\tLoss: 2.131712\tAccuracy: 18.95%\tGradient Norm: 0.002760\n",
      "Epoch: 0 [2560/60000 (4%)]\tLoss: 2.177887\tAccuracy: 19.21%\tGradient Norm: 0.004947\n",
      "Epoch: 0 [2624/60000 (4%)]\tLoss: 2.131307\tAccuracy: 20.01%\tGradient Norm: 0.003164\n",
      "Epoch: 0 [2688/60000 (4%)]\tLoss: 2.168546\tAccuracy: 20.24%\tGradient Norm: 0.003154\n",
      "Epoch: 0 [2752/60000 (5%)]\tLoss: 2.144860\tAccuracy: 20.60%\tGradient Norm: 0.003150\n",
      "Epoch: 0 [2816/60000 (5%)]\tLoss: 2.123168\tAccuracy: 21.08%\tGradient Norm: 0.002842\n",
      "Epoch: 0 [2880/60000 (5%)]\tLoss: 2.138308\tAccuracy: 21.54%\tGradient Norm: 0.003174\n",
      "Epoch: 0 [2944/60000 (5%)]\tLoss: 2.102486\tAccuracy: 22.11%\tGradient Norm: 0.002591\n",
      "Epoch: 0 [3008/60000 (5%)]\tLoss: 2.117790\tAccuracy: 22.43%\tGradient Norm: 0.002585\n",
      "Epoch: 0 [3072/60000 (5%)]\tLoss: 2.087912\tAccuracy: 22.96%\tGradient Norm: 0.003059\n",
      "Epoch: 0 [3136/60000 (5%)]\tLoss: 2.072934\tAccuracy: 23.47%\tGradient Norm: 0.002329\n",
      "Epoch: 0 [3200/60000 (5%)]\tLoss: 2.035504\tAccuracy: 24.26%\tGradient Norm: 0.002277\n",
      "Epoch: 0 [3264/60000 (5%)]\tLoss: 2.076765\tAccuracy: 24.76%\tGradient Norm: 0.003061\n",
      "Epoch: 0 [3328/60000 (6%)]\tLoss: 2.061367\tAccuracy: 25.27%\tGradient Norm: 0.002477\n",
      "Epoch: 0 [3392/60000 (6%)]\tLoss: 2.050597\tAccuracy: 25.81%\tGradient Norm: 0.002582\n",
      "Epoch: 0 [3456/60000 (6%)]\tLoss: 2.073857\tAccuracy: 26.19%\tGradient Norm: 0.002683\n",
      "Epoch: 0 [3520/60000 (6%)]\tLoss: 1.999930\tAccuracy: 26.79%\tGradient Norm: 0.002092\n",
      "Epoch: 0 [3584/60000 (6%)]\tLoss: 1.999388\tAccuracy: 27.41%\tGradient Norm: 0.002384\n",
      "Epoch: 0 [3648/60000 (6%)]\tLoss: 1.974616\tAccuracy: 27.94%\tGradient Norm: 0.003249\n",
      "Epoch: 0 [3712/60000 (6%)]\tLoss: 1.995299\tAccuracy: 28.31%\tGradient Norm: 0.002624\n",
      "Epoch: 0 [3776/60000 (6%)]\tLoss: 1.936173\tAccuracy: 28.98%\tGradient Norm: 0.002864\n",
      "Epoch: 0 [3840/60000 (6%)]\tLoss: 1.936786\tAccuracy: 29.41%\tGradient Norm: 0.002352\n",
      "Epoch: 0 [3904/60000 (7%)]\tLoss: 1.938328\tAccuracy: 29.89%\tGradient Norm: 0.002288\n",
      "Epoch: 0 [3968/60000 (7%)]\tLoss: 1.921712\tAccuracy: 30.26%\tGradient Norm: 0.002061\n",
      "Epoch: 0 [4032/60000 (7%)]\tLoss: 1.939429\tAccuracy: 30.64%\tGradient Norm: 0.002397\n",
      "Epoch: 0 [4096/60000 (7%)]\tLoss: 1.858024\tAccuracy: 31.23%\tGradient Norm: 0.002011\n",
      "Epoch: 0 [4160/60000 (7%)]\tLoss: 1.878849\tAccuracy: 31.77%\tGradient Norm: 0.002044\n",
      "Epoch: 0 [4224/60000 (7%)]\tLoss: 1.872741\tAccuracy: 32.28%\tGradient Norm: 0.002111\n",
      "Epoch: 0 [4288/60000 (7%)]\tLoss: 1.907686\tAccuracy: 32.77%\tGradient Norm: 0.002104\n",
      "Epoch: 0 [4352/60000 (7%)]\tLoss: 1.827089\tAccuracy: 33.29%\tGradient Norm: 0.002202\n",
      "Epoch: 0 [4416/60000 (7%)]\tLoss: 1.798315\tAccuracy: 33.75%\tGradient Norm: 0.002377\n",
      "Epoch: 0 [4480/60000 (7%)]\tLoss: 1.790255\tAccuracy: 34.15%\tGradient Norm: 0.001882\n",
      "Epoch: 0 [4544/60000 (8%)]\tLoss: 1.868811\tAccuracy: 34.46%\tGradient Norm: 0.002309\n",
      "Epoch: 0 [4608/60000 (8%)]\tLoss: 1.761755\tAccuracy: 34.91%\tGradient Norm: 0.002029\n",
      "Epoch: 0 [4672/60000 (8%)]\tLoss: 1.754926\tAccuracy: 35.41%\tGradient Norm: 0.001877\n",
      "Epoch: 0 [4736/60000 (8%)]\tLoss: 1.760954\tAccuracy: 35.92%\tGradient Norm: 0.002087\n",
      "Epoch: 0 [4800/60000 (8%)]\tLoss: 1.706707\tAccuracy: 36.35%\tGradient Norm: 0.001849\n",
      "Epoch: 0 [4864/60000 (8%)]\tLoss: 1.709810\tAccuracy: 36.81%\tGradient Norm: 0.001825\n",
      "Epoch: 0 [4928/60000 (8%)]\tLoss: 1.639739\tAccuracy: 37.28%\tGradient Norm: 0.002089\n",
      "Epoch: 0 [4992/60000 (8%)]\tLoss: 1.649274\tAccuracy: 37.68%\tGradient Norm: 0.002261\n",
      "Epoch: 0 [5056/60000 (8%)]\tLoss: 1.627361\tAccuracy: 38.11%\tGradient Norm: 0.001940\n",
      "Epoch: 0 [5120/60000 (9%)]\tLoss: 1.656545\tAccuracy: 38.39%\tGradient Norm: 0.001710\n",
      "Epoch: 0 [5184/60000 (9%)]\tLoss: 1.619309\tAccuracy: 38.78%\tGradient Norm: 0.001667\n",
      "Epoch: 0 [5248/60000 (9%)]\tLoss: 1.485081\tAccuracy: 39.34%\tGradient Norm: 0.001901\n",
      "Epoch: 0 [5312/60000 (9%)]\tLoss: 1.506257\tAccuracy: 39.84%\tGradient Norm: 0.001743\n",
      "Epoch: 0 [5376/60000 (9%)]\tLoss: 1.504643\tAccuracy: 40.31%\tGradient Norm: 0.001540\n",
      "Epoch: 0 [5440/60000 (9%)]\tLoss: 1.527397\tAccuracy: 40.66%\tGradient Norm: 0.001796\n",
      "Epoch: 0 [5504/60000 (9%)]\tLoss: 1.598207\tAccuracy: 40.98%\tGradient Norm: 0.001511\n",
      "Epoch: 0 [5568/60000 (9%)]\tLoss: 1.453249\tAccuracy: 41.42%\tGradient Norm: 0.001610\n",
      "Epoch: 0 [5632/60000 (9%)]\tLoss: 1.459012\tAccuracy: 41.77%\tGradient Norm: 0.001530\n",
      "Epoch: 0 [5696/60000 (9%)]\tLoss: 1.501821\tAccuracy: 42.01%\tGradient Norm: 0.001391\n",
      "Epoch: 0 [5760/60000 (10%)]\tLoss: 1.343088\tAccuracy: 42.45%\tGradient Norm: 0.001634\n",
      "Epoch: 0 [5824/60000 (10%)]\tLoss: 1.513675\tAccuracy: 42.80%\tGradient Norm: 0.002023\n",
      "Epoch: 0 [5888/60000 (10%)]\tLoss: 1.280216\tAccuracy: 43.16%\tGradient Norm: 0.001362\n",
      "Epoch: 0 [5952/60000 (10%)]\tLoss: 1.338208\tAccuracy: 43.57%\tGradient Norm: 0.001444\n",
      "Epoch: 0 [6016/60000 (10%)]\tLoss: 1.440887\tAccuracy: 43.82%\tGradient Norm: 0.001516\n",
      "Epoch: 0 [6080/60000 (10%)]\tLoss: 1.537142\tAccuracy: 44.04%\tGradient Norm: 0.001391\n",
      "Epoch: 0 [6144/60000 (10%)]\tLoss: 1.319589\tAccuracy: 44.36%\tGradient Norm: 0.001402\n",
      "Epoch: 0 [6208/60000 (10%)]\tLoss: 1.358479\tAccuracy: 44.64%\tGradient Norm: 0.001389\n",
      "Epoch: 0 [6272/60000 (10%)]\tLoss: 1.184293\tAccuracy: 45.03%\tGradient Norm: 0.001346\n",
      "Epoch: 0 [6336/60000 (11%)]\tLoss: 1.268821\tAccuracy: 45.38%\tGradient Norm: 0.001653\n",
      "Epoch: 0 [6400/60000 (11%)]\tLoss: 1.201923\tAccuracy: 45.78%\tGradient Norm: 0.001244\n",
      "Epoch: 0 [6464/60000 (11%)]\tLoss: 1.196427\tAccuracy: 46.17%\tGradient Norm: 0.001672\n",
      "Epoch: 0 [6528/60000 (11%)]\tLoss: 1.273097\tAccuracy: 46.54%\tGradient Norm: 0.001317\n",
      "Epoch: 0 [6592/60000 (11%)]\tLoss: 1.119573\tAccuracy: 46.92%\tGradient Norm: 0.001592\n",
      "Epoch: 0 [6656/60000 (11%)]\tLoss: 1.157515\tAccuracy: 47.29%\tGradient Norm: 0.001381\n",
      "Epoch: 0 [6720/60000 (11%)]\tLoss: 1.210663\tAccuracy: 47.61%\tGradient Norm: 0.001350\n",
      "Epoch: 0 [6784/60000 (11%)]\tLoss: 1.139429\tAccuracy: 47.94%\tGradient Norm: 0.001271\n",
      "Epoch: 0 [6848/60000 (11%)]\tLoss: 1.036568\tAccuracy: 48.34%\tGradient Norm: 0.001255\n",
      "Epoch: 0 [6912/60000 (12%)]\tLoss: 1.056693\tAccuracy: 48.71%\tGradient Norm: 0.001419\n",
      "Epoch: 0 [6976/60000 (12%)]\tLoss: 1.079902\tAccuracy: 49.05%\tGradient Norm: 0.001382\n",
      "Epoch: 0 [7040/60000 (12%)]\tLoss: 1.135538\tAccuracy: 49.32%\tGradient Norm: 0.001407\n",
      "Epoch: 0 [7104/60000 (12%)]\tLoss: 0.977614\tAccuracy: 49.62%\tGradient Norm: 0.001355\n",
      "Epoch: 0 [7168/60000 (12%)]\tLoss: 1.079648\tAccuracy: 49.92%\tGradient Norm: 0.001275\n",
      "Epoch: 0 [7232/60000 (12%)]\tLoss: 0.873651\tAccuracy: 50.27%\tGradient Norm: 0.001070\n",
      "Epoch: 0 [7296/60000 (12%)]\tLoss: 0.948944\tAccuracy: 50.61%\tGradient Norm: 0.001200\n",
      "Epoch: 0 [7360/60000 (12%)]\tLoss: 0.904795\tAccuracy: 50.93%\tGradient Norm: 0.001258\n",
      "Epoch: 0 [7424/60000 (12%)]\tLoss: 1.007674\tAccuracy: 51.24%\tGradient Norm: 0.001155\n",
      "Epoch: 0 [7488/60000 (12%)]\tLoss: 0.952496\tAccuracy: 51.54%\tGradient Norm: 0.001087\n",
      "Epoch: 0 [7552/60000 (13%)]\tLoss: 0.995678\tAccuracy: 51.85%\tGradient Norm: 0.001105\n",
      "Epoch: 0 [7616/60000 (13%)]\tLoss: 0.884772\tAccuracy: 52.12%\tGradient Norm: 0.001175\n",
      "Epoch: 0 [7680/60000 (13%)]\tLoss: 0.902736\tAccuracy: 52.40%\tGradient Norm: 0.001052\n",
      "Epoch: 0 [7744/60000 (13%)]\tLoss: 1.013087\tAccuracy: 52.64%\tGradient Norm: 0.000877\n",
      "Epoch: 0 [7808/60000 (13%)]\tLoss: 0.885467\tAccuracy: 52.91%\tGradient Norm: 0.000935\n",
      "Epoch: 0 [7872/60000 (13%)]\tLoss: 0.767044\tAccuracy: 53.23%\tGradient Norm: 0.000929\n",
      "Epoch: 0 [7936/60000 (13%)]\tLoss: 0.997295\tAccuracy: 53.41%\tGradient Norm: 0.001120\n",
      "Epoch: 0 [8000/60000 (13%)]\tLoss: 0.910861\tAccuracy: 53.66%\tGradient Norm: 0.000917\n",
      "Epoch: 0 [8064/60000 (13%)]\tLoss: 0.827657\tAccuracy: 53.94%\tGradient Norm: 0.000969\n",
      "Epoch: 0 [8128/60000 (14%)]\tLoss: 0.846396\tAccuracy: 54.20%\tGradient Norm: 0.000916\n",
      "Epoch: 0 [8192/60000 (14%)]\tLoss: 0.735628\tAccuracy: 54.49%\tGradient Norm: 0.001027\n",
      "Epoch: 0 [8256/60000 (14%)]\tLoss: 0.784853\tAccuracy: 54.75%\tGradient Norm: 0.001261\n",
      "Epoch: 0 [8320/60000 (14%)]\tLoss: 0.706338\tAccuracy: 55.01%\tGradient Norm: 0.001080\n",
      "Epoch: 0 [8384/60000 (14%)]\tLoss: 0.663584\tAccuracy: 55.30%\tGradient Norm: 0.000911\n",
      "Epoch: 0 [8448/60000 (14%)]\tLoss: 0.645324\tAccuracy: 55.62%\tGradient Norm: 0.000994\n",
      "Epoch: 0 [8512/60000 (14%)]\tLoss: 0.679946\tAccuracy: 55.87%\tGradient Norm: 0.000977\n",
      "Epoch: 0 [8576/60000 (14%)]\tLoss: 0.729552\tAccuracy: 56.15%\tGradient Norm: 0.001046\n",
      "Epoch: 0 [8640/60000 (14%)]\tLoss: 0.695832\tAccuracy: 56.39%\tGradient Norm: 0.000931\n",
      "Epoch: 0 [8704/60000 (14%)]\tLoss: 0.564232\tAccuracy: 56.68%\tGradient Norm: 0.000976\n",
      "Epoch: 0 [8768/60000 (15%)]\tLoss: 0.666614\tAccuracy: 56.93%\tGradient Norm: 0.000950\n",
      "Epoch: 0 [8832/60000 (15%)]\tLoss: 0.745586\tAccuracy: 57.14%\tGradient Norm: 0.000856\n",
      "Epoch: 0 [8896/60000 (15%)]\tLoss: 0.603006\tAccuracy: 57.39%\tGradient Norm: 0.001010\n",
      "Epoch: 0 [8960/60000 (15%)]\tLoss: 0.597671\tAccuracy: 57.65%\tGradient Norm: 0.000865\n",
      "Epoch: 0 [9024/60000 (15%)]\tLoss: 0.703057\tAccuracy: 57.85%\tGradient Norm: 0.000908\n",
      "Epoch: 0 [9088/60000 (15%)]\tLoss: 0.736659\tAccuracy: 58.03%\tGradient Norm: 0.000935\n",
      "Epoch: 0 [9152/60000 (15%)]\tLoss: 0.627570\tAccuracy: 58.27%\tGradient Norm: 0.001014\n",
      "Epoch: 0 [9216/60000 (15%)]\tLoss: 0.535011\tAccuracy: 58.52%\tGradient Norm: 0.000858\n",
      "Epoch: 0 [9280/60000 (15%)]\tLoss: 0.505470\tAccuracy: 58.75%\tGradient Norm: 0.000909\n",
      "Epoch: 0 [9344/60000 (16%)]\tLoss: 0.559475\tAccuracy: 58.98%\tGradient Norm: 0.000944\n",
      "Epoch: 0 [9408/60000 (16%)]\tLoss: 0.503567\tAccuracy: 59.23%\tGradient Norm: 0.001054\n",
      "Epoch: 0 [9472/60000 (16%)]\tLoss: 0.481224\tAccuracy: 59.45%\tGradient Norm: 0.000974\n",
      "Epoch: 0 [9536/60000 (16%)]\tLoss: 0.550472\tAccuracy: 59.67%\tGradient Norm: 0.001045\n",
      "Epoch: 0 [9600/60000 (16%)]\tLoss: 0.595113\tAccuracy: 59.86%\tGradient Norm: 0.000875\n",
      "Epoch: 0 [9664/60000 (16%)]\tLoss: 0.522774\tAccuracy: 60.05%\tGradient Norm: 0.000909\n",
      "Epoch: 0 [9728/60000 (16%)]\tLoss: 0.583284\tAccuracy: 60.26%\tGradient Norm: 0.001008\n",
      "Epoch: 0 [9792/60000 (16%)]\tLoss: 0.389532\tAccuracy: 60.52%\tGradient Norm: 0.000874\n",
      "Epoch: 0 [9856/60000 (16%)]\tLoss: 0.542555\tAccuracy: 60.75%\tGradient Norm: 0.000828\n",
      "Epoch: 0 [9920/60000 (17%)]\tLoss: 0.438660\tAccuracy: 60.96%\tGradient Norm: 0.000717\n",
      "Epoch: 0 [9984/60000 (17%)]\tLoss: 0.559534\tAccuracy: 61.15%\tGradient Norm: 0.000884\n",
      "Epoch: 0 [10048/60000 (17%)]\tLoss: 0.506869\tAccuracy: 61.36%\tGradient Norm: 0.000731\n",
      "Epoch: 0 [10112/60000 (17%)]\tLoss: 0.468112\tAccuracy: 61.54%\tGradient Norm: 0.000829\n",
      "Epoch: 0 [10176/60000 (17%)]\tLoss: 0.372905\tAccuracy: 61.75%\tGradient Norm: 0.000745\n",
      "Epoch: 0 [10240/60000 (17%)]\tLoss: 0.452571\tAccuracy: 61.94%\tGradient Norm: 0.000813\n",
      "Epoch: 0 [10304/60000 (17%)]\tLoss: 0.420482\tAccuracy: 62.12%\tGradient Norm: 0.000711\n",
      "Epoch: 0 [10368/60000 (17%)]\tLoss: 0.464937\tAccuracy: 62.30%\tGradient Norm: 0.000878\n",
      "Epoch: 0 [10432/60000 (17%)]\tLoss: 0.422800\tAccuracy: 62.47%\tGradient Norm: 0.000822\n",
      "Epoch: 0 [10496/60000 (17%)]\tLoss: 0.456193\tAccuracy: 62.65%\tGradient Norm: 0.000904\n",
      "Epoch: 0 [10560/60000 (18%)]\tLoss: 0.461382\tAccuracy: 62.85%\tGradient Norm: 0.000785\n",
      "Epoch: 0 [10624/60000 (18%)]\tLoss: 0.447378\tAccuracy: 63.02%\tGradient Norm: 0.000815\n",
      "Epoch: 0 [10688/60000 (18%)]\tLoss: 0.325532\tAccuracy: 63.21%\tGradient Norm: 0.000768\n",
      "Epoch: 0 [10752/60000 (18%)]\tLoss: 0.360633\tAccuracy: 63.40%\tGradient Norm: 0.000684\n",
      "Epoch: 0 [10816/60000 (18%)]\tLoss: 0.301838\tAccuracy: 63.59%\tGradient Norm: 0.000600\n",
      "Epoch: 0 [10880/60000 (18%)]\tLoss: 0.303076\tAccuracy: 63.79%\tGradient Norm: 0.000720\n",
      "Epoch: 0 [10944/60000 (18%)]\tLoss: 0.420115\tAccuracy: 63.93%\tGradient Norm: 0.000629\n",
      "Epoch: 0 [11008/60000 (18%)]\tLoss: 0.424002\tAccuracy: 64.09%\tGradient Norm: 0.000702\n",
      "Epoch: 0 [11072/60000 (18%)]\tLoss: 0.333130\tAccuracy: 64.27%\tGradient Norm: 0.000695\n",
      "Epoch: 0 [11136/60000 (19%)]\tLoss: 0.437463\tAccuracy: 64.42%\tGradient Norm: 0.000758\n",
      "Epoch: 0 [11200/60000 (19%)]\tLoss: 0.419200\tAccuracy: 64.60%\tGradient Norm: 0.000767\n",
      "Epoch: 0 [11264/60000 (19%)]\tLoss: 0.350739\tAccuracy: 64.76%\tGradient Norm: 0.000726\n",
      "Epoch: 0 [11328/60000 (19%)]\tLoss: 0.317160\tAccuracy: 64.93%\tGradient Norm: 0.000679\n",
      "Epoch: 0 [11392/60000 (19%)]\tLoss: 0.416121\tAccuracy: 65.08%\tGradient Norm: 0.000683\n",
      "Epoch: 0 [11456/60000 (19%)]\tLoss: 0.416364\tAccuracy: 65.21%\tGradient Norm: 0.000679\n",
      "Epoch: 0 [11520/60000 (19%)]\tLoss: 0.333729\tAccuracy: 65.37%\tGradient Norm: 0.000685\n",
      "Epoch: 0 [11584/60000 (19%)]\tLoss: 0.359826\tAccuracy: 65.51%\tGradient Norm: 0.000728\n",
      "Epoch: 0 [11648/60000 (19%)]\tLoss: 0.258294\tAccuracy: 65.69%\tGradient Norm: 0.000673\n",
      "Epoch: 0 [11712/60000 (20%)]\tLoss: 0.267510\tAccuracy: 65.86%\tGradient Norm: 0.000675\n",
      "Epoch: 0 [11776/60000 (20%)]\tLoss: 0.340079\tAccuracy: 66.00%\tGradient Norm: 0.000658\n",
      "Epoch: 0 [11840/60000 (20%)]\tLoss: 0.344468\tAccuracy: 66.15%\tGradient Norm: 0.000702\n",
      "Epoch: 0 [11904/60000 (20%)]\tLoss: 0.262541\tAccuracy: 66.32%\tGradient Norm: 0.000757\n",
      "Epoch: 0 [11968/60000 (20%)]\tLoss: 0.351713\tAccuracy: 66.44%\tGradient Norm: 0.000589\n",
      "Epoch: 0 [12032/60000 (20%)]\tLoss: 0.249123\tAccuracy: 66.60%\tGradient Norm: 0.000690\n",
      "Epoch: 0 [12096/60000 (20%)]\tLoss: 0.217098\tAccuracy: 66.77%\tGradient Norm: 0.000654\n",
      "Epoch: 0 [12160/60000 (20%)]\tLoss: 0.306275\tAccuracy: 66.93%\tGradient Norm: 0.000818\n",
      "Epoch: 0 [12224/60000 (20%)]\tLoss: 0.258187\tAccuracy: 67.10%\tGradient Norm: 0.000702\n",
      "Epoch: 0 [12288/60000 (20%)]\tLoss: 0.397554\tAccuracy: 67.21%\tGradient Norm: 0.000561\n",
      "Epoch: 0 [12352/60000 (21%)]\tLoss: 0.274454\tAccuracy: 67.36%\tGradient Norm: 0.000697\n",
      "Epoch: 0 [12416/60000 (21%)]\tLoss: 0.446596\tAccuracy: 67.48%\tGradient Norm: 0.000636\n",
      "Epoch: 0 [12480/60000 (21%)]\tLoss: 0.303587\tAccuracy: 67.61%\tGradient Norm: 0.000602\n",
      "Epoch: 0 [12544/60000 (21%)]\tLoss: 0.282821\tAccuracy: 67.76%\tGradient Norm: 0.000739\n",
      "Epoch: 0 [12608/60000 (21%)]\tLoss: 0.246190\tAccuracy: 67.89%\tGradient Norm: 0.000638\n",
      "Epoch: 0 [12672/60000 (21%)]\tLoss: 0.289853\tAccuracy: 68.04%\tGradient Norm: 0.000597\n",
      "Epoch: 0 [12736/60000 (21%)]\tLoss: 0.350274\tAccuracy: 68.16%\tGradient Norm: 0.000538\n",
      "Epoch: 0 [12800/60000 (21%)]\tLoss: 0.216971\tAccuracy: 68.31%\tGradient Norm: 0.000565\n",
      "Epoch: 0 [12864/60000 (21%)]\tLoss: 0.284999\tAccuracy: 68.45%\tGradient Norm: 0.000647\n",
      "Epoch: 0 [12928/60000 (22%)]\tLoss: 0.253191\tAccuracy: 68.58%\tGradient Norm: 0.000691\n",
      "Epoch: 0 [12992/60000 (22%)]\tLoss: 0.275310\tAccuracy: 68.72%\tGradient Norm: 0.000714\n",
      "Epoch: 0 [13056/60000 (22%)]\tLoss: 0.227241\tAccuracy: 68.85%\tGradient Norm: 0.000722\n",
      "Epoch: 0 [13120/60000 (22%)]\tLoss: 0.220232\tAccuracy: 68.99%\tGradient Norm: 0.000685\n",
      "Epoch: 0 [13184/60000 (22%)]\tLoss: 0.281742\tAccuracy: 69.11%\tGradient Norm: 0.000617\n",
      "Epoch: 0 [13248/60000 (22%)]\tLoss: 0.234024\tAccuracy: 69.26%\tGradient Norm: 0.000521\n",
      "Epoch: 0 [13312/60000 (22%)]\tLoss: 0.390027\tAccuracy: 69.35%\tGradient Norm: 0.000587\n",
      "Epoch: 0 [13376/60000 (22%)]\tLoss: 0.235732\tAccuracy: 69.46%\tGradient Norm: 0.000685\n",
      "Epoch: 0 [13440/60000 (22%)]\tLoss: 0.224989\tAccuracy: 69.58%\tGradient Norm: 0.000526\n",
      "Epoch: 0 [13504/60000 (22%)]\tLoss: 0.216595\tAccuracy: 69.71%\tGradient Norm: 0.000698\n",
      "Epoch: 0 [13568/60000 (23%)]\tLoss: 0.212946\tAccuracy: 69.84%\tGradient Norm: 0.000616\n",
      "Epoch: 0 [13632/60000 (23%)]\tLoss: 0.387937\tAccuracy: 69.95%\tGradient Norm: 0.000671\n",
      "Epoch: 0 [13696/60000 (23%)]\tLoss: 0.293204\tAccuracy: 70.06%\tGradient Norm: 0.000634\n",
      "Epoch: 0 [13760/60000 (23%)]\tLoss: 0.137397\tAccuracy: 70.20%\tGradient Norm: 0.000596\n",
      "Epoch: 0 [13824/60000 (23%)]\tLoss: 0.221542\tAccuracy: 70.32%\tGradient Norm: 0.000541\n",
      "Epoch: 0 [13888/60000 (23%)]\tLoss: 0.286179\tAccuracy: 70.43%\tGradient Norm: 0.000567\n",
      "Epoch: 0 [13952/60000 (23%)]\tLoss: 0.211423\tAccuracy: 70.56%\tGradient Norm: 0.000655\n",
      "Epoch: 0 [14016/60000 (23%)]\tLoss: 0.233570\tAccuracy: 70.68%\tGradient Norm: 0.000543\n",
      "Epoch: 0 [14080/60000 (23%)]\tLoss: 0.222837\tAccuracy: 70.80%\tGradient Norm: 0.000504\n",
      "Epoch: 0 [14144/60000 (24%)]\tLoss: 0.421078\tAccuracy: 70.88%\tGradient Norm: 0.000643\n",
      "Epoch: 0 [14208/60000 (24%)]\tLoss: 0.205213\tAccuracy: 70.99%\tGradient Norm: 0.000634\n",
      "Epoch: 0 [14272/60000 (24%)]\tLoss: 0.225772\tAccuracy: 71.10%\tGradient Norm: 0.000607\n",
      "Epoch: 0 [14336/60000 (24%)]\tLoss: 0.260077\tAccuracy: 71.20%\tGradient Norm: 0.000597\n",
      "Epoch: 0 [14400/60000 (24%)]\tLoss: 0.266143\tAccuracy: 71.30%\tGradient Norm: 0.000558\n",
      "Epoch: 0 [14464/60000 (24%)]\tLoss: 0.182513\tAccuracy: 71.41%\tGradient Norm: 0.000546\n",
      "Epoch: 0 [14528/60000 (24%)]\tLoss: 0.248145\tAccuracy: 71.50%\tGradient Norm: 0.000627\n",
      "Epoch: 0 [14592/60000 (24%)]\tLoss: 0.199370\tAccuracy: 71.61%\tGradient Norm: 0.000547\n",
      "Epoch: 0 [14656/60000 (24%)]\tLoss: 0.197489\tAccuracy: 71.72%\tGradient Norm: 0.000614\n",
      "Epoch: 0 [14720/60000 (25%)]\tLoss: 0.272632\tAccuracy: 71.81%\tGradient Norm: 0.000550\n",
      "Epoch: 0 [14784/60000 (25%)]\tLoss: 0.178128\tAccuracy: 71.92%\tGradient Norm: 0.000566\n",
      "Epoch: 0 [14848/60000 (25%)]\tLoss: 0.229849\tAccuracy: 72.01%\tGradient Norm: 0.000630\n",
      "Epoch: 0 [14912/60000 (25%)]\tLoss: 0.286331\tAccuracy: 72.11%\tGradient Norm: 0.000530\n",
      "Epoch: 0 [14976/60000 (25%)]\tLoss: 0.196372\tAccuracy: 72.21%\tGradient Norm: 0.000637\n",
      "Epoch: 0 [15040/60000 (25%)]\tLoss: 0.217481\tAccuracy: 72.31%\tGradient Norm: 0.000571\n",
      "Epoch: 0 [15104/60000 (25%)]\tLoss: 0.231378\tAccuracy: 72.42%\tGradient Norm: 0.000582\n",
      "Epoch: 0 [15168/60000 (25%)]\tLoss: 0.262751\tAccuracy: 72.51%\tGradient Norm: 0.000540\n",
      "Epoch: 0 [15232/60000 (25%)]\tLoss: 0.238030\tAccuracy: 72.60%\tGradient Norm: 0.000601\n",
      "Epoch: 0 [15296/60000 (25%)]\tLoss: 0.184939\tAccuracy: 72.70%\tGradient Norm: 0.000602\n",
      "Epoch: 0 [15360/60000 (26%)]\tLoss: 0.155465\tAccuracy: 72.80%\tGradient Norm: 0.000674\n",
      "Epoch: 0 [15424/60000 (26%)]\tLoss: 0.257685\tAccuracy: 72.90%\tGradient Norm: 0.000698\n",
      "Epoch: 0 [15488/60000 (26%)]\tLoss: 0.271989\tAccuracy: 72.99%\tGradient Norm: 0.000686\n",
      "Epoch: 0 [15552/60000 (26%)]\tLoss: 0.219268\tAccuracy: 73.09%\tGradient Norm: 0.000565\n",
      "Epoch: 0 [15616/60000 (26%)]\tLoss: 0.169773\tAccuracy: 73.19%\tGradient Norm: 0.000622\n",
      "Epoch: 0 [15680/60000 (26%)]\tLoss: 0.273174\tAccuracy: 73.27%\tGradient Norm: 0.000602\n",
      "Epoch: 0 [15744/60000 (26%)]\tLoss: 0.176212\tAccuracy: 73.37%\tGradient Norm: 0.000570\n",
      "Epoch: 0 [15808/60000 (26%)]\tLoss: 0.188025\tAccuracy: 73.46%\tGradient Norm: 0.000516\n",
      "Epoch: 0 [15872/60000 (26%)]\tLoss: 0.220938\tAccuracy: 73.54%\tGradient Norm: 0.000677\n",
      "Epoch: 0 [15936/60000 (27%)]\tLoss: 0.171046\tAccuracy: 73.64%\tGradient Norm: 0.000552\n",
      "Epoch: 0 [16000/60000 (27%)]\tLoss: 0.215929\tAccuracy: 73.73%\tGradient Norm: 0.000465\n",
      "Epoch: 0 [16064/60000 (27%)]\tLoss: 0.186482\tAccuracy: 73.83%\tGradient Norm: 0.000656\n",
      "Epoch: 0 [16128/60000 (27%)]\tLoss: 0.164959\tAccuracy: 73.92%\tGradient Norm: 0.000574\n",
      "Epoch: 0 [16192/60000 (27%)]\tLoss: 0.181235\tAccuracy: 74.02%\tGradient Norm: 0.000590\n",
      "Epoch: 0 [16256/60000 (27%)]\tLoss: 0.232193\tAccuracy: 74.10%\tGradient Norm: 0.000550\n",
      "Epoch: 0 [16320/60000 (27%)]\tLoss: 0.188339\tAccuracy: 74.18%\tGradient Norm: 0.000572\n",
      "Epoch: 0 [16384/60000 (27%)]\tLoss: 0.224279\tAccuracy: 74.25%\tGradient Norm: 0.000636\n",
      "Epoch: 0 [16448/60000 (27%)]\tLoss: 0.194520\tAccuracy: 74.33%\tGradient Norm: 0.000582\n",
      "Epoch: 0 [16512/60000 (28%)]\tLoss: 0.246480\tAccuracy: 74.41%\tGradient Norm: 0.000440\n",
      "Epoch: 0 [16576/60000 (28%)]\tLoss: 0.189945\tAccuracy: 74.50%\tGradient Norm: 0.000555\n",
      "Epoch: 0 [16640/60000 (28%)]\tLoss: 0.132347\tAccuracy: 74.59%\tGradient Norm: 0.000478\n",
      "Epoch: 0 [16704/60000 (28%)]\tLoss: 0.214026\tAccuracy: 74.66%\tGradient Norm: 0.000498\n",
      "Epoch: 0 [16768/60000 (28%)]\tLoss: 0.191848\tAccuracy: 74.74%\tGradient Norm: 0.000543\n",
      "Epoch: 0 [16832/60000 (28%)]\tLoss: 0.104027\tAccuracy: 74.83%\tGradient Norm: 0.000527\n",
      "Epoch: 0 [16896/60000 (28%)]\tLoss: 0.328906\tAccuracy: 74.91%\tGradient Norm: 0.000513\n",
      "Epoch: 0 [16960/60000 (28%)]\tLoss: 0.110880\tAccuracy: 75.00%\tGradient Norm: 0.000486\n",
      "Epoch: 0 [17024/60000 (28%)]\tLoss: 0.143767\tAccuracy: 75.08%\tGradient Norm: 0.000540\n",
      "Epoch: 0 [17088/60000 (28%)]\tLoss: 0.247166\tAccuracy: 75.15%\tGradient Norm: 0.000429\n",
      "Epoch: 0 [17152/60000 (29%)]\tLoss: 0.103358\tAccuracy: 75.24%\tGradient Norm: 0.000488\n",
      "Epoch: 0 [17216/60000 (29%)]\tLoss: 0.195833\tAccuracy: 75.31%\tGradient Norm: 0.000500\n",
      "Epoch: 0 [17280/60000 (29%)]\tLoss: 0.145244\tAccuracy: 75.40%\tGradient Norm: 0.000496\n",
      "Epoch: 0 [17344/60000 (29%)]\tLoss: 0.171368\tAccuracy: 75.48%\tGradient Norm: 0.000507\n",
      "Epoch: 0 [17408/60000 (29%)]\tLoss: 0.126867\tAccuracy: 75.57%\tGradient Norm: 0.000412\n",
      "Epoch: 0 [17472/60000 (29%)]\tLoss: 0.158986\tAccuracy: 75.64%\tGradient Norm: 0.000492\n",
      "Epoch: 0 [17536/60000 (29%)]\tLoss: 0.167861\tAccuracy: 75.72%\tGradient Norm: 0.000549\n",
      "Epoch: 0 [17600/60000 (29%)]\tLoss: 0.159029\tAccuracy: 75.80%\tGradient Norm: 0.000528\n",
      "Epoch: 0 [17664/60000 (29%)]\tLoss: 0.157553\tAccuracy: 75.87%\tGradient Norm: 0.000486\n",
      "Epoch: 0 [17728/60000 (30%)]\tLoss: 0.113933\tAccuracy: 75.96%\tGradient Norm: 0.000436\n",
      "Epoch: 0 [17792/60000 (30%)]\tLoss: 0.182863\tAccuracy: 76.02%\tGradient Norm: 0.000402\n",
      "Epoch: 0 [17856/60000 (30%)]\tLoss: 0.163753\tAccuracy: 76.10%\tGradient Norm: 0.000456\n",
      "Epoch: 0 [17920/60000 (30%)]\tLoss: 0.111379\tAccuracy: 76.18%\tGradient Norm: 0.000485\n",
      "Epoch: 0 [17984/60000 (30%)]\tLoss: 0.197660\tAccuracy: 76.25%\tGradient Norm: 0.000554\n",
      "Epoch: 0 [18048/60000 (30%)]\tLoss: 0.124469\tAccuracy: 76.33%\tGradient Norm: 0.000445\n",
      "Epoch: 0 [18112/60000 (30%)]\tLoss: 0.160511\tAccuracy: 76.40%\tGradient Norm: 0.000471\n",
      "Epoch: 0 [18176/60000 (30%)]\tLoss: 0.092729\tAccuracy: 76.49%\tGradient Norm: 0.000491\n",
      "Epoch: 0 [18240/60000 (30%)]\tLoss: 0.158249\tAccuracy: 76.56%\tGradient Norm: 0.000445\n",
      "Epoch: 0 [18304/60000 (30%)]\tLoss: 0.254735\tAccuracy: 76.61%\tGradient Norm: 0.000524\n",
      "Epoch: 0 [18368/60000 (31%)]\tLoss: 0.085803\tAccuracy: 76.69%\tGradient Norm: 0.000538\n",
      "Epoch: 0 [18432/60000 (31%)]\tLoss: 0.083092\tAccuracy: 76.77%\tGradient Norm: 0.000492\n",
      "Epoch: 0 [18496/60000 (31%)]\tLoss: 0.190917\tAccuracy: 76.84%\tGradient Norm: 0.000654\n",
      "Epoch: 0 [18560/60000 (31%)]\tLoss: 0.100278\tAccuracy: 76.91%\tGradient Norm: 0.000741\n",
      "Epoch: 0 [18624/60000 (31%)]\tLoss: 0.152563\tAccuracy: 76.97%\tGradient Norm: 0.000510\n",
      "Epoch: 0 [18688/60000 (31%)]\tLoss: 0.112381\tAccuracy: 77.05%\tGradient Norm: 0.000508\n",
      "Epoch: 0 [18752/60000 (31%)]\tLoss: 0.153491\tAccuracy: 77.11%\tGradient Norm: 0.000573\n",
      "Epoch: 0 [18816/60000 (31%)]\tLoss: 0.129319\tAccuracy: 77.17%\tGradient Norm: 0.000544\n",
      "Epoch: 0 [18880/60000 (31%)]\tLoss: 0.110818\tAccuracy: 77.24%\tGradient Norm: 0.000510\n",
      "Epoch: 0 [18944/60000 (32%)]\tLoss: 0.177135\tAccuracy: 77.30%\tGradient Norm: 0.000533\n",
      "Epoch: 0 [19008/60000 (32%)]\tLoss: 0.182176\tAccuracy: 77.36%\tGradient Norm: 0.000555\n",
      "Epoch: 0 [19072/60000 (32%)]\tLoss: 0.181399\tAccuracy: 77.42%\tGradient Norm: 0.000564\n",
      "Epoch: 0 [19136/60000 (32%)]\tLoss: 0.133315\tAccuracy: 77.48%\tGradient Norm: 0.000468\n",
      "Epoch: 0 [19200/60000 (32%)]\tLoss: 0.099320\tAccuracy: 77.55%\tGradient Norm: 0.000502\n",
      "Epoch: 0 [19264/60000 (32%)]\tLoss: 0.174741\tAccuracy: 77.61%\tGradient Norm: 0.000532\n",
      "Epoch: 0 [19328/60000 (32%)]\tLoss: 0.092960\tAccuracy: 77.68%\tGradient Norm: 0.000557\n",
      "Epoch: 0 [19392/60000 (32%)]\tLoss: 0.075480\tAccuracy: 77.75%\tGradient Norm: 0.000503\n",
      "Epoch: 0 [19456/60000 (32%)]\tLoss: 0.111168\tAccuracy: 77.82%\tGradient Norm: 0.000555\n",
      "Epoch: 0 [19520/60000 (33%)]\tLoss: 0.081870\tAccuracy: 77.89%\tGradient Norm: 0.000474\n",
      "Epoch: 0 [19584/60000 (33%)]\tLoss: 0.273485\tAccuracy: 77.94%\tGradient Norm: 0.000491\n",
      "Epoch: 0 [19648/60000 (33%)]\tLoss: 0.145197\tAccuracy: 77.99%\tGradient Norm: 0.000466\n",
      "Epoch: 0 [19712/60000 (33%)]\tLoss: 0.101163\tAccuracy: 78.06%\tGradient Norm: 0.000521\n",
      "Epoch: 0 [19776/60000 (33%)]\tLoss: 0.243956\tAccuracy: 78.11%\tGradient Norm: 0.000507\n",
      "Epoch: 0 [19840/60000 (33%)]\tLoss: 0.052988\tAccuracy: 78.18%\tGradient Norm: 0.000488\n",
      "Epoch: 0 [19904/60000 (33%)]\tLoss: 0.279905\tAccuracy: 78.22%\tGradient Norm: 0.000571\n",
      "Epoch: 0 [19968/60000 (33%)]\tLoss: 0.156960\tAccuracy: 78.27%\tGradient Norm: 0.000540\n",
      "Epoch: 0 [20032/60000 (33%)]\tLoss: 0.137982\tAccuracy: 78.33%\tGradient Norm: 0.000560\n",
      "Epoch: 0 [20096/60000 (33%)]\tLoss: 0.164600\tAccuracy: 78.39%\tGradient Norm: 0.000431\n",
      "Epoch: 0 [20160/60000 (34%)]\tLoss: 0.111425\tAccuracy: 78.45%\tGradient Norm: 0.000550\n",
      "Epoch: 0 [20224/60000 (34%)]\tLoss: 0.119990\tAccuracy: 78.51%\tGradient Norm: 0.000521\n",
      "Epoch: 0 [20288/60000 (34%)]\tLoss: 0.115747\tAccuracy: 78.57%\tGradient Norm: 0.000563\n",
      "Epoch: 0 [20352/60000 (34%)]\tLoss: 0.093903\tAccuracy: 78.63%\tGradient Norm: 0.000596\n",
      "Epoch: 0 [20416/60000 (34%)]\tLoss: 0.080348\tAccuracy: 78.70%\tGradient Norm: 0.000479\n",
      "Epoch: 0 [20480/60000 (34%)]\tLoss: 0.062764\tAccuracy: 78.77%\tGradient Norm: 0.000465\n",
      "Epoch: 0 [20544/60000 (34%)]\tLoss: 0.105305\tAccuracy: 78.83%\tGradient Norm: 0.000513\n",
      "Epoch: 0 [20608/60000 (34%)]\tLoss: 0.116018\tAccuracy: 78.89%\tGradient Norm: 0.000552\n",
      "Epoch: 0 [20672/60000 (34%)]\tLoss: 0.086608\tAccuracy: 78.94%\tGradient Norm: 0.000528\n",
      "Epoch: 0 [20736/60000 (35%)]\tLoss: 0.080127\tAccuracy: 79.00%\tGradient Norm: 0.000498\n",
      "Epoch: 0 [20800/60000 (35%)]\tLoss: 0.103899\tAccuracy: 79.06%\tGradient Norm: 0.000525\n",
      "Epoch: 0 [20864/60000 (35%)]\tLoss: 0.055624\tAccuracy: 79.13%\tGradient Norm: 0.000514\n",
      "Epoch: 0 [20928/60000 (35%)]\tLoss: 0.112791\tAccuracy: 79.19%\tGradient Norm: 0.000541\n",
      "Epoch: 0 [20992/60000 (35%)]\tLoss: 0.176787\tAccuracy: 79.24%\tGradient Norm: 0.000518\n",
      "Epoch: 0 [21056/60000 (35%)]\tLoss: 0.181430\tAccuracy: 79.28%\tGradient Norm: 0.000461\n",
      "Epoch: 0 [21120/60000 (35%)]\tLoss: 0.102833\tAccuracy: 79.34%\tGradient Norm: 0.000564\n",
      "Epoch: 0 [21184/60000 (35%)]\tLoss: 0.051138\tAccuracy: 79.41%\tGradient Norm: 0.000558\n",
      "Epoch: 0 [21248/60000 (35%)]\tLoss: 0.189801\tAccuracy: 79.45%\tGradient Norm: 0.000444\n",
      "Epoch: 0 [21312/60000 (36%)]\tLoss: 0.141444\tAccuracy: 79.51%\tGradient Norm: 0.000476\n",
      "Epoch: 0 [21376/60000 (36%)]\tLoss: 0.120735\tAccuracy: 79.56%\tGradient Norm: 0.000498\n",
      "Epoch: 0 [21440/60000 (36%)]\tLoss: 0.057914\tAccuracy: 79.62%\tGradient Norm: 0.000455\n",
      "Epoch: 0 [21504/60000 (36%)]\tLoss: 0.132203\tAccuracy: 79.67%\tGradient Norm: 0.000587\n",
      "Epoch: 0 [21568/60000 (36%)]\tLoss: 0.099188\tAccuracy: 79.73%\tGradient Norm: 0.000471\n",
      "Epoch: 0 [21632/60000 (36%)]\tLoss: 0.095065\tAccuracy: 79.78%\tGradient Norm: 0.000598\n",
      "Epoch: 0 [21696/60000 (36%)]\tLoss: 0.098367\tAccuracy: 79.83%\tGradient Norm: 0.000561\n",
      "Epoch: 0 [21760/60000 (36%)]\tLoss: 0.090536\tAccuracy: 79.88%\tGradient Norm: 0.000464\n",
      "Epoch: 0 [21824/60000 (36%)]\tLoss: 0.132868\tAccuracy: 79.93%\tGradient Norm: 0.000496\n",
      "Epoch: 0 [21888/60000 (36%)]\tLoss: 0.100202\tAccuracy: 79.99%\tGradient Norm: 0.000455\n",
      "Epoch: 0 [21952/60000 (37%)]\tLoss: 0.131047\tAccuracy: 80.04%\tGradient Norm: 0.000469\n",
      "Epoch: 0 [22016/60000 (37%)]\tLoss: 0.116177\tAccuracy: 80.09%\tGradient Norm: 0.000411\n",
      "Epoch: 0 [22080/60000 (37%)]\tLoss: 0.073901\tAccuracy: 80.13%\tGradient Norm: 0.000532\n",
      "Epoch: 0 [22144/60000 (37%)]\tLoss: 0.148988\tAccuracy: 80.18%\tGradient Norm: 0.000464\n",
      "Epoch: 0 [22208/60000 (37%)]\tLoss: 0.092883\tAccuracy: 80.24%\tGradient Norm: 0.000504\n",
      "Epoch: 0 [22272/60000 (37%)]\tLoss: 0.103413\tAccuracy: 80.28%\tGradient Norm: 0.000482\n",
      "Epoch: 0 [22336/60000 (37%)]\tLoss: 0.162204\tAccuracy: 80.33%\tGradient Norm: 0.000518\n",
      "Epoch: 0 [22400/60000 (37%)]\tLoss: 0.120732\tAccuracy: 80.37%\tGradient Norm: 0.000422\n",
      "Epoch: 0 [22464/60000 (37%)]\tLoss: 0.061564\tAccuracy: 80.43%\tGradient Norm: 0.000557\n",
      "Epoch: 0 [22528/60000 (38%)]\tLoss: 0.071554\tAccuracy: 80.48%\tGradient Norm: 0.000480\n",
      "Epoch: 0 [22592/60000 (38%)]\tLoss: 0.178319\tAccuracy: 80.52%\tGradient Norm: 0.000461\n",
      "Epoch: 0 [22656/60000 (38%)]\tLoss: 0.107519\tAccuracy: 80.57%\tGradient Norm: 0.000477\n",
      "Epoch: 0 [22720/60000 (38%)]\tLoss: 0.080412\tAccuracy: 80.62%\tGradient Norm: 0.000483\n",
      "Epoch: 0 [22784/60000 (38%)]\tLoss: 0.082913\tAccuracy: 80.67%\tGradient Norm: 0.000504\n",
      "Epoch: 0 [22848/60000 (38%)]\tLoss: 0.117937\tAccuracy: 80.72%\tGradient Norm: 0.000529\n",
      "Epoch: 0 [22912/60000 (38%)]\tLoss: 0.093536\tAccuracy: 80.77%\tGradient Norm: 0.000384\n",
      "Epoch: 0 [22976/60000 (38%)]\tLoss: 0.095861\tAccuracy: 80.82%\tGradient Norm: 0.000598\n",
      "Epoch: 0 [23040/60000 (38%)]\tLoss: 0.292046\tAccuracy: 80.85%\tGradient Norm: 0.000461\n",
      "Epoch: 0 [23104/60000 (38%)]\tLoss: 0.059396\tAccuracy: 80.90%\tGradient Norm: 0.000445\n",
      "Epoch: 0 [23168/60000 (39%)]\tLoss: 0.107328\tAccuracy: 80.95%\tGradient Norm: 0.000410\n",
      "Epoch: 0 [23232/60000 (39%)]\tLoss: 0.200612\tAccuracy: 80.99%\tGradient Norm: 0.000491\n",
      "Epoch: 0 [23296/60000 (39%)]\tLoss: 0.103758\tAccuracy: 81.03%\tGradient Norm: 0.000426\n",
      "Epoch: 0 [23360/60000 (39%)]\tLoss: 0.063728\tAccuracy: 81.08%\tGradient Norm: 0.000419\n",
      "Epoch: 0 [23424/60000 (39%)]\tLoss: 0.135720\tAccuracy: 81.12%\tGradient Norm: 0.000430\n",
      "Epoch: 0 [23488/60000 (39%)]\tLoss: 0.138769\tAccuracy: 81.16%\tGradient Norm: 0.000485\n",
      "Epoch: 0 [23552/60000 (39%)]\tLoss: 0.221982\tAccuracy: 81.18%\tGradient Norm: 0.000441\n",
      "Epoch: 0 [23616/60000 (39%)]\tLoss: 0.128726\tAccuracy: 81.22%\tGradient Norm: 0.000458\n",
      "Epoch: 0 [23680/60000 (39%)]\tLoss: 0.094424\tAccuracy: 81.27%\tGradient Norm: 0.000452\n",
      "Epoch: 0 [23744/60000 (40%)]\tLoss: 0.087218\tAccuracy: 81.32%\tGradient Norm: 0.000436\n",
      "Epoch: 0 [23808/60000 (40%)]\tLoss: 0.035242\tAccuracy: 81.37%\tGradient Norm: 0.000469\n",
      "Epoch: 0 [23872/60000 (40%)]\tLoss: 0.242812\tAccuracy: 81.40%\tGradient Norm: 0.000438\n",
      "Epoch: 0 [23936/60000 (40%)]\tLoss: 0.092255\tAccuracy: 81.44%\tGradient Norm: 0.000414\n",
      "Epoch: 0 [24000/60000 (40%)]\tLoss: 0.172823\tAccuracy: 81.48%\tGradient Norm: 0.000449\n",
      "Epoch: 0 [24064/60000 (40%)]\tLoss: 0.179893\tAccuracy: 81.52%\tGradient Norm: 0.000380\n",
      "Epoch: 0 [24128/60000 (40%)]\tLoss: 0.085350\tAccuracy: 81.56%\tGradient Norm: 0.000474\n",
      "Epoch: 0 [24192/60000 (40%)]\tLoss: 0.119740\tAccuracy: 81.61%\tGradient Norm: 0.000470\n",
      "Epoch: 0 [24256/60000 (40%)]\tLoss: 0.043025\tAccuracy: 81.66%\tGradient Norm: 0.000419\n",
      "Epoch: 0 [24320/60000 (41%)]\tLoss: 0.106896\tAccuracy: 81.70%\tGradient Norm: 0.000446\n",
      "Epoch: 0 [24384/60000 (41%)]\tLoss: 0.253840\tAccuracy: 81.72%\tGradient Norm: 0.000468\n",
      "Epoch: 0 [24448/60000 (41%)]\tLoss: 0.127028\tAccuracy: 81.76%\tGradient Norm: 0.000351\n",
      "Epoch: 0 [24512/60000 (41%)]\tLoss: 0.109906\tAccuracy: 81.80%\tGradient Norm: 0.000447\n",
      "Epoch: 0 [24576/60000 (41%)]\tLoss: 0.077067\tAccuracy: 81.84%\tGradient Norm: 0.000370\n",
      "Epoch: 0 [24640/60000 (41%)]\tLoss: 0.082229\tAccuracy: 81.89%\tGradient Norm: 0.000448\n",
      "Epoch: 0 [24704/60000 (41%)]\tLoss: 0.114208\tAccuracy: 81.93%\tGradient Norm: 0.000433\n",
      "Epoch: 0 [24768/60000 (41%)]\tLoss: 0.117889\tAccuracy: 81.97%\tGradient Norm: 0.000406\n",
      "Epoch: 0 [24832/60000 (41%)]\tLoss: 0.101171\tAccuracy: 82.01%\tGradient Norm: 0.000418\n",
      "Epoch: 0 [24896/60000 (41%)]\tLoss: 0.060109\tAccuracy: 82.06%\tGradient Norm: 0.000477\n",
      "Epoch: 0 [24960/60000 (42%)]\tLoss: 0.052244\tAccuracy: 82.10%\tGradient Norm: 0.000440\n",
      "Epoch: 0 [25024/60000 (42%)]\tLoss: 0.076247\tAccuracy: 82.14%\tGradient Norm: 0.000488\n",
      "Epoch: 0 [25088/60000 (42%)]\tLoss: 0.124356\tAccuracy: 82.18%\tGradient Norm: 0.000463\n",
      "Epoch: 0 [25152/60000 (42%)]\tLoss: 0.067618\tAccuracy: 82.23%\tGradient Norm: 0.000458\n",
      "Epoch: 0 [25216/60000 (42%)]\tLoss: 0.120247\tAccuracy: 82.26%\tGradient Norm: 0.000378\n",
      "Epoch: 0 [25280/60000 (42%)]\tLoss: 0.170965\tAccuracy: 82.29%\tGradient Norm: 0.000445\n",
      "Epoch: 0 [25344/60000 (42%)]\tLoss: 0.079754\tAccuracy: 82.33%\tGradient Norm: 0.000398\n",
      "Epoch: 0 [25408/60000 (42%)]\tLoss: 0.236187\tAccuracy: 82.36%\tGradient Norm: 0.000453\n",
      "Epoch: 0 [25472/60000 (42%)]\tLoss: 0.075898\tAccuracy: 82.40%\tGradient Norm: 0.000399\n",
      "Epoch: 0 [25536/60000 (43%)]\tLoss: 0.068713\tAccuracy: 82.44%\tGradient Norm: 0.000426\n",
      "Epoch: 0 [25600/60000 (43%)]\tLoss: 0.075249\tAccuracy: 82.48%\tGradient Norm: 0.000386\n",
      "Epoch: 0 [25664/60000 (43%)]\tLoss: 0.122039\tAccuracy: 82.52%\tGradient Norm: 0.000391\n",
      "Epoch: 0 [25728/60000 (43%)]\tLoss: 0.120325\tAccuracy: 82.56%\tGradient Norm: 0.000433\n",
      "Epoch: 0 [25792/60000 (43%)]\tLoss: 0.119289\tAccuracy: 82.60%\tGradient Norm: 0.000441\n",
      "Epoch: 0 [25856/60000 (43%)]\tLoss: 0.040183\tAccuracy: 82.64%\tGradient Norm: 0.000425\n",
      "Epoch: 0 [25920/60000 (43%)]\tLoss: 0.091026\tAccuracy: 82.67%\tGradient Norm: 0.000462\n",
      "Epoch: 0 [25984/60000 (43%)]\tLoss: 0.112579\tAccuracy: 82.71%\tGradient Norm: 0.000466\n",
      "Epoch: 0 [26048/60000 (43%)]\tLoss: 0.127924\tAccuracy: 82.74%\tGradient Norm: 0.000462\n",
      "Epoch: 0 [26112/60000 (43%)]\tLoss: 0.209105\tAccuracy: 82.76%\tGradient Norm: 0.000472\n",
      "Epoch: 0 [26176/60000 (44%)]\tLoss: 0.096535\tAccuracy: 82.80%\tGradient Norm: 0.000528\n",
      "Epoch: 0 [26240/60000 (44%)]\tLoss: 0.085254\tAccuracy: 82.83%\tGradient Norm: 0.000460\n",
      "Epoch: 0 [26304/60000 (44%)]\tLoss: 0.036938\tAccuracy: 82.87%\tGradient Norm: 0.000429\n",
      "Epoch: 0 [26368/60000 (44%)]\tLoss: 0.136826\tAccuracy: 82.90%\tGradient Norm: 0.000502\n",
      "Epoch: 0 [26432/60000 (44%)]\tLoss: 0.049235\tAccuracy: 82.94%\tGradient Norm: 0.000499\n",
      "Epoch: 0 [26496/60000 (44%)]\tLoss: 0.067254\tAccuracy: 82.98%\tGradient Norm: 0.000404\n",
      "Epoch: 0 [26560/60000 (44%)]\tLoss: 0.116878\tAccuracy: 83.02%\tGradient Norm: 0.000472\n",
      "Epoch: 0 [26624/60000 (44%)]\tLoss: 0.137861\tAccuracy: 83.05%\tGradient Norm: 0.000508\n",
      "Epoch: 0 [26688/60000 (44%)]\tLoss: 0.102435\tAccuracy: 83.08%\tGradient Norm: 0.000501\n",
      "Epoch: 0 [26752/60000 (45%)]\tLoss: 0.133586\tAccuracy: 83.11%\tGradient Norm: 0.000457\n",
      "Epoch: 0 [26816/60000 (45%)]\tLoss: 0.054167\tAccuracy: 83.15%\tGradient Norm: 0.000507\n",
      "Epoch: 0 [26880/60000 (45%)]\tLoss: 0.129060\tAccuracy: 83.19%\tGradient Norm: 0.000413\n",
      "Epoch: 0 [26944/60000 (45%)]\tLoss: 0.139037\tAccuracy: 83.22%\tGradient Norm: 0.000442\n",
      "Epoch: 0 [27008/60000 (45%)]\tLoss: 0.073463\tAccuracy: 83.26%\tGradient Norm: 0.000540\n",
      "Epoch: 0 [27072/60000 (45%)]\tLoss: 0.099638\tAccuracy: 83.29%\tGradient Norm: 0.000602\n",
      "Epoch: 0 [27136/60000 (45%)]\tLoss: 0.054290\tAccuracy: 83.33%\tGradient Norm: 0.000448\n",
      "Epoch: 0 [27200/60000 (45%)]\tLoss: 0.112471\tAccuracy: 83.37%\tGradient Norm: 0.000545\n",
      "Epoch: 0 [27264/60000 (45%)]\tLoss: 0.040870\tAccuracy: 83.41%\tGradient Norm: 0.000420\n",
      "Epoch: 0 [27328/60000 (46%)]\tLoss: 0.111911\tAccuracy: 83.44%\tGradient Norm: 0.000494\n",
      "Epoch: 0 [27392/60000 (46%)]\tLoss: 0.147091\tAccuracy: 83.47%\tGradient Norm: 0.000522\n",
      "Epoch: 0 [27456/60000 (46%)]\tLoss: 0.091781\tAccuracy: 83.50%\tGradient Norm: 0.000502\n",
      "Epoch: 0 [27520/60000 (46%)]\tLoss: 0.074330\tAccuracy: 83.53%\tGradient Norm: 0.000486\n",
      "Epoch: 0 [27584/60000 (46%)]\tLoss: 0.061238\tAccuracy: 83.57%\tGradient Norm: 0.000551\n",
      "Epoch: 0 [27648/60000 (46%)]\tLoss: 0.149260\tAccuracy: 83.60%\tGradient Norm: 0.000596\n",
      "Epoch: 0 [27712/60000 (46%)]\tLoss: 0.061507\tAccuracy: 83.64%\tGradient Norm: 0.000564\n",
      "Epoch: 0 [27776/60000 (46%)]\tLoss: 0.103586\tAccuracy: 83.67%\tGradient Norm: 0.000710\n",
      "Epoch: 0 [27840/60000 (46%)]\tLoss: 0.106673\tAccuracy: 83.70%\tGradient Norm: 0.000554\n",
      "Epoch: 0 [27904/60000 (46%)]\tLoss: 0.142138\tAccuracy: 83.72%\tGradient Norm: 0.000521\n",
      "Epoch: 0 [27968/60000 (47%)]\tLoss: 0.049272\tAccuracy: 83.76%\tGradient Norm: 0.000573\n",
      "Epoch: 0 [28032/60000 (47%)]\tLoss: 0.082044\tAccuracy: 83.79%\tGradient Norm: 0.000540\n",
      "Epoch: 0 [28096/60000 (47%)]\tLoss: 0.055830\tAccuracy: 83.82%\tGradient Norm: 0.000557\n",
      "Epoch: 0 [28160/60000 (47%)]\tLoss: 0.092230\tAccuracy: 83.86%\tGradient Norm: 0.000563\n",
      "Epoch: 0 [28224/60000 (47%)]\tLoss: 0.178733\tAccuracy: 83.89%\tGradient Norm: 0.000457\n",
      "Epoch: 0 [28288/60000 (47%)]\tLoss: 0.153977\tAccuracy: 83.92%\tGradient Norm: 0.000574\n",
      "Epoch: 0 [28352/60000 (47%)]\tLoss: 0.098079\tAccuracy: 83.95%\tGradient Norm: 0.000491\n",
      "Epoch: 0 [28416/60000 (47%)]\tLoss: 0.169309\tAccuracy: 83.98%\tGradient Norm: 0.000570\n",
      "Epoch: 0 [28480/60000 (47%)]\tLoss: 0.056067\tAccuracy: 84.01%\tGradient Norm: 0.000540\n",
      "Epoch: 0 [28544/60000 (48%)]\tLoss: 0.132006\tAccuracy: 84.04%\tGradient Norm: 0.000522\n",
      "Epoch: 0 [28608/60000 (48%)]\tLoss: 0.061095\tAccuracy: 84.07%\tGradient Norm: 0.000506\n",
      "Epoch: 0 [28672/60000 (48%)]\tLoss: 0.094868\tAccuracy: 84.10%\tGradient Norm: 0.000469\n",
      "Epoch: 0 [28736/60000 (48%)]\tLoss: 0.091357\tAccuracy: 84.13%\tGradient Norm: 0.000467\n",
      "Epoch: 0 [28800/60000 (48%)]\tLoss: 0.047647\tAccuracy: 84.16%\tGradient Norm: 0.000558\n",
      "Epoch: 0 [28864/60000 (48%)]\tLoss: 0.062194\tAccuracy: 84.20%\tGradient Norm: 0.000463\n",
      "Epoch: 0 [28928/60000 (48%)]\tLoss: 0.135952\tAccuracy: 84.22%\tGradient Norm: 0.000619\n",
      "Epoch: 0 [28992/60000 (48%)]\tLoss: 0.137545\tAccuracy: 84.25%\tGradient Norm: 0.000523\n",
      "Epoch: 0 [29056/60000 (48%)]\tLoss: 0.146476\tAccuracy: 84.27%\tGradient Norm: 0.000543\n",
      "Epoch: 0 [29120/60000 (49%)]\tLoss: 0.146045\tAccuracy: 84.30%\tGradient Norm: 0.000625\n",
      "Epoch: 0 [29184/60000 (49%)]\tLoss: 0.192643\tAccuracy: 84.32%\tGradient Norm: 0.000523\n",
      "Epoch: 0 [29248/60000 (49%)]\tLoss: 0.079724\tAccuracy: 84.35%\tGradient Norm: 0.000529\n",
      "Epoch: 0 [29312/60000 (49%)]\tLoss: 0.019784\tAccuracy: 84.39%\tGradient Norm: 0.000510\n",
      "Epoch: 0 [29376/60000 (49%)]\tLoss: 0.132335\tAccuracy: 84.41%\tGradient Norm: 0.000493\n",
      "Epoch: 0 [29440/60000 (49%)]\tLoss: 0.052954\tAccuracy: 84.44%\tGradient Norm: 0.000503\n",
      "Epoch: 0 [29504/60000 (49%)]\tLoss: 0.055426\tAccuracy: 84.47%\tGradient Norm: 0.000527\n",
      "Epoch: 0 [29568/60000 (49%)]\tLoss: 0.081516\tAccuracy: 84.50%\tGradient Norm: 0.000500\n",
      "Epoch: 0 [29632/60000 (49%)]\tLoss: 0.065579\tAccuracy: 84.53%\tGradient Norm: 0.000542\n",
      "Epoch: 0 [29696/60000 (49%)]\tLoss: 0.058166\tAccuracy: 84.56%\tGradient Norm: 0.000456\n",
      "Epoch: 0 [29760/60000 (50%)]\tLoss: 0.084815\tAccuracy: 84.59%\tGradient Norm: 0.000465\n",
      "Epoch: 0 [29824/60000 (50%)]\tLoss: 0.123565\tAccuracy: 84.62%\tGradient Norm: 0.000489\n",
      "Epoch: 0 [29888/60000 (50%)]\tLoss: 0.106919\tAccuracy: 84.65%\tGradient Norm: 0.000472\n",
      "Epoch: 0 [29952/60000 (50%)]\tLoss: 0.096163\tAccuracy: 84.67%\tGradient Norm: 0.000493\n",
      "Epoch: 0 [30016/60000 (50%)]\tLoss: 0.066064\tAccuracy: 84.70%\tGradient Norm: 0.000512\n",
      "Epoch: 0 [30080/60000 (50%)]\tLoss: 0.116789\tAccuracy: 84.73%\tGradient Norm: 0.000509\n",
      "Epoch: 0 [30144/60000 (50%)]\tLoss: 0.031743\tAccuracy: 84.76%\tGradient Norm: 0.000506\n",
      "Epoch: 0 [30208/60000 (50%)]\tLoss: 0.033156\tAccuracy: 84.79%\tGradient Norm: 0.000407\n",
      "Epoch: 0 [30272/60000 (50%)]\tLoss: 0.082103\tAccuracy: 84.82%\tGradient Norm: 0.000419\n",
      "Epoch: 0 [30336/60000 (51%)]\tLoss: 0.100044\tAccuracy: 84.85%\tGradient Norm: 0.000481\n",
      "Epoch: 0 [30400/60000 (51%)]\tLoss: 0.140527\tAccuracy: 84.87%\tGradient Norm: 0.000468\n",
      "Epoch: 0 [30464/60000 (51%)]\tLoss: 0.116831\tAccuracy: 84.90%\tGradient Norm: 0.000477\n",
      "Epoch: 0 [30528/60000 (51%)]\tLoss: 0.058551\tAccuracy: 84.92%\tGradient Norm: 0.000411\n",
      "Epoch: 0 [30592/60000 (51%)]\tLoss: 0.120117\tAccuracy: 84.95%\tGradient Norm: 0.000526\n",
      "Epoch: 0 [30656/60000 (51%)]\tLoss: 0.118034\tAccuracy: 84.98%\tGradient Norm: 0.000429\n",
      "Epoch: 0 [30720/60000 (51%)]\tLoss: 0.083644\tAccuracy: 85.01%\tGradient Norm: 0.000527\n",
      "Epoch: 0 [30784/60000 (51%)]\tLoss: 0.107768\tAccuracy: 85.03%\tGradient Norm: 0.000505\n",
      "Epoch: 0 [30848/60000 (51%)]\tLoss: 0.124411\tAccuracy: 85.05%\tGradient Norm: 0.000393\n",
      "Epoch: 0 [30912/60000 (51%)]\tLoss: 0.081615\tAccuracy: 85.08%\tGradient Norm: 0.000419\n",
      "Epoch: 0 [30976/60000 (52%)]\tLoss: 0.047448\tAccuracy: 85.11%\tGradient Norm: 0.000390\n",
      "Epoch: 0 [31040/60000 (52%)]\tLoss: 0.045761\tAccuracy: 85.14%\tGradient Norm: 0.000411\n",
      "Epoch: 0 [31104/60000 (52%)]\tLoss: 0.244388\tAccuracy: 85.16%\tGradient Norm: 0.000382\n",
      "Epoch: 0 [31168/60000 (52%)]\tLoss: 0.166814\tAccuracy: 85.19%\tGradient Norm: 0.000448\n",
      "Epoch: 0 [31232/60000 (52%)]\tLoss: 0.135666\tAccuracy: 85.21%\tGradient Norm: 0.000388\n",
      "Epoch: 0 [31296/60000 (52%)]\tLoss: 0.070749\tAccuracy: 85.23%\tGradient Norm: 0.000391\n",
      "Epoch: 0 [31360/60000 (52%)]\tLoss: 0.059401\tAccuracy: 85.26%\tGradient Norm: 0.000484\n",
      "Epoch: 0 [31424/60000 (52%)]\tLoss: 0.049538\tAccuracy: 85.29%\tGradient Norm: 0.000545\n",
      "Epoch: 0 [31488/60000 (52%)]\tLoss: 0.091299\tAccuracy: 85.31%\tGradient Norm: 0.000408\n",
      "Epoch: 0 [31552/60000 (53%)]\tLoss: 0.059207\tAccuracy: 85.34%\tGradient Norm: 0.000508\n",
      "Epoch: 0 [31616/60000 (53%)]\tLoss: 0.043248\tAccuracy: 85.37%\tGradient Norm: 0.000414\n",
      "Epoch: 0 [31680/60000 (53%)]\tLoss: 0.080752\tAccuracy: 85.40%\tGradient Norm: 0.000411\n",
      "Epoch: 0 [31744/60000 (53%)]\tLoss: 0.121570\tAccuracy: 85.42%\tGradient Norm: 0.000443\n",
      "Epoch: 0 [31808/60000 (53%)]\tLoss: 0.072097\tAccuracy: 85.44%\tGradient Norm: 0.000399\n",
      "Epoch: 0 [31872/60000 (53%)]\tLoss: 0.079424\tAccuracy: 85.47%\tGradient Norm: 0.000463\n",
      "Epoch: 0 [31936/60000 (53%)]\tLoss: 0.113109\tAccuracy: 85.49%\tGradient Norm: 0.000394\n",
      "Epoch: 0 [32000/60000 (53%)]\tLoss: 0.060049\tAccuracy: 85.52%\tGradient Norm: 0.000321\n",
      "Epoch: 0 [32064/60000 (53%)]\tLoss: 0.180553\tAccuracy: 85.54%\tGradient Norm: 0.000400\n",
      "Epoch: 0 [32128/60000 (54%)]\tLoss: 0.087490\tAccuracy: 85.56%\tGradient Norm: 0.000377\n",
      "Epoch: 0 [32192/60000 (54%)]\tLoss: 0.056254\tAccuracy: 85.59%\tGradient Norm: 0.000351\n",
      "Epoch: 0 [32256/60000 (54%)]\tLoss: 0.027860\tAccuracy: 85.62%\tGradient Norm: 0.000337\n",
      "Epoch: 0 [32320/60000 (54%)]\tLoss: 0.034308\tAccuracy: 85.64%\tGradient Norm: 0.000427\n",
      "Epoch: 0 [32384/60000 (54%)]\tLoss: 0.123460\tAccuracy: 85.67%\tGradient Norm: 0.000449\n",
      "Epoch: 0 [32448/60000 (54%)]\tLoss: 0.045780\tAccuracy: 85.69%\tGradient Norm: 0.000391\n",
      "Epoch: 0 [32512/60000 (54%)]\tLoss: 0.066923\tAccuracy: 85.72%\tGradient Norm: 0.000407\n",
      "Epoch: 0 [32576/60000 (54%)]\tLoss: 0.088325\tAccuracy: 85.75%\tGradient Norm: 0.000355\n",
      "Epoch: 0 [32640/60000 (54%)]\tLoss: 0.096010\tAccuracy: 85.77%\tGradient Norm: 0.000357\n",
      "Epoch: 0 [32704/60000 (54%)]\tLoss: 0.090992\tAccuracy: 85.80%\tGradient Norm: 0.000375\n",
      "Epoch: 0 [32768/60000 (55%)]\tLoss: 0.084456\tAccuracy: 85.82%\tGradient Norm: 0.000379\n",
      "Epoch: 0 [32832/60000 (55%)]\tLoss: 0.095126\tAccuracy: 85.84%\tGradient Norm: 0.000378\n",
      "Epoch: 0 [32896/60000 (55%)]\tLoss: 0.034265\tAccuracy: 85.87%\tGradient Norm: 0.000327\n",
      "Epoch: 0 [32960/60000 (55%)]\tLoss: 0.053116\tAccuracy: 85.90%\tGradient Norm: 0.000362\n",
      "Epoch: 0 [33024/60000 (55%)]\tLoss: 0.058364\tAccuracy: 85.92%\tGradient Norm: 0.000383\n",
      "Epoch: 0 [33088/60000 (55%)]\tLoss: 0.072870\tAccuracy: 85.94%\tGradient Norm: 0.000407\n",
      "Epoch: 0 [33152/60000 (55%)]\tLoss: 0.086491\tAccuracy: 85.97%\tGradient Norm: 0.000392\n",
      "Epoch: 0 [33216/60000 (55%)]\tLoss: 0.040024\tAccuracy: 85.99%\tGradient Norm: 0.000353\n",
      "Epoch: 0 [33280/60000 (55%)]\tLoss: 0.033955\tAccuracy: 86.02%\tGradient Norm: 0.000368\n",
      "Epoch: 0 [33344/60000 (56%)]\tLoss: 0.100663\tAccuracy: 86.04%\tGradient Norm: 0.000344\n",
      "Epoch: 0 [33408/60000 (56%)]\tLoss: 0.055881\tAccuracy: 86.07%\tGradient Norm: 0.000405\n",
      "Epoch: 0 [33472/60000 (56%)]\tLoss: 0.072658\tAccuracy: 86.09%\tGradient Norm: 0.000371\n",
      "Epoch: 0 [33536/60000 (56%)]\tLoss: 0.036489\tAccuracy: 86.12%\tGradient Norm: 0.000330\n",
      "Epoch: 0 [33600/60000 (56%)]\tLoss: 0.158536\tAccuracy: 86.13%\tGradient Norm: 0.000337\n",
      "Epoch: 0 [33664/60000 (56%)]\tLoss: 0.129384\tAccuracy: 86.15%\tGradient Norm: 0.000337\n",
      "Epoch: 0 [33728/60000 (56%)]\tLoss: 0.050617\tAccuracy: 86.17%\tGradient Norm: 0.000375\n",
      "Epoch: 0 [33792/60000 (56%)]\tLoss: 0.097456\tAccuracy: 86.19%\tGradient Norm: 0.000401\n",
      "Epoch: 0 [33856/60000 (56%)]\tLoss: 0.050048\tAccuracy: 86.22%\tGradient Norm: 0.000376\n",
      "Epoch: 0 [33920/60000 (57%)]\tLoss: 0.029372\tAccuracy: 86.25%\tGradient Norm: 0.000403\n",
      "Epoch: 0 [33984/60000 (57%)]\tLoss: 0.052532\tAccuracy: 86.27%\tGradient Norm: 0.000328\n",
      "Epoch: 0 [34048/60000 (57%)]\tLoss: 0.093130\tAccuracy: 86.29%\tGradient Norm: 0.000371\n",
      "Epoch: 0 [34112/60000 (57%)]\tLoss: 0.032376\tAccuracy: 86.31%\tGradient Norm: 0.000342\n",
      "Epoch: 0 [34176/60000 (57%)]\tLoss: 0.096836\tAccuracy: 86.33%\tGradient Norm: 0.000342\n",
      "Epoch: 0 [34240/60000 (57%)]\tLoss: 0.056924\tAccuracy: 86.36%\tGradient Norm: 0.000352\n",
      "Epoch: 0 [34304/60000 (57%)]\tLoss: 0.245440\tAccuracy: 86.37%\tGradient Norm: 0.000332\n",
      "Epoch: 0 [34368/60000 (57%)]\tLoss: 0.120150\tAccuracy: 86.39%\tGradient Norm: 0.000368\n",
      "Epoch: 0 [34432/60000 (57%)]\tLoss: 0.034876\tAccuracy: 86.41%\tGradient Norm: 0.000366\n",
      "Epoch: 0 [34496/60000 (57%)]\tLoss: 0.035918\tAccuracy: 86.44%\tGradient Norm: 0.000315\n",
      "Epoch: 0 [34560/60000 (58%)]\tLoss: 0.047355\tAccuracy: 86.46%\tGradient Norm: 0.000369\n",
      "Epoch: 0 [34624/60000 (58%)]\tLoss: 0.154053\tAccuracy: 86.48%\tGradient Norm: 0.000401\n",
      "Epoch: 0 [34688/60000 (58%)]\tLoss: 0.125123\tAccuracy: 86.50%\tGradient Norm: 0.000335\n",
      "Epoch: 0 [34752/60000 (58%)]\tLoss: 0.048032\tAccuracy: 86.52%\tGradient Norm: 0.000316\n",
      "Epoch: 0 [34816/60000 (58%)]\tLoss: 0.154933\tAccuracy: 86.54%\tGradient Norm: 0.000375\n",
      "Epoch: 0 [34880/60000 (58%)]\tLoss: 0.077183\tAccuracy: 86.56%\tGradient Norm: 0.000385\n",
      "Epoch: 0 [34944/60000 (58%)]\tLoss: 0.088279\tAccuracy: 86.58%\tGradient Norm: 0.000378\n",
      "Epoch: 0 [35008/60000 (58%)]\tLoss: 0.032121\tAccuracy: 86.60%\tGradient Norm: 0.000347\n",
      "Epoch: 0 [35072/60000 (58%)]\tLoss: 0.096171\tAccuracy: 86.62%\tGradient Norm: 0.000323\n",
      "Epoch: 0 [35136/60000 (59%)]\tLoss: 0.093355\tAccuracy: 86.64%\tGradient Norm: 0.000377\n",
      "Epoch: 0 [35200/60000 (59%)]\tLoss: 0.083520\tAccuracy: 86.65%\tGradient Norm: 0.000366\n",
      "Epoch: 0 [35264/60000 (59%)]\tLoss: 0.057189\tAccuracy: 86.68%\tGradient Norm: 0.000335\n",
      "Epoch: 0 [35328/60000 (59%)]\tLoss: 0.165718\tAccuracy: 86.69%\tGradient Norm: 0.000361\n",
      "Epoch: 0 [35392/60000 (59%)]\tLoss: 0.046843\tAccuracy: 86.72%\tGradient Norm: 0.000329\n",
      "Epoch: 0 [35456/60000 (59%)]\tLoss: 0.069506\tAccuracy: 86.74%\tGradient Norm: 0.000358\n",
      "Epoch: 0 [35520/60000 (59%)]\tLoss: 0.120321\tAccuracy: 86.75%\tGradient Norm: 0.000379\n",
      "Epoch: 0 [35584/60000 (59%)]\tLoss: 0.063307\tAccuracy: 86.77%\tGradient Norm: 0.000350\n",
      "Epoch: 0 [35648/60000 (59%)]\tLoss: 0.085635\tAccuracy: 86.79%\tGradient Norm: 0.000399\n",
      "Epoch: 0 [35712/60000 (59%)]\tLoss: 0.055272\tAccuracy: 86.82%\tGradient Norm: 0.000395\n",
      "Epoch: 0 [35776/60000 (60%)]\tLoss: 0.055830\tAccuracy: 86.84%\tGradient Norm: 0.000361\n",
      "Epoch: 0 [35840/60000 (60%)]\tLoss: 0.095238\tAccuracy: 86.85%\tGradient Norm: 0.000424\n",
      "Epoch: 0 [35904/60000 (60%)]\tLoss: 0.062404\tAccuracy: 86.87%\tGradient Norm: 0.000425\n",
      "Epoch: 0 [35968/60000 (60%)]\tLoss: 0.041528\tAccuracy: 86.90%\tGradient Norm: 0.000365\n",
      "Epoch: 0 [36032/60000 (60%)]\tLoss: 0.077271\tAccuracy: 86.92%\tGradient Norm: 0.000425\n",
      "Epoch: 0 [36096/60000 (60%)]\tLoss: 0.072887\tAccuracy: 86.94%\tGradient Norm: 0.000394\n",
      "Epoch: 0 [36160/60000 (60%)]\tLoss: 0.087320\tAccuracy: 86.96%\tGradient Norm: 0.000407\n",
      "Epoch: 0 [36224/60000 (60%)]\tLoss: 0.054718\tAccuracy: 86.98%\tGradient Norm: 0.000399\n",
      "Epoch: 0 [36288/60000 (60%)]\tLoss: 0.085599\tAccuracy: 87.00%\tGradient Norm: 0.000428\n",
      "Epoch: 0 [36352/60000 (61%)]\tLoss: 0.107408\tAccuracy: 87.01%\tGradient Norm: 0.000345\n",
      "Epoch: 0 [36416/60000 (61%)]\tLoss: 0.117688\tAccuracy: 87.03%\tGradient Norm: 0.000487\n",
      "Epoch: 0 [36480/60000 (61%)]\tLoss: 0.056459\tAccuracy: 87.05%\tGradient Norm: 0.000385\n",
      "Epoch: 0 [36544/60000 (61%)]\tLoss: 0.044755\tAccuracy: 87.07%\tGradient Norm: 0.000375\n",
      "Epoch: 0 [36608/60000 (61%)]\tLoss: 0.080332\tAccuracy: 87.09%\tGradient Norm: 0.000418\n",
      "Epoch: 0 [36672/60000 (61%)]\tLoss: 0.111921\tAccuracy: 87.11%\tGradient Norm: 0.000393\n",
      "Epoch: 0 [36736/60000 (61%)]\tLoss: 0.028981\tAccuracy: 87.13%\tGradient Norm: 0.000374\n",
      "Epoch: 0 [36800/60000 (61%)]\tLoss: 0.129912\tAccuracy: 87.15%\tGradient Norm: 0.000345\n",
      "Epoch: 0 [36864/60000 (61%)]\tLoss: 0.115016\tAccuracy: 87.17%\tGradient Norm: 0.000363\n",
      "Epoch: 0 [36928/60000 (62%)]\tLoss: 0.069351\tAccuracy: 87.19%\tGradient Norm: 0.000435\n",
      "Epoch: 0 [36992/60000 (62%)]\tLoss: 0.075278\tAccuracy: 87.20%\tGradient Norm: 0.000344\n",
      "Epoch: 0 [37056/60000 (62%)]\tLoss: 0.090659\tAccuracy: 87.22%\tGradient Norm: 0.000357\n",
      "Epoch: 0 [37120/60000 (62%)]\tLoss: 0.131200\tAccuracy: 87.23%\tGradient Norm: 0.000401\n",
      "Epoch: 0 [37184/60000 (62%)]\tLoss: 0.036424\tAccuracy: 87.25%\tGradient Norm: 0.000361\n",
      "Epoch: 0 [37248/60000 (62%)]\tLoss: 0.024552\tAccuracy: 87.27%\tGradient Norm: 0.000323\n",
      "Epoch: 0 [37312/60000 (62%)]\tLoss: 0.039740\tAccuracy: 87.29%\tGradient Norm: 0.000331\n",
      "Epoch: 0 [37376/60000 (62%)]\tLoss: 0.018761\tAccuracy: 87.32%\tGradient Norm: 0.000382\n",
      "Epoch: 0 [37440/60000 (62%)]\tLoss: 0.119683\tAccuracy: 87.33%\tGradient Norm: 0.000363\n",
      "Epoch: 0 [37504/60000 (62%)]\tLoss: 0.026548\tAccuracy: 87.36%\tGradient Norm: 0.000317\n",
      "Epoch: 0 [37568/60000 (63%)]\tLoss: 0.062145\tAccuracy: 87.38%\tGradient Norm: 0.000376\n",
      "Epoch: 0 [37632/60000 (63%)]\tLoss: 0.036899\tAccuracy: 87.40%\tGradient Norm: 0.000353\n",
      "Epoch: 0 [37696/60000 (63%)]\tLoss: 0.296157\tAccuracy: 87.40%\tGradient Norm: 0.000324\n",
      "Epoch: 0 [37760/60000 (63%)]\tLoss: 0.034728\tAccuracy: 87.42%\tGradient Norm: 0.000351\n",
      "Epoch: 0 [37824/60000 (63%)]\tLoss: 0.055998\tAccuracy: 87.44%\tGradient Norm: 0.000362\n",
      "Epoch: 0 [37888/60000 (63%)]\tLoss: 0.045874\tAccuracy: 87.46%\tGradient Norm: 0.000347\n",
      "Epoch: 0 [37952/60000 (63%)]\tLoss: 0.050216\tAccuracy: 87.48%\tGradient Norm: 0.000343\n",
      "Epoch: 0 [38016/60000 (63%)]\tLoss: 0.050664\tAccuracy: 87.50%\tGradient Norm: 0.000389\n",
      "Epoch: 0 [38080/60000 (63%)]\tLoss: 0.030508\tAccuracy: 87.52%\tGradient Norm: 0.000350\n",
      "Epoch: 0 [38144/60000 (64%)]\tLoss: 0.112936\tAccuracy: 87.54%\tGradient Norm: 0.000302\n",
      "Epoch: 0 [38208/60000 (64%)]\tLoss: 0.068033\tAccuracy: 87.55%\tGradient Norm: 0.000336\n",
      "Epoch: 0 [38272/60000 (64%)]\tLoss: 0.102123\tAccuracy: 87.57%\tGradient Norm: 0.000336\n",
      "Epoch: 0 [38336/60000 (64%)]\tLoss: 0.188291\tAccuracy: 87.58%\tGradient Norm: 0.000308\n",
      "Epoch: 0 [38400/60000 (64%)]\tLoss: 0.057259\tAccuracy: 87.60%\tGradient Norm: 0.000262\n",
      "Epoch: 0 [38464/60000 (64%)]\tLoss: 0.159645\tAccuracy: 87.61%\tGradient Norm: 0.000276\n",
      "Epoch: 0 [38528/60000 (64%)]\tLoss: 0.135204\tAccuracy: 87.63%\tGradient Norm: 0.000340\n",
      "Epoch: 0 [38592/60000 (64%)]\tLoss: 0.051276\tAccuracy: 87.64%\tGradient Norm: 0.000307\n",
      "Epoch: 0 [38656/60000 (64%)]\tLoss: 0.093038\tAccuracy: 87.66%\tGradient Norm: 0.000349\n",
      "Epoch: 0 [38720/60000 (64%)]\tLoss: 0.073864\tAccuracy: 87.68%\tGradient Norm: 0.000327\n",
      "Epoch: 0 [38784/60000 (65%)]\tLoss: 0.045401\tAccuracy: 87.70%\tGradient Norm: 0.000326\n",
      "Epoch: 0 [38848/60000 (65%)]\tLoss: 0.166971\tAccuracy: 87.71%\tGradient Norm: 0.000339\n",
      "Epoch: 0 [38912/60000 (65%)]\tLoss: 0.081558\tAccuracy: 87.72%\tGradient Norm: 0.000319\n",
      "Epoch: 0 [38976/60000 (65%)]\tLoss: 0.082735\tAccuracy: 87.74%\tGradient Norm: 0.000318\n",
      "Epoch: 0 [39040/60000 (65%)]\tLoss: 0.029338\tAccuracy: 87.76%\tGradient Norm: 0.000349\n",
      "Epoch: 0 [39104/60000 (65%)]\tLoss: 0.029401\tAccuracy: 87.78%\tGradient Norm: 0.000333\n",
      "Epoch: 0 [39168/60000 (65%)]\tLoss: 0.029029\tAccuracy: 87.80%\tGradient Norm: 0.000346\n",
      "Epoch: 0 [39232/60000 (65%)]\tLoss: 0.156456\tAccuracy: 87.81%\tGradient Norm: 0.000333\n",
      "Epoch: 0 [39296/60000 (65%)]\tLoss: 0.131347\tAccuracy: 87.83%\tGradient Norm: 0.000349\n",
      "Epoch: 0 [39360/60000 (66%)]\tLoss: 0.137918\tAccuracy: 87.84%\tGradient Norm: 0.000346\n",
      "Epoch: 0 [39424/60000 (66%)]\tLoss: 0.025425\tAccuracy: 87.86%\tGradient Norm: 0.000351\n",
      "Epoch: 0 [39488/60000 (66%)]\tLoss: 0.024693\tAccuracy: 87.88%\tGradient Norm: 0.000325\n",
      "Epoch: 0 [39552/60000 (66%)]\tLoss: 0.071683\tAccuracy: 87.89%\tGradient Norm: 0.000327\n",
      "Epoch: 0 [39616/60000 (66%)]\tLoss: 0.065509\tAccuracy: 87.91%\tGradient Norm: 0.000352\n",
      "Epoch: 0 [39680/60000 (66%)]\tLoss: 0.037461\tAccuracy: 87.93%\tGradient Norm: 0.000334\n",
      "Epoch: 0 [39744/60000 (66%)]\tLoss: 0.073092\tAccuracy: 87.94%\tGradient Norm: 0.000337\n",
      "Epoch: 0 [39808/60000 (66%)]\tLoss: 0.150003\tAccuracy: 87.96%\tGradient Norm: 0.000379\n",
      "Epoch: 0 [39872/60000 (66%)]\tLoss: 0.089311\tAccuracy: 87.97%\tGradient Norm: 0.000360\n",
      "Epoch: 0 [39936/60000 (67%)]\tLoss: 0.056177\tAccuracy: 87.99%\tGradient Norm: 0.000323\n",
      "Epoch: 0 [40000/60000 (67%)]\tLoss: 0.059490\tAccuracy: 88.00%\tGradient Norm: 0.000300\n",
      "Epoch: 0 [40064/60000 (67%)]\tLoss: 0.119803\tAccuracy: 88.02%\tGradient Norm: 0.000331\n",
      "Epoch: 0 [40128/60000 (67%)]\tLoss: 0.058779\tAccuracy: 88.03%\tGradient Norm: 0.000330\n",
      "Epoch: 0 [40192/60000 (67%)]\tLoss: 0.060262\tAccuracy: 88.05%\tGradient Norm: 0.000326\n",
      "Epoch: 0 [40256/60000 (67%)]\tLoss: 0.045994\tAccuracy: 88.07%\tGradient Norm: 0.000376\n",
      "Epoch: 0 [40320/60000 (67%)]\tLoss: 0.122729\tAccuracy: 88.08%\tGradient Norm: 0.000308\n",
      "Epoch: 0 [40384/60000 (67%)]\tLoss: 0.155310\tAccuracy: 88.09%\tGradient Norm: 0.000346\n",
      "Epoch: 0 [40448/60000 (67%)]\tLoss: 0.097832\tAccuracy: 88.10%\tGradient Norm: 0.000314\n",
      "Epoch: 0 [40512/60000 (67%)]\tLoss: 0.035460\tAccuracy: 88.12%\tGradient Norm: 0.000321\n",
      "Epoch: 0 [40576/60000 (68%)]\tLoss: 0.068961\tAccuracy: 88.14%\tGradient Norm: 0.000316\n",
      "Epoch: 0 [40640/60000 (68%)]\tLoss: 0.086666\tAccuracy: 88.16%\tGradient Norm: 0.000359\n",
      "Epoch: 0 [40704/60000 (68%)]\tLoss: 0.081325\tAccuracy: 88.17%\tGradient Norm: 0.000270\n",
      "Epoch: 0 [40768/60000 (68%)]\tLoss: 0.135258\tAccuracy: 88.18%\tGradient Norm: 0.000362\n",
      "Epoch: 0 [40832/60000 (68%)]\tLoss: 0.071488\tAccuracy: 88.20%\tGradient Norm: 0.000379\n",
      "Epoch: 0 [40896/60000 (68%)]\tLoss: 0.025287\tAccuracy: 88.22%\tGradient Norm: 0.000306\n",
      "Epoch: 0 [40960/60000 (68%)]\tLoss: 0.066419\tAccuracy: 88.23%\tGradient Norm: 0.000390\n",
      "Epoch: 0 [41024/60000 (68%)]\tLoss: 0.100619\tAccuracy: 88.24%\tGradient Norm: 0.000338\n",
      "Epoch: 0 [41088/60000 (68%)]\tLoss: 0.039453\tAccuracy: 88.26%\tGradient Norm: 0.000311\n",
      "Epoch: 0 [41152/60000 (69%)]\tLoss: 0.021537\tAccuracy: 88.28%\tGradient Norm: 0.000333\n",
      "Epoch: 0 [41216/60000 (69%)]\tLoss: 0.068060\tAccuracy: 88.29%\tGradient Norm: 0.000311\n",
      "Epoch: 0 [41280/60000 (69%)]\tLoss: 0.045579\tAccuracy: 88.31%\tGradient Norm: 0.000299\n",
      "Epoch: 0 [41344/60000 (69%)]\tLoss: 0.048864\tAccuracy: 88.32%\tGradient Norm: 0.000310\n",
      "Epoch: 0 [41408/60000 (69%)]\tLoss: 0.026950\tAccuracy: 88.34%\tGradient Norm: 0.000348\n",
      "Epoch: 0 [41472/60000 (69%)]\tLoss: 0.012977\tAccuracy: 88.36%\tGradient Norm: 0.000312\n",
      "Epoch: 0 [41536/60000 (69%)]\tLoss: 0.107141\tAccuracy: 88.37%\tGradient Norm: 0.000388\n",
      "Epoch: 0 [41600/60000 (69%)]\tLoss: 0.053965\tAccuracy: 88.39%\tGradient Norm: 0.000434\n",
      "Epoch: 0 [41664/60000 (69%)]\tLoss: 0.047124\tAccuracy: 88.40%\tGradient Norm: 0.000317\n",
      "Epoch: 0 [41728/60000 (70%)]\tLoss: 0.043217\tAccuracy: 88.42%\tGradient Norm: 0.000327\n",
      "Epoch: 0 [41792/60000 (70%)]\tLoss: 0.079964\tAccuracy: 88.43%\tGradient Norm: 0.000316\n",
      "Epoch: 0 [41856/60000 (70%)]\tLoss: 0.043034\tAccuracy: 88.45%\tGradient Norm: 0.000310\n",
      "Epoch: 0 [41920/60000 (70%)]\tLoss: 0.115352\tAccuracy: 88.46%\tGradient Norm: 0.000322\n",
      "Epoch: 0 [41984/60000 (70%)]\tLoss: 0.022747\tAccuracy: 88.48%\tGradient Norm: 0.000301\n",
      "Epoch: 0 [42048/60000 (70%)]\tLoss: 0.114779\tAccuracy: 88.49%\tGradient Norm: 0.000329\n",
      "Epoch: 0 [42112/60000 (70%)]\tLoss: 0.033741\tAccuracy: 88.51%\tGradient Norm: 0.000336\n",
      "Epoch: 0 [42176/60000 (70%)]\tLoss: 0.041229\tAccuracy: 88.52%\tGradient Norm: 0.000350\n",
      "Epoch: 0 [42240/60000 (70%)]\tLoss: 0.031220\tAccuracy: 88.54%\tGradient Norm: 0.000311\n",
      "Epoch: 0 [42304/60000 (70%)]\tLoss: 0.077282\tAccuracy: 88.55%\tGradient Norm: 0.000299\n",
      "Epoch: 0 [42368/60000 (71%)]\tLoss: 0.021413\tAccuracy: 88.57%\tGradient Norm: 0.000356\n",
      "Epoch: 0 [42432/60000 (71%)]\tLoss: 0.067308\tAccuracy: 88.58%\tGradient Norm: 0.000353\n",
      "Epoch: 0 [42496/60000 (71%)]\tLoss: 0.092917\tAccuracy: 88.59%\tGradient Norm: 0.000350\n",
      "Epoch: 0 [42560/60000 (71%)]\tLoss: 0.020238\tAccuracy: 88.61%\tGradient Norm: 0.000372\n",
      "Epoch: 0 [42624/60000 (71%)]\tLoss: 0.088422\tAccuracy: 88.62%\tGradient Norm: 0.000359\n",
      "Epoch: 0 [42688/60000 (71%)]\tLoss: 0.070850\tAccuracy: 88.63%\tGradient Norm: 0.000332\n",
      "Epoch: 0 [42752/60000 (71%)]\tLoss: 0.041486\tAccuracy: 88.65%\tGradient Norm: 0.000337\n",
      "Epoch: 0 [42816/60000 (71%)]\tLoss: 0.010655\tAccuracy: 88.67%\tGradient Norm: 0.000296\n",
      "Epoch: 0 [42880/60000 (71%)]\tLoss: 0.067273\tAccuracy: 88.68%\tGradient Norm: 0.000331\n",
      "Epoch: 0 [42944/60000 (72%)]\tLoss: 0.031663\tAccuracy: 88.70%\tGradient Norm: 0.000304\n",
      "Epoch: 0 [43008/60000 (72%)]\tLoss: 0.151455\tAccuracy: 88.70%\tGradient Norm: 0.000365\n",
      "Epoch: 0 [43072/60000 (72%)]\tLoss: 0.023536\tAccuracy: 88.72%\tGradient Norm: 0.000278\n",
      "Epoch: 0 [43136/60000 (72%)]\tLoss: 0.035525\tAccuracy: 88.73%\tGradient Norm: 0.000312\n",
      "Epoch: 0 [43200/60000 (72%)]\tLoss: 0.038278\tAccuracy: 88.75%\tGradient Norm: 0.000317\n",
      "Epoch: 0 [43264/60000 (72%)]\tLoss: 0.041918\tAccuracy: 88.76%\tGradient Norm: 0.000336\n",
      "Epoch: 0 [43328/60000 (72%)]\tLoss: 0.040736\tAccuracy: 88.78%\tGradient Norm: 0.000344\n",
      "Epoch: 0 [43392/60000 (72%)]\tLoss: 0.020947\tAccuracy: 88.80%\tGradient Norm: 0.000307\n",
      "Epoch: 0 [43456/60000 (72%)]\tLoss: 0.019613\tAccuracy: 88.81%\tGradient Norm: 0.000333\n",
      "Epoch: 0 [43520/60000 (72%)]\tLoss: 0.044177\tAccuracy: 88.83%\tGradient Norm: 0.000310\n",
      "Epoch: 0 [43584/60000 (73%)]\tLoss: 0.090036\tAccuracy: 88.84%\tGradient Norm: 0.000355\n",
      "Epoch: 0 [43648/60000 (73%)]\tLoss: 0.089660\tAccuracy: 88.85%\tGradient Norm: 0.000299\n",
      "Epoch: 0 [43712/60000 (73%)]\tLoss: 0.019433\tAccuracy: 88.87%\tGradient Norm: 0.000326\n",
      "Epoch: 0 [43776/60000 (73%)]\tLoss: 0.023912\tAccuracy: 88.89%\tGradient Norm: 0.000315\n",
      "Epoch: 0 [43840/60000 (73%)]\tLoss: 0.034665\tAccuracy: 88.90%\tGradient Norm: 0.000367\n",
      "Epoch: 0 [43904/60000 (73%)]\tLoss: 0.037167\tAccuracy: 88.92%\tGradient Norm: 0.000341\n",
      "Epoch: 0 [43968/60000 (73%)]\tLoss: 0.014365\tAccuracy: 88.93%\tGradient Norm: 0.000362\n",
      "Epoch: 0 [44032/60000 (73%)]\tLoss: 0.051791\tAccuracy: 88.95%\tGradient Norm: 0.000377\n",
      "Epoch: 0 [44096/60000 (73%)]\tLoss: 0.082236\tAccuracy: 88.96%\tGradient Norm: 0.000346\n",
      "Epoch: 0 [44160/60000 (74%)]\tLoss: 0.147745\tAccuracy: 88.97%\tGradient Norm: 0.000305\n",
      "Epoch: 0 [44224/60000 (74%)]\tLoss: 0.026933\tAccuracy: 88.98%\tGradient Norm: 0.000297\n",
      "Epoch: 0 [44288/60000 (74%)]\tLoss: 0.037671\tAccuracy: 89.00%\tGradient Norm: 0.000347\n",
      "Epoch: 0 [44352/60000 (74%)]\tLoss: 0.023288\tAccuracy: 89.01%\tGradient Norm: 0.000323\n",
      "Epoch: 0 [44416/60000 (74%)]\tLoss: 0.125166\tAccuracy: 89.02%\tGradient Norm: 0.000383\n",
      "Epoch: 0 [44480/60000 (74%)]\tLoss: 0.058808\tAccuracy: 89.04%\tGradient Norm: 0.000350\n",
      "Epoch: 0 [44544/60000 (74%)]\tLoss: 0.102899\tAccuracy: 89.04%\tGradient Norm: 0.000310\n",
      "Epoch: 0 [44608/60000 (74%)]\tLoss: 0.043184\tAccuracy: 89.06%\tGradient Norm: 0.000362\n",
      "Epoch: 0 [44672/60000 (74%)]\tLoss: 0.075593\tAccuracy: 89.07%\tGradient Norm: 0.000332\n",
      "Epoch: 0 [44736/60000 (75%)]\tLoss: 0.097114\tAccuracy: 89.08%\tGradient Norm: 0.000323\n",
      "Epoch: 0 [44800/60000 (75%)]\tLoss: 0.020274\tAccuracy: 89.09%\tGradient Norm: 0.000338\n",
      "Epoch: 0 [44864/60000 (75%)]\tLoss: 0.031025\tAccuracy: 89.11%\tGradient Norm: 0.000341\n",
      "Epoch: 0 [44928/60000 (75%)]\tLoss: 0.032019\tAccuracy: 89.12%\tGradient Norm: 0.000355\n",
      "Epoch: 0 [44992/60000 (75%)]\tLoss: 0.178618\tAccuracy: 89.13%\tGradient Norm: 0.000358\n",
      "Epoch: 0 [45056/60000 (75%)]\tLoss: 0.062440\tAccuracy: 89.15%\tGradient Norm: 0.000374\n",
      "Epoch: 0 [45120/60000 (75%)]\tLoss: 0.077327\tAccuracy: 89.16%\tGradient Norm: 0.000327\n",
      "Epoch: 0 [45184/60000 (75%)]\tLoss: 0.066667\tAccuracy: 89.17%\tGradient Norm: 0.000345\n",
      "Epoch: 0 [45248/60000 (75%)]\tLoss: 0.031881\tAccuracy: 89.18%\tGradient Norm: 0.000380\n",
      "Epoch: 0 [45312/60000 (75%)]\tLoss: 0.036828\tAccuracy: 89.20%\tGradient Norm: 0.000353\n",
      "Epoch: 0 [45376/60000 (76%)]\tLoss: 0.068927\tAccuracy: 89.21%\tGradient Norm: 0.000323\n",
      "Epoch: 0 [45440/60000 (76%)]\tLoss: 0.139478\tAccuracy: 89.22%\tGradient Norm: 0.000277\n",
      "Epoch: 0 [45504/60000 (76%)]\tLoss: 0.054250\tAccuracy: 89.23%\tGradient Norm: 0.000365\n",
      "Epoch: 0 [45568/60000 (76%)]\tLoss: 0.041611\tAccuracy: 89.25%\tGradient Norm: 0.000293\n",
      "Epoch: 0 [45632/60000 (76%)]\tLoss: 0.064865\tAccuracy: 89.26%\tGradient Norm: 0.000403\n",
      "Epoch: 0 [45696/60000 (76%)]\tLoss: 0.063864\tAccuracy: 89.27%\tGradient Norm: 0.000337\n",
      "Epoch: 0 [45760/60000 (76%)]\tLoss: 0.034140\tAccuracy: 89.29%\tGradient Norm: 0.000329\n",
      "Epoch: 0 [45824/60000 (76%)]\tLoss: 0.062941\tAccuracy: 89.30%\tGradient Norm: 0.000364\n",
      "Epoch: 0 [45888/60000 (76%)]\tLoss: 0.063524\tAccuracy: 89.31%\tGradient Norm: 0.000310\n",
      "Epoch: 0 [45952/60000 (77%)]\tLoss: 0.091108\tAccuracy: 89.33%\tGradient Norm: 0.000326\n",
      "Epoch: 0 [46016/60000 (77%)]\tLoss: 0.032066\tAccuracy: 89.34%\tGradient Norm: 0.000346\n",
      "Epoch: 0 [46080/60000 (77%)]\tLoss: 0.051726\tAccuracy: 89.35%\tGradient Norm: 0.000325\n",
      "Epoch: 0 [46144/60000 (77%)]\tLoss: 0.022979\tAccuracy: 89.37%\tGradient Norm: 0.000344\n",
      "Epoch: 0 [46208/60000 (77%)]\tLoss: 0.021686\tAccuracy: 89.38%\tGradient Norm: 0.000335\n",
      "Epoch: 0 [46272/60000 (77%)]\tLoss: 0.025138\tAccuracy: 89.39%\tGradient Norm: 0.000309\n",
      "Epoch: 0 [46336/60000 (77%)]\tLoss: 0.033988\tAccuracy: 89.41%\tGradient Norm: 0.000282\n",
      "Epoch: 0 [46400/60000 (77%)]\tLoss: 0.047648\tAccuracy: 89.42%\tGradient Norm: 0.000258\n",
      "Epoch: 0 [46464/60000 (77%)]\tLoss: 0.072459\tAccuracy: 89.43%\tGradient Norm: 0.000307\n",
      "Epoch: 0 [46528/60000 (78%)]\tLoss: 0.029094\tAccuracy: 89.45%\tGradient Norm: 0.000295\n",
      "Epoch: 0 [46592/60000 (78%)]\tLoss: 0.016292\tAccuracy: 89.46%\tGradient Norm: 0.000263\n",
      "Epoch: 0 [46656/60000 (78%)]\tLoss: 0.065188\tAccuracy: 89.47%\tGradient Norm: 0.000280\n",
      "Epoch: 0 [46720/60000 (78%)]\tLoss: 0.037595\tAccuracy: 89.49%\tGradient Norm: 0.000324\n",
      "Epoch: 0 [46784/60000 (78%)]\tLoss: 0.053410\tAccuracy: 89.50%\tGradient Norm: 0.000292\n",
      "Epoch: 0 [46848/60000 (78%)]\tLoss: 0.123313\tAccuracy: 89.51%\tGradient Norm: 0.000279\n",
      "Epoch: 0 [46912/60000 (78%)]\tLoss: 0.045663\tAccuracy: 89.52%\tGradient Norm: 0.000292\n",
      "Epoch: 0 [46976/60000 (78%)]\tLoss: 0.053399\tAccuracy: 89.54%\tGradient Norm: 0.000292\n",
      "Epoch: 0 [47040/60000 (78%)]\tLoss: 0.038930\tAccuracy: 89.55%\tGradient Norm: 0.000312\n",
      "Epoch: 0 [47104/60000 (78%)]\tLoss: 0.024035\tAccuracy: 89.57%\tGradient Norm: 0.000249\n",
      "Epoch: 0 [47168/60000 (79%)]\tLoss: 0.037460\tAccuracy: 89.58%\tGradient Norm: 0.000262\n",
      "Epoch: 0 [47232/60000 (79%)]\tLoss: 0.069727\tAccuracy: 89.59%\tGradient Norm: 0.000282\n",
      "Epoch: 0 [47296/60000 (79%)]\tLoss: 0.112211\tAccuracy: 89.60%\tGradient Norm: 0.000261\n",
      "Epoch: 0 [47360/60000 (79%)]\tLoss: 0.045720\tAccuracy: 89.61%\tGradient Norm: 0.000320\n",
      "Epoch: 0 [47424/60000 (79%)]\tLoss: 0.041921\tAccuracy: 89.62%\tGradient Norm: 0.000277\n",
      "Epoch: 0 [47488/60000 (79%)]\tLoss: 0.020006\tAccuracy: 89.64%\tGradient Norm: 0.000281\n",
      "Epoch: 0 [47552/60000 (79%)]\tLoss: 0.094299\tAccuracy: 89.65%\tGradient Norm: 0.000322\n",
      "Epoch: 0 [47616/60000 (79%)]\tLoss: 0.023942\tAccuracy: 89.66%\tGradient Norm: 0.000293\n",
      "Epoch: 0 [47680/60000 (79%)]\tLoss: 0.073584\tAccuracy: 89.67%\tGradient Norm: 0.000297\n",
      "Epoch: 0 [47744/60000 (80%)]\tLoss: 0.049850\tAccuracy: 89.69%\tGradient Norm: 0.000325\n",
      "Epoch: 0 [47808/60000 (80%)]\tLoss: 0.038410\tAccuracy: 89.70%\tGradient Norm: 0.000319\n",
      "Epoch: 0 [47872/60000 (80%)]\tLoss: 0.021555\tAccuracy: 89.71%\tGradient Norm: 0.000318\n",
      "Epoch: 0 [47936/60000 (80%)]\tLoss: 0.156915\tAccuracy: 89.72%\tGradient Norm: 0.000319\n",
      "Epoch: 0 [48000/60000 (80%)]\tLoss: 0.067186\tAccuracy: 89.73%\tGradient Norm: 0.000311\n",
      "Epoch: 0 [48064/60000 (80%)]\tLoss: 0.088266\tAccuracy: 89.74%\tGradient Norm: 0.000305\n",
      "Epoch: 0 [48128/60000 (80%)]\tLoss: 0.030420\tAccuracy: 89.75%\tGradient Norm: 0.000288\n",
      "Epoch: 0 [48192/60000 (80%)]\tLoss: 0.084832\tAccuracy: 89.76%\tGradient Norm: 0.000260\n",
      "Epoch: 0 [48256/60000 (80%)]\tLoss: 0.022640\tAccuracy: 89.77%\tGradient Norm: 0.000273\n",
      "Epoch: 0 [48320/60000 (80%)]\tLoss: 0.027202\tAccuracy: 89.79%\tGradient Norm: 0.000278\n",
      "Epoch: 0 [48384/60000 (81%)]\tLoss: 0.073660\tAccuracy: 89.80%\tGradient Norm: 0.000264\n",
      "Epoch: 0 [48448/60000 (81%)]\tLoss: 0.035774\tAccuracy: 89.81%\tGradient Norm: 0.000273\n",
      "Epoch: 0 [48512/60000 (81%)]\tLoss: 0.032612\tAccuracy: 89.82%\tGradient Norm: 0.000277\n",
      "Epoch: 0 [48576/60000 (81%)]\tLoss: 0.022889\tAccuracy: 89.83%\tGradient Norm: 0.000260\n",
      "Epoch: 0 [48640/60000 (81%)]\tLoss: 0.048459\tAccuracy: 89.85%\tGradient Norm: 0.000283\n",
      "Epoch: 0 [48704/60000 (81%)]\tLoss: 0.036099\tAccuracy: 89.86%\tGradient Norm: 0.000303\n",
      "Epoch: 0 [48768/60000 (81%)]\tLoss: 0.044103\tAccuracy: 89.87%\tGradient Norm: 0.000330\n",
      "Epoch: 0 [48832/60000 (81%)]\tLoss: 0.029195\tAccuracy: 89.88%\tGradient Norm: 0.000272\n",
      "Epoch: 0 [48896/60000 (81%)]\tLoss: 0.172137\tAccuracy: 89.89%\tGradient Norm: 0.000311\n",
      "Epoch: 0 [48960/60000 (82%)]\tLoss: 0.024278\tAccuracy: 89.90%\tGradient Norm: 0.000254\n",
      "Epoch: 0 [49024/60000 (82%)]\tLoss: 0.016321\tAccuracy: 89.91%\tGradient Norm: 0.000292\n",
      "Epoch: 0 [49088/60000 (82%)]\tLoss: 0.016277\tAccuracy: 89.93%\tGradient Norm: 0.000277\n",
      "Epoch: 0 [49152/60000 (82%)]\tLoss: 0.019546\tAccuracy: 89.94%\tGradient Norm: 0.000274\n",
      "Epoch: 0 [49216/60000 (82%)]\tLoss: 0.110649\tAccuracy: 89.95%\tGradient Norm: 0.000262\n",
      "Epoch: 0 [49280/60000 (82%)]\tLoss: 0.096137\tAccuracy: 89.96%\tGradient Norm: 0.000298\n",
      "Epoch: 0 [49344/60000 (82%)]\tLoss: 0.161634\tAccuracy: 89.97%\tGradient Norm: 0.000269\n",
      "Epoch: 0 [49408/60000 (82%)]\tLoss: 0.124673\tAccuracy: 89.97%\tGradient Norm: 0.000257\n",
      "Epoch: 0 [49472/60000 (82%)]\tLoss: 0.024238\tAccuracy: 89.99%\tGradient Norm: 0.000299\n",
      "Epoch: 0 [49536/60000 (83%)]\tLoss: 0.023963\tAccuracy: 90.00%\tGradient Norm: 0.000281\n",
      "Epoch: 0 [49600/60000 (83%)]\tLoss: 0.032150\tAccuracy: 90.01%\tGradient Norm: 0.000225\n",
      "Epoch: 0 [49664/60000 (83%)]\tLoss: 0.029031\tAccuracy: 90.02%\tGradient Norm: 0.000227\n",
      "Epoch: 0 [49728/60000 (83%)]\tLoss: 0.067694\tAccuracy: 90.03%\tGradient Norm: 0.000277\n",
      "Epoch: 0 [49792/60000 (83%)]\tLoss: 0.048221\tAccuracy: 90.04%\tGradient Norm: 0.000285\n",
      "Epoch: 0 [49856/60000 (83%)]\tLoss: 0.136164\tAccuracy: 90.05%\tGradient Norm: 0.000319\n",
      "Epoch: 0 [49920/60000 (83%)]\tLoss: 0.076887\tAccuracy: 90.06%\tGradient Norm: 0.000277\n",
      "Epoch: 0 [49984/60000 (83%)]\tLoss: 0.016959\tAccuracy: 90.07%\tGradient Norm: 0.000269\n",
      "Epoch: 0 [50048/60000 (83%)]\tLoss: 0.018014\tAccuracy: 90.09%\tGradient Norm: 0.000226\n",
      "Epoch: 0 [50112/60000 (83%)]\tLoss: 0.053921\tAccuracy: 90.09%\tGradient Norm: 0.000281\n",
      "Epoch: 0 [50176/60000 (84%)]\tLoss: 0.062852\tAccuracy: 90.11%\tGradient Norm: 0.000258\n",
      "Epoch: 0 [50240/60000 (84%)]\tLoss: 0.044176\tAccuracy: 90.12%\tGradient Norm: 0.000259\n",
      "Epoch: 0 [50304/60000 (84%)]\tLoss: 0.021294\tAccuracy: 90.13%\tGradient Norm: 0.000293\n",
      "Epoch: 0 [50368/60000 (84%)]\tLoss: 0.085808\tAccuracy: 90.14%\tGradient Norm: 0.000262\n",
      "Epoch: 0 [50432/60000 (84%)]\tLoss: 0.021372\tAccuracy: 90.15%\tGradient Norm: 0.000295\n",
      "Epoch: 0 [50496/60000 (84%)]\tLoss: 0.014257\tAccuracy: 90.16%\tGradient Norm: 0.000289\n",
      "Epoch: 0 [50560/60000 (84%)]\tLoss: 0.012470\tAccuracy: 90.18%\tGradient Norm: 0.000268\n",
      "Epoch: 0 [50624/60000 (84%)]\tLoss: 0.027862\tAccuracy: 90.19%\tGradient Norm: 0.000260\n",
      "Epoch: 0 [50688/60000 (84%)]\tLoss: 0.017773\tAccuracy: 90.20%\tGradient Norm: 0.000287\n",
      "Epoch: 0 [50752/60000 (85%)]\tLoss: 0.150367\tAccuracy: 90.21%\tGradient Norm: 0.000256\n",
      "Epoch: 0 [50816/60000 (85%)]\tLoss: 0.154522\tAccuracy: 90.22%\tGradient Norm: 0.000241\n",
      "Epoch: 0 [50880/60000 (85%)]\tLoss: 0.101883\tAccuracy: 90.22%\tGradient Norm: 0.000306\n",
      "Epoch: 0 [50944/60000 (85%)]\tLoss: 0.088238\tAccuracy: 90.23%\tGradient Norm: 0.000325\n",
      "Epoch: 0 [51008/60000 (85%)]\tLoss: 0.051974\tAccuracy: 90.24%\tGradient Norm: 0.000273\n",
      "Epoch: 0 [51072/60000 (85%)]\tLoss: 0.118118\tAccuracy: 90.24%\tGradient Norm: 0.000267\n",
      "Epoch: 0 [51136/60000 (85%)]\tLoss: 0.073407\tAccuracy: 90.25%\tGradient Norm: 0.000293\n",
      "Epoch: 0 [51200/60000 (85%)]\tLoss: 0.042515\tAccuracy: 90.26%\tGradient Norm: 0.000257\n",
      "Epoch: 0 [51264/60000 (85%)]\tLoss: 0.090534\tAccuracy: 90.27%\tGradient Norm: 0.000264\n",
      "Epoch: 0 [51328/60000 (86%)]\tLoss: 0.037407\tAccuracy: 90.28%\tGradient Norm: 0.000276\n",
      "Epoch: 0 [51392/60000 (86%)]\tLoss: 0.024280\tAccuracy: 90.30%\tGradient Norm: 0.000284\n",
      "Epoch: 0 [51456/60000 (86%)]\tLoss: 0.056210\tAccuracy: 90.31%\tGradient Norm: 0.000264\n",
      "Epoch: 0 [51520/60000 (86%)]\tLoss: 0.057398\tAccuracy: 90.32%\tGradient Norm: 0.000308\n",
      "Epoch: 0 [51584/60000 (86%)]\tLoss: 0.124668\tAccuracy: 90.32%\tGradient Norm: 0.000319\n",
      "Epoch: 0 [51648/60000 (86%)]\tLoss: 0.063821\tAccuracy: 90.33%\tGradient Norm: 0.000342\n",
      "Epoch: 0 [51712/60000 (86%)]\tLoss: 0.079133\tAccuracy: 90.34%\tGradient Norm: 0.000289\n",
      "Epoch: 0 [51776/60000 (86%)]\tLoss: 0.057414\tAccuracy: 90.35%\tGradient Norm: 0.000356\n",
      "Epoch: 0 [51840/60000 (86%)]\tLoss: 0.012669\tAccuracy: 90.36%\tGradient Norm: 0.000286\n",
      "Epoch: 0 [51904/60000 (86%)]\tLoss: 0.028622\tAccuracy: 90.38%\tGradient Norm: 0.000295\n",
      "Epoch: 0 [51968/60000 (87%)]\tLoss: 0.022795\tAccuracy: 90.39%\tGradient Norm: 0.000299\n",
      "Epoch: 0 [52032/60000 (87%)]\tLoss: 0.039035\tAccuracy: 90.40%\tGradient Norm: 0.000303\n",
      "Epoch: 0 [52096/60000 (87%)]\tLoss: 0.128415\tAccuracy: 90.40%\tGradient Norm: 0.000257\n",
      "Epoch: 0 [52160/60000 (87%)]\tLoss: 0.025055\tAccuracy: 90.42%\tGradient Norm: 0.000291\n",
      "Epoch: 0 [52224/60000 (87%)]\tLoss: 0.034222\tAccuracy: 90.43%\tGradient Norm: 0.000295\n",
      "Epoch: 0 [52288/60000 (87%)]\tLoss: 0.020769\tAccuracy: 90.44%\tGradient Norm: 0.000300\n",
      "Epoch: 0 [52352/60000 (87%)]\tLoss: 0.037270\tAccuracy: 90.45%\tGradient Norm: 0.000289\n",
      "Epoch: 0 [52416/60000 (87%)]\tLoss: 0.021574\tAccuracy: 90.46%\tGradient Norm: 0.000294\n",
      "Epoch: 0 [52480/60000 (87%)]\tLoss: 0.010153\tAccuracy: 90.47%\tGradient Norm: 0.000290\n",
      "Epoch: 0 [52544/60000 (88%)]\tLoss: 0.092828\tAccuracy: 90.48%\tGradient Norm: 0.000269\n",
      "Epoch: 0 [52608/60000 (88%)]\tLoss: 0.107967\tAccuracy: 90.49%\tGradient Norm: 0.000276\n",
      "Epoch: 0 [52672/60000 (88%)]\tLoss: 0.156953\tAccuracy: 90.49%\tGradient Norm: 0.000355\n",
      "Epoch: 0 [52736/60000 (88%)]\tLoss: 0.055332\tAccuracy: 90.50%\tGradient Norm: 0.000322\n",
      "Epoch: 0 [52800/60000 (88%)]\tLoss: 0.010483\tAccuracy: 90.51%\tGradient Norm: 0.000319\n",
      "Epoch: 0 [52864/60000 (88%)]\tLoss: 0.042461\tAccuracy: 90.52%\tGradient Norm: 0.000320\n",
      "Epoch: 0 [52928/60000 (88%)]\tLoss: 0.066926\tAccuracy: 90.53%\tGradient Norm: 0.000315\n",
      "Epoch: 0 [52992/60000 (88%)]\tLoss: 0.051728\tAccuracy: 90.54%\tGradient Norm: 0.000267\n",
      "Epoch: 0 [53056/60000 (88%)]\tLoss: 0.038593\tAccuracy: 90.55%\tGradient Norm: 0.000287\n",
      "Epoch: 0 [53120/60000 (88%)]\tLoss: 0.087102\tAccuracy: 90.56%\tGradient Norm: 0.000311\n",
      "Epoch: 0 [53184/60000 (89%)]\tLoss: 0.041980\tAccuracy: 90.57%\tGradient Norm: 0.000267\n",
      "Epoch: 0 [53248/60000 (89%)]\tLoss: 0.019403\tAccuracy: 90.58%\tGradient Norm: 0.000308\n",
      "Epoch: 0 [53312/60000 (89%)]\tLoss: 0.031458\tAccuracy: 90.59%\tGradient Norm: 0.000310\n",
      "Epoch: 0 [53376/60000 (89%)]\tLoss: 0.016870\tAccuracy: 90.60%\tGradient Norm: 0.000263\n",
      "Epoch: 0 [53440/60000 (89%)]\tLoss: 0.048847\tAccuracy: 90.61%\tGradient Norm: 0.000313\n",
      "Epoch: 0 [53504/60000 (89%)]\tLoss: 0.066685\tAccuracy: 90.62%\tGradient Norm: 0.000275\n",
      "Epoch: 0 [53568/60000 (89%)]\tLoss: 0.143164\tAccuracy: 90.63%\tGradient Norm: 0.000299\n",
      "Epoch: 0 [53632/60000 (89%)]\tLoss: 0.026162\tAccuracy: 90.64%\tGradient Norm: 0.000248\n",
      "Epoch: 0 [53696/60000 (89%)]\tLoss: 0.083872\tAccuracy: 90.65%\tGradient Norm: 0.000304\n",
      "Epoch: 0 [53760/60000 (90%)]\tLoss: 0.077090\tAccuracy: 90.65%\tGradient Norm: 0.000252\n",
      "Epoch: 0 [53824/60000 (90%)]\tLoss: 0.022897\tAccuracy: 90.66%\tGradient Norm: 0.000279\n",
      "Epoch: 0 [53888/60000 (90%)]\tLoss: 0.047938\tAccuracy: 90.67%\tGradient Norm: 0.000255\n",
      "Epoch: 0 [53952/60000 (90%)]\tLoss: 0.096692\tAccuracy: 90.68%\tGradient Norm: 0.000302\n",
      "Epoch: 0 [54016/60000 (90%)]\tLoss: 0.007652\tAccuracy: 90.69%\tGradient Norm: 0.000284\n",
      "Epoch: 0 [54080/60000 (90%)]\tLoss: 0.226027\tAccuracy: 90.70%\tGradient Norm: 0.000308\n",
      "Epoch: 0 [54144/60000 (90%)]\tLoss: 0.108852\tAccuracy: 90.71%\tGradient Norm: 0.000323\n",
      "Epoch: 0 [54208/60000 (90%)]\tLoss: 0.048055\tAccuracy: 90.72%\tGradient Norm: 0.000293\n",
      "Epoch: 0 [54272/60000 (90%)]\tLoss: 0.081478\tAccuracy: 90.73%\tGradient Norm: 0.000289\n",
      "Epoch: 0 [54336/60000 (91%)]\tLoss: 0.068216\tAccuracy: 90.74%\tGradient Norm: 0.000279\n",
      "Epoch: 0 [54400/60000 (91%)]\tLoss: 0.082810\tAccuracy: 90.74%\tGradient Norm: 0.000343\n",
      "Epoch: 0 [54464/60000 (91%)]\tLoss: 0.062012\tAccuracy: 90.75%\tGradient Norm: 0.000284\n",
      "Epoch: 0 [54528/60000 (91%)]\tLoss: 0.054984\tAccuracy: 90.76%\tGradient Norm: 0.000246\n",
      "Epoch: 0 [54592/60000 (91%)]\tLoss: 0.012941\tAccuracy: 90.77%\tGradient Norm: 0.000263\n",
      "Epoch: 0 [54656/60000 (91%)]\tLoss: 0.014783\tAccuracy: 90.78%\tGradient Norm: 0.000293\n",
      "Epoch: 0 [54720/60000 (91%)]\tLoss: 0.017629\tAccuracy: 90.79%\tGradient Norm: 0.000257\n",
      "Epoch: 0 [54784/60000 (91%)]\tLoss: 0.065554\tAccuracy: 90.80%\tGradient Norm: 0.000296\n",
      "Epoch: 0 [54848/60000 (91%)]\tLoss: 0.021965\tAccuracy: 90.81%\tGradient Norm: 0.000287\n",
      "Epoch: 0 [54912/60000 (91%)]\tLoss: 0.099830\tAccuracy: 90.82%\tGradient Norm: 0.000316\n",
      "Epoch: 0 [54976/60000 (92%)]\tLoss: 0.021550\tAccuracy: 90.83%\tGradient Norm: 0.000287\n",
      "Epoch: 0 [55040/60000 (92%)]\tLoss: 0.092098\tAccuracy: 90.83%\tGradient Norm: 0.000279\n",
      "Epoch: 0 [55104/60000 (92%)]\tLoss: 0.034754\tAccuracy: 90.84%\tGradient Norm: 0.000291\n",
      "Epoch: 0 [55168/60000 (92%)]\tLoss: 0.117935\tAccuracy: 90.85%\tGradient Norm: 0.000322\n",
      "Epoch: 0 [55232/60000 (92%)]\tLoss: 0.144551\tAccuracy: 90.86%\tGradient Norm: 0.000334\n",
      "Epoch: 0 [55296/60000 (92%)]\tLoss: 0.036579\tAccuracy: 90.87%\tGradient Norm: 0.000269\n",
      "Epoch: 0 [55360/60000 (92%)]\tLoss: 0.015287\tAccuracy: 90.88%\tGradient Norm: 0.000272\n",
      "Epoch: 0 [55424/60000 (92%)]\tLoss: 0.048037\tAccuracy: 90.89%\tGradient Norm: 0.000265\n",
      "Epoch: 0 [55488/60000 (92%)]\tLoss: 0.020042\tAccuracy: 90.90%\tGradient Norm: 0.000241\n",
      "Epoch: 0 [55552/60000 (93%)]\tLoss: 0.025636\tAccuracy: 90.91%\tGradient Norm: 0.000244\n",
      "Epoch: 0 [55616/60000 (93%)]\tLoss: 0.014828\tAccuracy: 90.92%\tGradient Norm: 0.000275\n",
      "Epoch: 0 [55680/60000 (93%)]\tLoss: 0.061490\tAccuracy: 90.93%\tGradient Norm: 0.000280\n",
      "Epoch: 0 [55744/60000 (93%)]\tLoss: 0.035434\tAccuracy: 90.94%\tGradient Norm: 0.000263\n",
      "Epoch: 0 [55808/60000 (93%)]\tLoss: 0.132755\tAccuracy: 90.94%\tGradient Norm: 0.000287\n",
      "Epoch: 0 [55872/60000 (93%)]\tLoss: 0.068792\tAccuracy: 90.95%\tGradient Norm: 0.000290\n",
      "Epoch: 0 [55936/60000 (93%)]\tLoss: 0.035779\tAccuracy: 90.96%\tGradient Norm: 0.000291\n",
      "Epoch: 0 [56000/60000 (93%)]\tLoss: 0.141170\tAccuracy: 90.96%\tGradient Norm: 0.000257\n",
      "Epoch: 0 [56064/60000 (93%)]\tLoss: 0.032646\tAccuracy: 90.97%\tGradient Norm: 0.000255\n",
      "Epoch: 0 [56128/60000 (93%)]\tLoss: 0.037226\tAccuracy: 90.98%\tGradient Norm: 0.000270\n",
      "Epoch: 0 [56192/60000 (94%)]\tLoss: 0.034480\tAccuracy: 90.99%\tGradient Norm: 0.000278\n",
      "Epoch: 0 [56256/60000 (94%)]\tLoss: 0.064540\tAccuracy: 91.00%\tGradient Norm: 0.000247\n",
      "Epoch: 0 [56320/60000 (94%)]\tLoss: 0.027106\tAccuracy: 91.01%\tGradient Norm: 0.000253\n",
      "Epoch: 0 [56384/60000 (94%)]\tLoss: 0.008812\tAccuracy: 91.02%\tGradient Norm: 0.000262\n",
      "Epoch: 0 [56448/60000 (94%)]\tLoss: 0.103112\tAccuracy: 91.03%\tGradient Norm: 0.000251\n",
      "Epoch: 0 [56512/60000 (94%)]\tLoss: 0.046991\tAccuracy: 91.04%\tGradient Norm: 0.000234\n",
      "Epoch: 0 [56576/60000 (94%)]\tLoss: 0.062595\tAccuracy: 91.05%\tGradient Norm: 0.000252\n",
      "Epoch: 0 [56640/60000 (94%)]\tLoss: 0.071143\tAccuracy: 91.06%\tGradient Norm: 0.000255\n",
      "Epoch: 0 [56704/60000 (94%)]\tLoss: 0.032343\tAccuracy: 91.07%\tGradient Norm: 0.000246\n",
      "Epoch: 0 [56768/60000 (95%)]\tLoss: 0.111484\tAccuracy: 91.07%\tGradient Norm: 0.000241\n",
      "Epoch: 0 [56832/60000 (95%)]\tLoss: 0.052858\tAccuracy: 91.08%\tGradient Norm: 0.000243\n",
      "Epoch: 0 [56896/60000 (95%)]\tLoss: 0.018488\tAccuracy: 91.09%\tGradient Norm: 0.000252\n",
      "Epoch: 0 [56960/60000 (95%)]\tLoss: 0.055567\tAccuracy: 91.10%\tGradient Norm: 0.000226\n",
      "Epoch: 0 [57024/60000 (95%)]\tLoss: 0.043699\tAccuracy: 91.11%\tGradient Norm: 0.000245\n",
      "Epoch: 0 [57088/60000 (95%)]\tLoss: 0.062930\tAccuracy: 91.11%\tGradient Norm: 0.000231\n",
      "Epoch: 0 [57152/60000 (95%)]\tLoss: 0.074194\tAccuracy: 91.12%\tGradient Norm: 0.000230\n",
      "Epoch: 0 [57216/60000 (95%)]\tLoss: 0.078107\tAccuracy: 91.13%\tGradient Norm: 0.000248\n",
      "Epoch: 0 [57280/60000 (95%)]\tLoss: 0.009055\tAccuracy: 91.14%\tGradient Norm: 0.000277\n",
      "Epoch: 0 [57344/60000 (96%)]\tLoss: 0.012177\tAccuracy: 91.15%\tGradient Norm: 0.000211\n",
      "Epoch: 0 [57408/60000 (96%)]\tLoss: 0.055224\tAccuracy: 91.16%\tGradient Norm: 0.000237\n",
      "Epoch: 0 [57472/60000 (96%)]\tLoss: 0.116333\tAccuracy: 91.16%\tGradient Norm: 0.000235\n",
      "Epoch: 0 [57536/60000 (96%)]\tLoss: 0.019342\tAccuracy: 91.17%\tGradient Norm: 0.000266\n",
      "Epoch: 0 [57600/60000 (96%)]\tLoss: 0.041269\tAccuracy: 91.18%\tGradient Norm: 0.000200\n",
      "Epoch: 0 [57664/60000 (96%)]\tLoss: 0.019827\tAccuracy: 91.19%\tGradient Norm: 0.000259\n",
      "Epoch: 0 [57728/60000 (96%)]\tLoss: 0.030121\tAccuracy: 91.20%\tGradient Norm: 0.000245\n",
      "Epoch: 0 [57792/60000 (96%)]\tLoss: 0.022974\tAccuracy: 91.21%\tGradient Norm: 0.000250\n",
      "Epoch: 0 [57856/60000 (96%)]\tLoss: 0.046072\tAccuracy: 91.22%\tGradient Norm: 0.000218\n",
      "Epoch: 0 [57920/60000 (96%)]\tLoss: 0.155736\tAccuracy: 91.22%\tGradient Norm: 0.000254\n",
      "Epoch: 0 [57984/60000 (97%)]\tLoss: 0.081565\tAccuracy: 91.23%\tGradient Norm: 0.000252\n",
      "Epoch: 0 [58048/60000 (97%)]\tLoss: 0.061149\tAccuracy: 91.24%\tGradient Norm: 0.000230\n",
      "Epoch: 0 [58112/60000 (97%)]\tLoss: 0.048012\tAccuracy: 91.25%\tGradient Norm: 0.000220\n",
      "Epoch: 0 [58176/60000 (97%)]\tLoss: 0.025228\tAccuracy: 91.26%\tGradient Norm: 0.000273\n",
      "Epoch: 0 [58240/60000 (97%)]\tLoss: 0.027465\tAccuracy: 91.27%\tGradient Norm: 0.000234\n",
      "Epoch: 0 [58304/60000 (97%)]\tLoss: 0.006085\tAccuracy: 91.28%\tGradient Norm: 0.000261\n",
      "Epoch: 0 [58368/60000 (97%)]\tLoss: 0.043323\tAccuracy: 91.28%\tGradient Norm: 0.000272\n",
      "Epoch: 0 [58432/60000 (97%)]\tLoss: 0.034526\tAccuracy: 91.29%\tGradient Norm: 0.000232\n",
      "Epoch: 0 [58496/60000 (97%)]\tLoss: 0.005768\tAccuracy: 91.30%\tGradient Norm: 0.000245\n",
      "Epoch: 0 [58560/60000 (98%)]\tLoss: 0.070565\tAccuracy: 91.31%\tGradient Norm: 0.000230\n",
      "Epoch: 0 [58624/60000 (98%)]\tLoss: 0.087971\tAccuracy: 91.32%\tGradient Norm: 0.000218\n",
      "Epoch: 0 [58688/60000 (98%)]\tLoss: 0.051120\tAccuracy: 91.32%\tGradient Norm: 0.000228\n",
      "Epoch: 0 [58752/60000 (98%)]\tLoss: 0.080106\tAccuracy: 91.33%\tGradient Norm: 0.000227\n",
      "Epoch: 0 [58816/60000 (98%)]\tLoss: 0.073438\tAccuracy: 91.34%\tGradient Norm: 0.000253\n",
      "Epoch: 0 [58880/60000 (98%)]\tLoss: 0.060575\tAccuracy: 91.34%\tGradient Norm: 0.000245\n",
      "Epoch: 0 [58944/60000 (98%)]\tLoss: 0.015128\tAccuracy: 91.35%\tGradient Norm: 0.000218\n",
      "Epoch: 0 [59008/60000 (98%)]\tLoss: 0.056252\tAccuracy: 91.36%\tGradient Norm: 0.000298\n",
      "Epoch: 0 [59072/60000 (98%)]\tLoss: 0.026518\tAccuracy: 91.37%\tGradient Norm: 0.000259\n",
      "Epoch: 0 [59136/60000 (99%)]\tLoss: 0.030799\tAccuracy: 91.38%\tGradient Norm: 0.000235\n",
      "Epoch: 0 [59200/60000 (99%)]\tLoss: 0.030002\tAccuracy: 91.39%\tGradient Norm: 0.000252\n",
      "Epoch: 0 [59264/60000 (99%)]\tLoss: 0.030871\tAccuracy: 91.40%\tGradient Norm: 0.000242\n",
      "Epoch: 0 [59328/60000 (99%)]\tLoss: 0.035087\tAccuracy: 91.40%\tGradient Norm: 0.000266\n",
      "Epoch: 0 [59392/60000 (99%)]\tLoss: 0.129843\tAccuracy: 91.41%\tGradient Norm: 0.000250\n",
      "Epoch: 0 [59456/60000 (99%)]\tLoss: 0.074331\tAccuracy: 91.42%\tGradient Norm: 0.000250\n",
      "Epoch: 0 [59520/60000 (99%)]\tLoss: 0.112632\tAccuracy: 91.42%\tGradient Norm: 0.000236\n",
      "Epoch: 0 [59584/60000 (99%)]\tLoss: 0.010343\tAccuracy: 91.43%\tGradient Norm: 0.000254\n",
      "Epoch: 0 [59648/60000 (99%)]\tLoss: 0.043224\tAccuracy: 91.44%\tGradient Norm: 0.000273\n",
      "Epoch: 0 [59712/60000 (99%)]\tLoss: 0.051617\tAccuracy: 91.45%\tGradient Norm: 0.000255\n",
      "Epoch: 0 [59776/60000 (100%)]\tLoss: 0.034597\tAccuracy: 91.46%\tGradient Norm: 0.000269\n",
      "Epoch: 0 [59840/60000 (100%)]\tLoss: 0.045399\tAccuracy: 91.46%\tGradient Norm: 0.000261\n",
      "Epoch: 0 [59904/60000 (100%)]\tLoss: 0.013549\tAccuracy: 91.47%\tGradient Norm: 0.000241\n",
      "Epoch: 0 [29984/60000 (100%)]\tLoss: 0.166920\tAccuracy: 91.47%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [0/60000 (0%)]\tLoss: 0.026768\tAccuracy: 100.00%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [64/60000 (0%)]\tLoss: 0.156197\tAccuracy: 98.44%\tGradient Norm: 0.000294\n",
      "Epoch: 1 [128/60000 (0%)]\tLoss: 0.035162\tAccuracy: 98.44%\tGradient Norm: 0.000280\n",
      "Epoch: 1 [192/60000 (0%)]\tLoss: 0.023119\tAccuracy: 98.83%\tGradient Norm: 0.000251\n",
      "Epoch: 1 [256/60000 (0%)]\tLoss: 0.056782\tAccuracy: 98.75%\tGradient Norm: 0.000252\n",
      "Epoch: 1 [320/60000 (1%)]\tLoss: 0.049209\tAccuracy: 98.96%\tGradient Norm: 0.000274\n",
      "Epoch: 1 [384/60000 (1%)]\tLoss: 0.023929\tAccuracy: 99.11%\tGradient Norm: 0.000251\n",
      "Epoch: 1 [448/60000 (1%)]\tLoss: 0.006846\tAccuracy: 99.22%\tGradient Norm: 0.000271\n",
      "Epoch: 1 [512/60000 (1%)]\tLoss: 0.033889\tAccuracy: 99.31%\tGradient Norm: 0.000278\n",
      "Epoch: 1 [576/60000 (1%)]\tLoss: 0.035290\tAccuracy: 99.22%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [640/60000 (1%)]\tLoss: 0.012014\tAccuracy: 99.29%\tGradient Norm: 0.000256\n",
      "Epoch: 1 [704/60000 (1%)]\tLoss: 0.060261\tAccuracy: 99.22%\tGradient Norm: 0.000265\n",
      "Epoch: 1 [768/60000 (1%)]\tLoss: 0.093525\tAccuracy: 99.04%\tGradient Norm: 0.000241\n",
      "Epoch: 1 [832/60000 (1%)]\tLoss: 0.086512\tAccuracy: 99.00%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [896/60000 (1%)]\tLoss: 0.021093\tAccuracy: 99.06%\tGradient Norm: 0.000238\n",
      "Epoch: 1 [960/60000 (2%)]\tLoss: 0.013693\tAccuracy: 99.12%\tGradient Norm: 0.000306\n",
      "Epoch: 1 [1024/60000 (2%)]\tLoss: 0.020454\tAccuracy: 99.17%\tGradient Norm: 0.000272\n",
      "Epoch: 1 [1088/60000 (2%)]\tLoss: 0.105628\tAccuracy: 99.05%\tGradient Norm: 0.000289\n",
      "Epoch: 1 [1152/60000 (2%)]\tLoss: 0.049956\tAccuracy: 99.01%\tGradient Norm: 0.000251\n",
      "Epoch: 1 [1216/60000 (2%)]\tLoss: 0.022336\tAccuracy: 99.06%\tGradient Norm: 0.000269\n",
      "Epoch: 1 [1280/60000 (2%)]\tLoss: 0.009554\tAccuracy: 99.11%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [1344/60000 (2%)]\tLoss: 0.065663\tAccuracy: 99.08%\tGradient Norm: 0.000247\n",
      "Epoch: 1 [1408/60000 (2%)]\tLoss: 0.059579\tAccuracy: 99.05%\tGradient Norm: 0.000252\n",
      "Epoch: 1 [1472/60000 (2%)]\tLoss: 0.046975\tAccuracy: 99.09%\tGradient Norm: 0.000264\n",
      "Epoch: 1 [1536/60000 (3%)]\tLoss: 0.031444\tAccuracy: 99.12%\tGradient Norm: 0.000297\n",
      "Epoch: 1 [1600/60000 (3%)]\tLoss: 0.009812\tAccuracy: 99.16%\tGradient Norm: 0.000263\n",
      "Epoch: 1 [1664/60000 (3%)]\tLoss: 0.025904\tAccuracy: 99.13%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [1728/60000 (3%)]\tLoss: 0.085843\tAccuracy: 99.11%\tGradient Norm: 0.000285\n",
      "Epoch: 1 [1792/60000 (3%)]\tLoss: 0.061413\tAccuracy: 99.03%\tGradient Norm: 0.000275\n",
      "Epoch: 1 [1856/60000 (3%)]\tLoss: 0.102945\tAccuracy: 98.96%\tGradient Norm: 0.000241\n",
      "Epoch: 1 [1920/60000 (3%)]\tLoss: 0.043687\tAccuracy: 98.94%\tGradient Norm: 0.000254\n",
      "Epoch: 1 [1984/60000 (3%)]\tLoss: 0.005018\tAccuracy: 98.97%\tGradient Norm: 0.000274\n",
      "Epoch: 1 [2048/60000 (3%)]\tLoss: 0.029808\tAccuracy: 99.01%\tGradient Norm: 0.000273\n",
      "Epoch: 1 [2112/60000 (4%)]\tLoss: 0.031841\tAccuracy: 99.03%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [2176/60000 (4%)]\tLoss: 0.025669\tAccuracy: 99.06%\tGradient Norm: 0.000291\n",
      "Epoch: 1 [2240/60000 (4%)]\tLoss: 0.025829\tAccuracy: 99.09%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [2304/60000 (4%)]\tLoss: 0.028377\tAccuracy: 99.07%\tGradient Norm: 0.000247\n",
      "Epoch: 1 [2368/60000 (4%)]\tLoss: 0.046505\tAccuracy: 99.01%\tGradient Norm: 0.000257\n",
      "Epoch: 1 [2432/60000 (4%)]\tLoss: 0.048668\tAccuracy: 98.96%\tGradient Norm: 0.000244\n",
      "Epoch: 1 [2496/60000 (4%)]\tLoss: 0.040841\tAccuracy: 98.98%\tGradient Norm: 0.000290\n",
      "Epoch: 1 [2560/60000 (4%)]\tLoss: 0.023362\tAccuracy: 99.01%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [2624/60000 (4%)]\tLoss: 0.012777\tAccuracy: 99.03%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [2688/60000 (4%)]\tLoss: 0.011177\tAccuracy: 99.06%\tGradient Norm: 0.000248\n",
      "Epoch: 1 [2752/60000 (5%)]\tLoss: 0.032224\tAccuracy: 99.04%\tGradient Norm: 0.000294\n",
      "Epoch: 1 [2816/60000 (5%)]\tLoss: 0.049878\tAccuracy: 99.03%\tGradient Norm: 0.000245\n",
      "Epoch: 1 [2880/60000 (5%)]\tLoss: 0.018334\tAccuracy: 99.05%\tGradient Norm: 0.000239\n",
      "Epoch: 1 [2944/60000 (5%)]\tLoss: 0.024029\tAccuracy: 99.04%\tGradient Norm: 0.000269\n",
      "Epoch: 1 [3008/60000 (5%)]\tLoss: 0.101570\tAccuracy: 99.02%\tGradient Norm: 0.000239\n",
      "Epoch: 1 [3072/60000 (5%)]\tLoss: 0.043433\tAccuracy: 99.01%\tGradient Norm: 0.000261\n",
      "Epoch: 1 [3136/60000 (5%)]\tLoss: 0.014541\tAccuracy: 99.03%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [3200/60000 (5%)]\tLoss: 0.060766\tAccuracy: 99.02%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [3264/60000 (5%)]\tLoss: 0.125115\tAccuracy: 99.01%\tGradient Norm: 0.000274\n",
      "Epoch: 1 [3328/60000 (6%)]\tLoss: 0.047874\tAccuracy: 99.00%\tGradient Norm: 0.000267\n",
      "Epoch: 1 [3392/60000 (6%)]\tLoss: 0.013597\tAccuracy: 99.02%\tGradient Norm: 0.000269\n",
      "Epoch: 1 [3456/60000 (6%)]\tLoss: 0.015027\tAccuracy: 99.03%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [3520/60000 (6%)]\tLoss: 0.023606\tAccuracy: 99.05%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [3584/60000 (6%)]\tLoss: 0.053717\tAccuracy: 99.04%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [3648/60000 (6%)]\tLoss: 0.005183\tAccuracy: 99.06%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [3712/60000 (6%)]\tLoss: 0.074168\tAccuracy: 99.02%\tGradient Norm: 0.000221\n",
      "Epoch: 1 [3776/60000 (6%)]\tLoss: 0.032454\tAccuracy: 99.01%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [3840/60000 (6%)]\tLoss: 0.084405\tAccuracy: 99.00%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [3904/60000 (7%)]\tLoss: 0.036850\tAccuracy: 98.99%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [3968/60000 (7%)]\tLoss: 0.008169\tAccuracy: 99.01%\tGradient Norm: 0.000252\n",
      "Epoch: 1 [4032/60000 (7%)]\tLoss: 0.078318\tAccuracy: 98.97%\tGradient Norm: 0.000241\n",
      "Epoch: 1 [4096/60000 (7%)]\tLoss: 0.013975\tAccuracy: 98.99%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [4160/60000 (7%)]\tLoss: 0.027034\tAccuracy: 98.98%\tGradient Norm: 0.000265\n",
      "Epoch: 1 [4224/60000 (7%)]\tLoss: 0.061275\tAccuracy: 98.95%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [4288/60000 (7%)]\tLoss: 0.008271\tAccuracy: 98.97%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [4352/60000 (7%)]\tLoss: 0.025640\tAccuracy: 98.98%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [4416/60000 (7%)]\tLoss: 0.032656\tAccuracy: 98.97%\tGradient Norm: 0.000249\n",
      "Epoch: 1 [4480/60000 (7%)]\tLoss: 0.025689\tAccuracy: 98.99%\tGradient Norm: 0.000245\n",
      "Epoch: 1 [4544/60000 (8%)]\tLoss: 0.018648\tAccuracy: 99.00%\tGradient Norm: 0.000262\n",
      "Epoch: 1 [4608/60000 (8%)]\tLoss: 0.011030\tAccuracy: 99.02%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [4672/60000 (8%)]\tLoss: 0.032088\tAccuracy: 99.01%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [4736/60000 (8%)]\tLoss: 0.136244\tAccuracy: 98.98%\tGradient Norm: 0.000221\n",
      "Epoch: 1 [4800/60000 (8%)]\tLoss: 0.092178\tAccuracy: 98.93%\tGradient Norm: 0.000261\n",
      "Epoch: 1 [4864/60000 (8%)]\tLoss: 0.025109\tAccuracy: 98.94%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [4928/60000 (8%)]\tLoss: 0.014565\tAccuracy: 98.96%\tGradient Norm: 0.000254\n",
      "Epoch: 1 [4992/60000 (8%)]\tLoss: 0.069185\tAccuracy: 98.93%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [5056/60000 (8%)]\tLoss: 0.082131\tAccuracy: 98.93%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [5120/60000 (9%)]\tLoss: 0.035390\tAccuracy: 98.94%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [5184/60000 (9%)]\tLoss: 0.045574\tAccuracy: 98.93%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [5248/60000 (9%)]\tLoss: 0.095012\tAccuracy: 98.93%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [5312/60000 (9%)]\tLoss: 0.021475\tAccuracy: 98.94%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [5376/60000 (9%)]\tLoss: 0.007906\tAccuracy: 98.95%\tGradient Norm: 0.000266\n",
      "Epoch: 1 [5440/60000 (9%)]\tLoss: 0.073128\tAccuracy: 98.93%\tGradient Norm: 0.000243\n",
      "Epoch: 1 [5504/60000 (9%)]\tLoss: 0.103096\tAccuracy: 98.90%\tGradient Norm: 0.000241\n",
      "Epoch: 1 [5568/60000 (9%)]\tLoss: 0.057768\tAccuracy: 98.88%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [5632/60000 (9%)]\tLoss: 0.080277\tAccuracy: 98.86%\tGradient Norm: 0.000268\n",
      "Epoch: 1 [5696/60000 (9%)]\tLoss: 0.020228\tAccuracy: 98.87%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [5760/60000 (10%)]\tLoss: 0.008180\tAccuracy: 98.88%\tGradient Norm: 0.000265\n",
      "Epoch: 1 [5824/60000 (10%)]\tLoss: 0.009654\tAccuracy: 98.90%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [5888/60000 (10%)]\tLoss: 0.016789\tAccuracy: 98.91%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [5952/60000 (10%)]\tLoss: 0.054448\tAccuracy: 98.89%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [6016/60000 (10%)]\tLoss: 0.020421\tAccuracy: 98.90%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [6080/60000 (10%)]\tLoss: 0.036108\tAccuracy: 98.89%\tGradient Norm: 0.000267\n",
      "Epoch: 1 [6144/60000 (10%)]\tLoss: 0.044966\tAccuracy: 98.87%\tGradient Norm: 0.000270\n",
      "Epoch: 1 [6208/60000 (10%)]\tLoss: 0.022787\tAccuracy: 98.88%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [6272/60000 (10%)]\tLoss: 0.018240\tAccuracy: 98.90%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [6336/60000 (11%)]\tLoss: 0.076750\tAccuracy: 98.88%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [6400/60000 (11%)]\tLoss: 0.023117\tAccuracy: 98.87%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [6464/60000 (11%)]\tLoss: 0.021283\tAccuracy: 98.88%\tGradient Norm: 0.000219\n",
      "Epoch: 1 [6528/60000 (11%)]\tLoss: 0.056655\tAccuracy: 98.86%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [6592/60000 (11%)]\tLoss: 0.014243\tAccuracy: 98.87%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [6656/60000 (11%)]\tLoss: 0.035997\tAccuracy: 98.87%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [6720/60000 (11%)]\tLoss: 0.024785\tAccuracy: 98.88%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [6784/60000 (11%)]\tLoss: 0.088540\tAccuracy: 98.86%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [6848/60000 (11%)]\tLoss: 0.095259\tAccuracy: 98.84%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [6912/60000 (12%)]\tLoss: 0.012345\tAccuracy: 98.85%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [6976/60000 (12%)]\tLoss: 0.065439\tAccuracy: 98.85%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [7040/60000 (12%)]\tLoss: 0.048845\tAccuracy: 98.85%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [7104/60000 (12%)]\tLoss: 0.140185\tAccuracy: 98.83%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [7168/60000 (12%)]\tLoss: 0.036785\tAccuracy: 98.82%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [7232/60000 (12%)]\tLoss: 0.058218\tAccuracy: 98.82%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [7296/60000 (12%)]\tLoss: 0.020692\tAccuracy: 98.83%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [7360/60000 (12%)]\tLoss: 0.026457\tAccuracy: 98.84%\tGradient Norm: 0.000240\n",
      "Epoch: 1 [7424/60000 (12%)]\tLoss: 0.028849\tAccuracy: 98.84%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [7488/60000 (12%)]\tLoss: 0.015927\tAccuracy: 98.85%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [7552/60000 (13%)]\tLoss: 0.101817\tAccuracy: 98.84%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [7616/60000 (13%)]\tLoss: 0.101504\tAccuracy: 98.82%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [7680/60000 (13%)]\tLoss: 0.042471\tAccuracy: 98.81%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [7744/60000 (13%)]\tLoss: 0.004658\tAccuracy: 98.82%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [7808/60000 (13%)]\tLoss: 0.020284\tAccuracy: 98.83%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [7872/60000 (13%)]\tLoss: 0.041803\tAccuracy: 98.83%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [7936/60000 (13%)]\tLoss: 0.099395\tAccuracy: 98.81%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [8000/60000 (13%)]\tLoss: 0.008604\tAccuracy: 98.82%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [8064/60000 (13%)]\tLoss: 0.017065\tAccuracy: 98.83%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [8128/60000 (14%)]\tLoss: 0.037044\tAccuracy: 98.83%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [8192/60000 (14%)]\tLoss: 0.021630\tAccuracy: 98.84%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [8256/60000 (14%)]\tLoss: 0.066951\tAccuracy: 98.83%\tGradient Norm: 0.000300\n",
      "Epoch: 1 [8320/60000 (14%)]\tLoss: 0.062694\tAccuracy: 98.83%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [8384/60000 (14%)]\tLoss: 0.072227\tAccuracy: 98.82%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [8448/60000 (14%)]\tLoss: 0.023843\tAccuracy: 98.81%\tGradient Norm: 0.000247\n",
      "Epoch: 1 [8512/60000 (14%)]\tLoss: 0.006678\tAccuracy: 98.82%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [8576/60000 (14%)]\tLoss: 0.006931\tAccuracy: 98.83%\tGradient Norm: 0.000238\n",
      "Epoch: 1 [8640/60000 (14%)]\tLoss: 0.081660\tAccuracy: 98.81%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [8704/60000 (14%)]\tLoss: 0.004857\tAccuracy: 98.81%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [8768/60000 (15%)]\tLoss: 0.022114\tAccuracy: 98.81%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [8832/60000 (15%)]\tLoss: 0.007647\tAccuracy: 98.82%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [8896/60000 (15%)]\tLoss: 0.022880\tAccuracy: 98.83%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [8960/60000 (15%)]\tLoss: 0.016407\tAccuracy: 98.84%\tGradient Norm: 0.000268\n",
      "Epoch: 1 [9024/60000 (15%)]\tLoss: 0.007074\tAccuracy: 98.84%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [9088/60000 (15%)]\tLoss: 0.018112\tAccuracy: 98.85%\tGradient Norm: 0.000254\n",
      "Epoch: 1 [9152/60000 (15%)]\tLoss: 0.023910\tAccuracy: 98.86%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [9216/60000 (15%)]\tLoss: 0.057800\tAccuracy: 98.85%\tGradient Norm: 0.000239\n",
      "Epoch: 1 [9280/60000 (15%)]\tLoss: 0.027482\tAccuracy: 98.85%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [9344/60000 (16%)]\tLoss: 0.046656\tAccuracy: 98.85%\tGradient Norm: 0.000254\n",
      "Epoch: 1 [9408/60000 (16%)]\tLoss: 0.041843\tAccuracy: 98.85%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [9472/60000 (16%)]\tLoss: 0.014880\tAccuracy: 98.86%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [9536/60000 (16%)]\tLoss: 0.028147\tAccuracy: 98.85%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [9600/60000 (16%)]\tLoss: 0.006908\tAccuracy: 98.86%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [9664/60000 (16%)]\tLoss: 0.015027\tAccuracy: 98.87%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [9728/60000 (16%)]\tLoss: 0.018495\tAccuracy: 98.88%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [9792/60000 (16%)]\tLoss: 0.010678\tAccuracy: 98.88%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [9856/60000 (16%)]\tLoss: 0.018241\tAccuracy: 98.89%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [9920/60000 (17%)]\tLoss: 0.030396\tAccuracy: 98.89%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [9984/60000 (17%)]\tLoss: 0.047575\tAccuracy: 98.88%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [10048/60000 (17%)]\tLoss: 0.112561\tAccuracy: 98.87%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [10112/60000 (17%)]\tLoss: 0.036809\tAccuracy: 98.88%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [10176/60000 (17%)]\tLoss: 0.028769\tAccuracy: 98.89%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [10240/60000 (17%)]\tLoss: 0.045611\tAccuracy: 98.88%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [10304/60000 (17%)]\tLoss: 0.043888\tAccuracy: 98.88%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [10368/60000 (17%)]\tLoss: 0.048957\tAccuracy: 98.88%\tGradient Norm: 0.000240\n",
      "Epoch: 1 [10432/60000 (17%)]\tLoss: 0.012724\tAccuracy: 98.89%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [10496/60000 (17%)]\tLoss: 0.017481\tAccuracy: 98.89%\tGradient Norm: 0.000245\n",
      "Epoch: 1 [10560/60000 (18%)]\tLoss: 0.040144\tAccuracy: 98.90%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [10624/60000 (18%)]\tLoss: 0.086574\tAccuracy: 98.88%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [10688/60000 (18%)]\tLoss: 0.034723\tAccuracy: 98.87%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [10752/60000 (18%)]\tLoss: 0.035869\tAccuracy: 98.87%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [10816/60000 (18%)]\tLoss: 0.031351\tAccuracy: 98.88%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [10880/60000 (18%)]\tLoss: 0.004720\tAccuracy: 98.89%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [10944/60000 (18%)]\tLoss: 0.009835\tAccuracy: 98.89%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [11008/60000 (18%)]\tLoss: 0.015707\tAccuracy: 98.90%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [11072/60000 (18%)]\tLoss: 0.022617\tAccuracy: 98.90%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [11136/60000 (19%)]\tLoss: 0.036522\tAccuracy: 98.90%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [11200/60000 (19%)]\tLoss: 0.051223\tAccuracy: 98.90%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [11264/60000 (19%)]\tLoss: 0.076180\tAccuracy: 98.88%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [11328/60000 (19%)]\tLoss: 0.041400\tAccuracy: 98.88%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [11392/60000 (19%)]\tLoss: 0.015452\tAccuracy: 98.88%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [11456/60000 (19%)]\tLoss: 0.014137\tAccuracy: 98.89%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [11520/60000 (19%)]\tLoss: 0.014165\tAccuracy: 98.90%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [11584/60000 (19%)]\tLoss: 0.012485\tAccuracy: 98.90%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [11648/60000 (19%)]\tLoss: 0.041210\tAccuracy: 98.90%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [11712/60000 (20%)]\tLoss: 0.032653\tAccuracy: 98.90%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [11776/60000 (20%)]\tLoss: 0.010339\tAccuracy: 98.90%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [11840/60000 (20%)]\tLoss: 0.027962\tAccuracy: 98.91%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [11904/60000 (20%)]\tLoss: 0.020889\tAccuracy: 98.91%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [11968/60000 (20%)]\tLoss: 0.067364\tAccuracy: 98.91%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [12032/60000 (20%)]\tLoss: 0.188207\tAccuracy: 98.90%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [12096/60000 (20%)]\tLoss: 0.013797\tAccuracy: 98.91%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [12160/60000 (20%)]\tLoss: 0.052994\tAccuracy: 98.90%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [12224/60000 (20%)]\tLoss: 0.060902\tAccuracy: 98.89%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [12288/60000 (20%)]\tLoss: 0.017978\tAccuracy: 98.89%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [12352/60000 (21%)]\tLoss: 0.016713\tAccuracy: 98.90%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [12416/60000 (21%)]\tLoss: 0.124456\tAccuracy: 98.88%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [12480/60000 (21%)]\tLoss: 0.045359\tAccuracy: 98.88%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [12544/60000 (21%)]\tLoss: 0.024520\tAccuracy: 98.87%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [12608/60000 (21%)]\tLoss: 0.039639\tAccuracy: 98.87%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [12672/60000 (21%)]\tLoss: 0.055195\tAccuracy: 98.86%\tGradient Norm: 0.000251\n",
      "Epoch: 1 [12736/60000 (21%)]\tLoss: 0.006499\tAccuracy: 98.87%\tGradient Norm: 0.000257\n",
      "Epoch: 1 [12800/60000 (21%)]\tLoss: 0.016245\tAccuracy: 98.87%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [12864/60000 (21%)]\tLoss: 0.072901\tAccuracy: 98.87%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [12928/60000 (22%)]\tLoss: 0.044879\tAccuracy: 98.86%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [12992/60000 (22%)]\tLoss: 0.027335\tAccuracy: 98.87%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [13056/60000 (22%)]\tLoss: 0.019266\tAccuracy: 98.87%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [13120/60000 (22%)]\tLoss: 0.007234\tAccuracy: 98.88%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [13184/60000 (22%)]\tLoss: 0.090305\tAccuracy: 98.87%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [13248/60000 (22%)]\tLoss: 0.013292\tAccuracy: 98.87%\tGradient Norm: 0.000244\n",
      "Epoch: 1 [13312/60000 (22%)]\tLoss: 0.015401\tAccuracy: 98.88%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [13376/60000 (22%)]\tLoss: 0.006829\tAccuracy: 98.88%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [13440/60000 (22%)]\tLoss: 0.011553\tAccuracy: 98.89%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [13504/60000 (22%)]\tLoss: 0.032754\tAccuracy: 98.89%\tGradient Norm: 0.000238\n",
      "Epoch: 1 [13568/60000 (23%)]\tLoss: 0.029542\tAccuracy: 98.89%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [13632/60000 (23%)]\tLoss: 0.024306\tAccuracy: 98.89%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [13696/60000 (23%)]\tLoss: 0.039070\tAccuracy: 98.89%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [13760/60000 (23%)]\tLoss: 0.018223\tAccuracy: 98.89%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [13824/60000 (23%)]\tLoss: 0.008225\tAccuracy: 98.90%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [13888/60000 (23%)]\tLoss: 0.064996\tAccuracy: 98.89%\tGradient Norm: 0.000261\n",
      "Epoch: 1 [13952/60000 (23%)]\tLoss: 0.022540\tAccuracy: 98.89%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [14016/60000 (23%)]\tLoss: 0.020032\tAccuracy: 98.89%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [14080/60000 (23%)]\tLoss: 0.009403\tAccuracy: 98.90%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [14144/60000 (24%)]\tLoss: 0.067662\tAccuracy: 98.89%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [14208/60000 (24%)]\tLoss: 0.011129\tAccuracy: 98.89%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [14272/60000 (24%)]\tLoss: 0.020773\tAccuracy: 98.90%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [14336/60000 (24%)]\tLoss: 0.003789\tAccuracy: 98.90%\tGradient Norm: 0.000240\n",
      "Epoch: 1 [14400/60000 (24%)]\tLoss: 0.012837\tAccuracy: 98.91%\tGradient Norm: 0.000219\n",
      "Epoch: 1 [14464/60000 (24%)]\tLoss: 0.030034\tAccuracy: 98.91%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [14528/60000 (24%)]\tLoss: 0.038023\tAccuracy: 98.92%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [14592/60000 (24%)]\tLoss: 0.023561\tAccuracy: 98.92%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [14656/60000 (24%)]\tLoss: 0.065204\tAccuracy: 98.92%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [14720/60000 (25%)]\tLoss: 0.013652\tAccuracy: 98.92%\tGradient Norm: 0.000219\n",
      "Epoch: 1 [14784/60000 (25%)]\tLoss: 0.057079\tAccuracy: 98.92%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [14848/60000 (25%)]\tLoss: 0.089810\tAccuracy: 98.90%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [14912/60000 (25%)]\tLoss: 0.048778\tAccuracy: 98.90%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [14976/60000 (25%)]\tLoss: 0.022964\tAccuracy: 98.90%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [15040/60000 (25%)]\tLoss: 0.057567\tAccuracy: 98.90%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [15104/60000 (25%)]\tLoss: 0.010147\tAccuracy: 98.91%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [15168/60000 (25%)]\tLoss: 0.098062\tAccuracy: 98.90%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [15232/60000 (25%)]\tLoss: 0.032967\tAccuracy: 98.90%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [15296/60000 (25%)]\tLoss: 0.006316\tAccuracy: 98.91%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [15360/60000 (26%)]\tLoss: 0.037776\tAccuracy: 98.90%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [15424/60000 (26%)]\tLoss: 0.076746\tAccuracy: 98.90%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [15488/60000 (26%)]\tLoss: 0.044913\tAccuracy: 98.90%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [15552/60000 (26%)]\tLoss: 0.014953\tAccuracy: 98.90%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [15616/60000 (26%)]\tLoss: 0.022092\tAccuracy: 98.91%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [15680/60000 (26%)]\tLoss: 0.035528\tAccuracy: 98.91%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [15744/60000 (26%)]\tLoss: 0.100956\tAccuracy: 98.91%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [15808/60000 (26%)]\tLoss: 0.018413\tAccuracy: 98.91%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [15872/60000 (26%)]\tLoss: 0.027294\tAccuracy: 98.91%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [15936/60000 (27%)]\tLoss: 0.077825\tAccuracy: 98.91%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [16000/60000 (27%)]\tLoss: 0.004391\tAccuracy: 98.91%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [16064/60000 (27%)]\tLoss: 0.028410\tAccuracy: 98.91%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [16128/60000 (27%)]\tLoss: 0.127173\tAccuracy: 98.90%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [16192/60000 (27%)]\tLoss: 0.034289\tAccuracy: 98.90%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [16256/60000 (27%)]\tLoss: 0.010195\tAccuracy: 98.90%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [16320/60000 (27%)]\tLoss: 0.034830\tAccuracy: 98.90%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [16384/60000 (27%)]\tLoss: 0.032252\tAccuracy: 98.90%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [16448/60000 (27%)]\tLoss: 0.017104\tAccuracy: 98.90%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [16512/60000 (28%)]\tLoss: 0.122891\tAccuracy: 98.89%\tGradient Norm: 0.000249\n",
      "Epoch: 1 [16576/60000 (28%)]\tLoss: 0.028025\tAccuracy: 98.89%\tGradient Norm: 0.000253\n",
      "Epoch: 1 [16640/60000 (28%)]\tLoss: 0.079023\tAccuracy: 98.88%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [16704/60000 (28%)]\tLoss: 0.026086\tAccuracy: 98.88%\tGradient Norm: 0.000244\n",
      "Epoch: 1 [16768/60000 (28%)]\tLoss: 0.028915\tAccuracy: 98.89%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [16832/60000 (28%)]\tLoss: 0.045842\tAccuracy: 98.89%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [16896/60000 (28%)]\tLoss: 0.008978\tAccuracy: 98.89%\tGradient Norm: 0.000257\n",
      "Epoch: 1 [16960/60000 (28%)]\tLoss: 0.085140\tAccuracy: 98.88%\tGradient Norm: 0.000247\n",
      "Epoch: 1 [17024/60000 (28%)]\tLoss: 0.018771\tAccuracy: 98.88%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [17088/60000 (28%)]\tLoss: 0.042942\tAccuracy: 98.88%\tGradient Norm: 0.000248\n",
      "Epoch: 1 [17152/60000 (29%)]\tLoss: 0.012050\tAccuracy: 98.88%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [17216/60000 (29%)]\tLoss: 0.048125\tAccuracy: 98.88%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [17280/60000 (29%)]\tLoss: 0.023641\tAccuracy: 98.88%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [17344/60000 (29%)]\tLoss: 0.019107\tAccuracy: 98.89%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [17408/60000 (29%)]\tLoss: 0.003596\tAccuracy: 98.89%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [17472/60000 (29%)]\tLoss: 0.046380\tAccuracy: 98.89%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [17536/60000 (29%)]\tLoss: 0.045287\tAccuracy: 98.88%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [17600/60000 (29%)]\tLoss: 0.061899\tAccuracy: 98.88%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [17664/60000 (29%)]\tLoss: 0.058249\tAccuracy: 98.88%\tGradient Norm: 0.000221\n",
      "Epoch: 1 [17728/60000 (30%)]\tLoss: 0.056341\tAccuracy: 98.88%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [17792/60000 (30%)]\tLoss: 0.038409\tAccuracy: 98.87%\tGradient Norm: 0.000264\n",
      "Epoch: 1 [17856/60000 (30%)]\tLoss: 0.009237\tAccuracy: 98.88%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [17920/60000 (30%)]\tLoss: 0.052410\tAccuracy: 98.88%\tGradient Norm: 0.000249\n",
      "Epoch: 1 [17984/60000 (30%)]\tLoss: 0.058002\tAccuracy: 98.88%\tGradient Norm: 0.000258\n",
      "Epoch: 1 [18048/60000 (30%)]\tLoss: 0.032264\tAccuracy: 98.88%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [18112/60000 (30%)]\tLoss: 0.019128\tAccuracy: 98.88%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [18176/60000 (30%)]\tLoss: 0.005257\tAccuracy: 98.89%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [18240/60000 (30%)]\tLoss: 0.118491\tAccuracy: 98.87%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [18304/60000 (30%)]\tLoss: 0.008592\tAccuracy: 98.88%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [18368/60000 (31%)]\tLoss: 0.031811\tAccuracy: 98.88%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [18432/60000 (31%)]\tLoss: 0.033530\tAccuracy: 98.88%\tGradient Norm: 0.000251\n",
      "Epoch: 1 [18496/60000 (31%)]\tLoss: 0.018500\tAccuracy: 98.88%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [18560/60000 (31%)]\tLoss: 0.039259\tAccuracy: 98.88%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [18624/60000 (31%)]\tLoss: 0.123121\tAccuracy: 98.88%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [18688/60000 (31%)]\tLoss: 0.060492\tAccuracy: 98.87%\tGradient Norm: 0.000252\n",
      "Epoch: 1 [18752/60000 (31%)]\tLoss: 0.016122\tAccuracy: 98.88%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [18816/60000 (31%)]\tLoss: 0.029897\tAccuracy: 98.88%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [18880/60000 (31%)]\tLoss: 0.137738\tAccuracy: 98.87%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [18944/60000 (32%)]\tLoss: 0.012594\tAccuracy: 98.87%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [19008/60000 (32%)]\tLoss: 0.011911\tAccuracy: 98.88%\tGradient Norm: 0.000247\n",
      "Epoch: 1 [19072/60000 (32%)]\tLoss: 0.072401\tAccuracy: 98.88%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [19136/60000 (32%)]\tLoss: 0.007957\tAccuracy: 98.88%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [19200/60000 (32%)]\tLoss: 0.036043\tAccuracy: 98.88%\tGradient Norm: 0.000238\n",
      "Epoch: 1 [19264/60000 (32%)]\tLoss: 0.008138\tAccuracy: 98.88%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [19328/60000 (32%)]\tLoss: 0.009958\tAccuracy: 98.89%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [19392/60000 (32%)]\tLoss: 0.022120\tAccuracy: 98.88%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [19456/60000 (32%)]\tLoss: 0.104181\tAccuracy: 98.88%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [19520/60000 (33%)]\tLoss: 0.009111\tAccuracy: 98.88%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [19584/60000 (33%)]\tLoss: 0.009911\tAccuracy: 98.89%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [19648/60000 (33%)]\tLoss: 0.015257\tAccuracy: 98.89%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [19712/60000 (33%)]\tLoss: 0.097465\tAccuracy: 98.88%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [19776/60000 (33%)]\tLoss: 0.011851\tAccuracy: 98.89%\tGradient Norm: 0.000241\n",
      "Epoch: 1 [19840/60000 (33%)]\tLoss: 0.004143\tAccuracy: 98.89%\tGradient Norm: 0.000245\n",
      "Epoch: 1 [19904/60000 (33%)]\tLoss: 0.024547\tAccuracy: 98.89%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [19968/60000 (33%)]\tLoss: 0.026939\tAccuracy: 98.90%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [20032/60000 (33%)]\tLoss: 0.009358\tAccuracy: 98.90%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [20096/60000 (33%)]\tLoss: 0.026430\tAccuracy: 98.90%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [20160/60000 (34%)]\tLoss: 0.032092\tAccuracy: 98.90%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [20224/60000 (34%)]\tLoss: 0.057439\tAccuracy: 98.90%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [20288/60000 (34%)]\tLoss: 0.044387\tAccuracy: 98.90%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [20352/60000 (34%)]\tLoss: 0.022710\tAccuracy: 98.90%\tGradient Norm: 0.000240\n",
      "Epoch: 1 [20416/60000 (34%)]\tLoss: 0.017575\tAccuracy: 98.91%\tGradient Norm: 0.000238\n",
      "Epoch: 1 [20480/60000 (34%)]\tLoss: 0.008736\tAccuracy: 98.91%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [20544/60000 (34%)]\tLoss: 0.044805\tAccuracy: 98.91%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [20608/60000 (34%)]\tLoss: 0.031350\tAccuracy: 98.91%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [20672/60000 (34%)]\tLoss: 0.005875\tAccuracy: 98.91%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [20736/60000 (35%)]\tLoss: 0.024055\tAccuracy: 98.91%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [20800/60000 (35%)]\tLoss: 0.031331\tAccuracy: 98.91%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [20864/60000 (35%)]\tLoss: 0.046959\tAccuracy: 98.91%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [20928/60000 (35%)]\tLoss: 0.025643\tAccuracy: 98.91%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [20992/60000 (35%)]\tLoss: 0.033256\tAccuracy: 98.90%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [21056/60000 (35%)]\tLoss: 0.091696\tAccuracy: 98.90%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [21120/60000 (35%)]\tLoss: 0.073038\tAccuracy: 98.90%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [21184/60000 (35%)]\tLoss: 0.023086\tAccuracy: 98.90%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [21248/60000 (35%)]\tLoss: 0.006525\tAccuracy: 98.90%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [21312/60000 (36%)]\tLoss: 0.051434\tAccuracy: 98.90%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [21376/60000 (36%)]\tLoss: 0.061010\tAccuracy: 98.90%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [21440/60000 (36%)]\tLoss: 0.022433\tAccuracy: 98.90%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [21504/60000 (36%)]\tLoss: 0.133679\tAccuracy: 98.89%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [21568/60000 (36%)]\tLoss: 0.009669\tAccuracy: 98.90%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [21632/60000 (36%)]\tLoss: 0.015398\tAccuracy: 98.90%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [21696/60000 (36%)]\tLoss: 0.031987\tAccuracy: 98.90%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [21760/60000 (36%)]\tLoss: 0.027736\tAccuracy: 98.90%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [21824/60000 (36%)]\tLoss: 0.034696\tAccuracy: 98.90%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [21888/60000 (36%)]\tLoss: 0.008584\tAccuracy: 98.90%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [21952/60000 (37%)]\tLoss: 0.021271\tAccuracy: 98.91%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [22016/60000 (37%)]\tLoss: 0.038412\tAccuracy: 98.90%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [22080/60000 (37%)]\tLoss: 0.016593\tAccuracy: 98.91%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [22144/60000 (37%)]\tLoss: 0.011585\tAccuracy: 98.91%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [22208/60000 (37%)]\tLoss: 0.049094\tAccuracy: 98.91%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [22272/60000 (37%)]\tLoss: 0.036842\tAccuracy: 98.91%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [22336/60000 (37%)]\tLoss: 0.014851\tAccuracy: 98.91%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [22400/60000 (37%)]\tLoss: 0.044017\tAccuracy: 98.91%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [22464/60000 (37%)]\tLoss: 0.078658\tAccuracy: 98.91%\tGradient Norm: 0.000221\n",
      "Epoch: 1 [22528/60000 (38%)]\tLoss: 0.010071\tAccuracy: 98.91%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [22592/60000 (38%)]\tLoss: 0.020856\tAccuracy: 98.91%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [22656/60000 (38%)]\tLoss: 0.059456\tAccuracy: 98.91%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [22720/60000 (38%)]\tLoss: 0.301569\tAccuracy: 98.90%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [22784/60000 (38%)]\tLoss: 0.007544\tAccuracy: 98.91%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [22848/60000 (38%)]\tLoss: 0.038084\tAccuracy: 98.90%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [22912/60000 (38%)]\tLoss: 0.032340\tAccuracy: 98.91%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [22976/60000 (38%)]\tLoss: 0.017253\tAccuracy: 98.91%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [23040/60000 (38%)]\tLoss: 0.099539\tAccuracy: 98.91%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [23104/60000 (38%)]\tLoss: 0.021808\tAccuracy: 98.91%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [23168/60000 (39%)]\tLoss: 0.025721\tAccuracy: 98.92%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [23232/60000 (39%)]\tLoss: 0.031335\tAccuracy: 98.92%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [23296/60000 (39%)]\tLoss: 0.125509\tAccuracy: 98.91%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [23360/60000 (39%)]\tLoss: 0.087747\tAccuracy: 98.91%\tGradient Norm: 0.000243\n",
      "Epoch: 1 [23424/60000 (39%)]\tLoss: 0.021028\tAccuracy: 98.91%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [23488/60000 (39%)]\tLoss: 0.008886\tAccuracy: 98.92%\tGradient Norm: 0.000221\n",
      "Epoch: 1 [23552/60000 (39%)]\tLoss: 0.025565\tAccuracy: 98.92%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [23616/60000 (39%)]\tLoss: 0.015865\tAccuracy: 98.92%\tGradient Norm: 0.000271\n",
      "Epoch: 1 [23680/60000 (39%)]\tLoss: 0.068190\tAccuracy: 98.92%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [23744/60000 (40%)]\tLoss: 0.047006\tAccuracy: 98.92%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [23808/60000 (40%)]\tLoss: 0.011313\tAccuracy: 98.92%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [23872/60000 (40%)]\tLoss: 0.036118\tAccuracy: 98.92%\tGradient Norm: 0.000247\n",
      "Epoch: 1 [23936/60000 (40%)]\tLoss: 0.015645\tAccuracy: 98.92%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [24000/60000 (40%)]\tLoss: 0.004308\tAccuracy: 98.93%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [24064/60000 (40%)]\tLoss: 0.005517\tAccuracy: 98.93%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [24128/60000 (40%)]\tLoss: 0.011717\tAccuracy: 98.93%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [24192/60000 (40%)]\tLoss: 0.008138\tAccuracy: 98.94%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [24256/60000 (40%)]\tLoss: 0.022920\tAccuracy: 98.94%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [24320/60000 (41%)]\tLoss: 0.014183\tAccuracy: 98.94%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [24384/60000 (41%)]\tLoss: 0.084989\tAccuracy: 98.94%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [24448/60000 (41%)]\tLoss: 0.016527\tAccuracy: 98.94%\tGradient Norm: 0.000261\n",
      "Epoch: 1 [24512/60000 (41%)]\tLoss: 0.011381\tAccuracy: 98.94%\tGradient Norm: 0.000239\n",
      "Epoch: 1 [24576/60000 (41%)]\tLoss: 0.012136\tAccuracy: 98.94%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [24640/60000 (41%)]\tLoss: 0.013576\tAccuracy: 98.95%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [24704/60000 (41%)]\tLoss: 0.018259\tAccuracy: 98.95%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [24768/60000 (41%)]\tLoss: 0.009697\tAccuracy: 98.95%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [24832/60000 (41%)]\tLoss: 0.005701\tAccuracy: 98.96%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [24896/60000 (41%)]\tLoss: 0.014998\tAccuracy: 98.96%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [24960/60000 (42%)]\tLoss: 0.013448\tAccuracy: 98.96%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [25024/60000 (42%)]\tLoss: 0.011859\tAccuracy: 98.96%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [25088/60000 (42%)]\tLoss: 0.008387\tAccuracy: 98.97%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [25152/60000 (42%)]\tLoss: 0.025949\tAccuracy: 98.97%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [25216/60000 (42%)]\tLoss: 0.010154\tAccuracy: 98.97%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [25280/60000 (42%)]\tLoss: 0.062738\tAccuracy: 98.97%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [25344/60000 (42%)]\tLoss: 0.020250\tAccuracy: 98.97%\tGradient Norm: 0.000261\n",
      "Epoch: 1 [25408/60000 (42%)]\tLoss: 0.013217\tAccuracy: 98.97%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [25472/60000 (42%)]\tLoss: 0.034300\tAccuracy: 98.97%\tGradient Norm: 0.000248\n",
      "Epoch: 1 [25536/60000 (43%)]\tLoss: 0.026549\tAccuracy: 98.97%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [25600/60000 (43%)]\tLoss: 0.021954\tAccuracy: 98.98%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [25664/60000 (43%)]\tLoss: 0.051190\tAccuracy: 98.97%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [25728/60000 (43%)]\tLoss: 0.023968\tAccuracy: 98.98%\tGradient Norm: 0.000255\n",
      "Epoch: 1 [25792/60000 (43%)]\tLoss: 0.015167\tAccuracy: 98.98%\tGradient Norm: 0.000266\n",
      "Epoch: 1 [25856/60000 (43%)]\tLoss: 0.013560\tAccuracy: 98.98%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [25920/60000 (43%)]\tLoss: 0.057755\tAccuracy: 98.98%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [25984/60000 (43%)]\tLoss: 0.048472\tAccuracy: 98.97%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [26048/60000 (43%)]\tLoss: 0.052091\tAccuracy: 98.97%\tGradient Norm: 0.000249\n",
      "Epoch: 1 [26112/60000 (43%)]\tLoss: 0.012089\tAccuracy: 98.98%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [26176/60000 (44%)]\tLoss: 0.078006\tAccuracy: 98.97%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [26240/60000 (44%)]\tLoss: 0.020995\tAccuracy: 98.97%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [26304/60000 (44%)]\tLoss: 0.028342\tAccuracy: 98.97%\tGradient Norm: 0.000246\n",
      "Epoch: 1 [26368/60000 (44%)]\tLoss: 0.014137\tAccuracy: 98.97%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [26432/60000 (44%)]\tLoss: 0.024484\tAccuracy: 98.97%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [26496/60000 (44%)]\tLoss: 0.023352\tAccuracy: 98.97%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [26560/60000 (44%)]\tLoss: 0.005942\tAccuracy: 98.97%\tGradient Norm: 0.000239\n",
      "Epoch: 1 [26624/60000 (44%)]\tLoss: 0.006199\tAccuracy: 98.98%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [26688/60000 (44%)]\tLoss: 0.018604\tAccuracy: 98.98%\tGradient Norm: 0.000273\n",
      "Epoch: 1 [26752/60000 (45%)]\tLoss: 0.019072\tAccuracy: 98.98%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [26816/60000 (45%)]\tLoss: 0.069533\tAccuracy: 98.98%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [26880/60000 (45%)]\tLoss: 0.020599\tAccuracy: 98.98%\tGradient Norm: 0.000304\n",
      "Epoch: 1 [26944/60000 (45%)]\tLoss: 0.028937\tAccuracy: 98.99%\tGradient Norm: 0.000241\n",
      "Epoch: 1 [27008/60000 (45%)]\tLoss: 0.049475\tAccuracy: 98.98%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [27072/60000 (45%)]\tLoss: 0.016164\tAccuracy: 98.98%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [27136/60000 (45%)]\tLoss: 0.018060\tAccuracy: 98.99%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [27200/60000 (45%)]\tLoss: 0.006920\tAccuracy: 98.99%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [27264/60000 (45%)]\tLoss: 0.004959\tAccuracy: 98.99%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [27328/60000 (46%)]\tLoss: 0.019172\tAccuracy: 98.99%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [27392/60000 (46%)]\tLoss: 0.049631\tAccuracy: 98.99%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [27456/60000 (46%)]\tLoss: 0.035757\tAccuracy: 98.99%\tGradient Norm: 0.000243\n",
      "Epoch: 1 [27520/60000 (46%)]\tLoss: 0.029135\tAccuracy: 98.99%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [27584/60000 (46%)]\tLoss: 0.219858\tAccuracy: 98.98%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [27648/60000 (46%)]\tLoss: 0.011934\tAccuracy: 98.98%\tGradient Norm: 0.000265\n",
      "Epoch: 1 [27712/60000 (46%)]\tLoss: 0.017293\tAccuracy: 98.98%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [27776/60000 (46%)]\tLoss: 0.012422\tAccuracy: 98.98%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [27840/60000 (46%)]\tLoss: 0.097859\tAccuracy: 98.98%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [27904/60000 (46%)]\tLoss: 0.054643\tAccuracy: 98.98%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [27968/60000 (47%)]\tLoss: 0.021698\tAccuracy: 98.98%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [28032/60000 (47%)]\tLoss: 0.015995\tAccuracy: 98.98%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [28096/60000 (47%)]\tLoss: 0.020031\tAccuracy: 98.98%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [28160/60000 (47%)]\tLoss: 0.016431\tAccuracy: 98.99%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [28224/60000 (47%)]\tLoss: 0.011827\tAccuracy: 98.99%\tGradient Norm: 0.000221\n",
      "Epoch: 1 [28288/60000 (47%)]\tLoss: 0.009635\tAccuracy: 98.99%\tGradient Norm: 0.000253\n",
      "Epoch: 1 [28352/60000 (47%)]\tLoss: 0.005027\tAccuracy: 98.99%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [28416/60000 (47%)]\tLoss: 0.080310\tAccuracy: 98.99%\tGradient Norm: 0.000240\n",
      "Epoch: 1 [28480/60000 (47%)]\tLoss: 0.048916\tAccuracy: 98.99%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [28544/60000 (48%)]\tLoss: 0.013923\tAccuracy: 98.99%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [28608/60000 (48%)]\tLoss: 0.144901\tAccuracy: 98.99%\tGradient Norm: 0.000240\n",
      "Epoch: 1 [28672/60000 (48%)]\tLoss: 0.044368\tAccuracy: 98.99%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [28736/60000 (48%)]\tLoss: 0.009474\tAccuracy: 98.99%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [28800/60000 (48%)]\tLoss: 0.020152\tAccuracy: 98.99%\tGradient Norm: 0.000250\n",
      "Epoch: 1 [28864/60000 (48%)]\tLoss: 0.026185\tAccuracy: 98.99%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [28928/60000 (48%)]\tLoss: 0.004442\tAccuracy: 99.00%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [28992/60000 (48%)]\tLoss: 0.033875\tAccuracy: 99.00%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [29056/60000 (48%)]\tLoss: 0.029863\tAccuracy: 99.00%\tGradient Norm: 0.000253\n",
      "Epoch: 1 [29120/60000 (49%)]\tLoss: 0.006260\tAccuracy: 99.00%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [29184/60000 (49%)]\tLoss: 0.021685\tAccuracy: 99.00%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [29248/60000 (49%)]\tLoss: 0.020268\tAccuracy: 99.00%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [29312/60000 (49%)]\tLoss: 0.097757\tAccuracy: 98.99%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [29376/60000 (49%)]\tLoss: 0.025225\tAccuracy: 98.99%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [29440/60000 (49%)]\tLoss: 0.032036\tAccuracy: 98.99%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [29504/60000 (49%)]\tLoss: 0.015847\tAccuracy: 99.00%\tGradient Norm: 0.000239\n",
      "Epoch: 1 [29568/60000 (49%)]\tLoss: 0.008810\tAccuracy: 99.00%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [29632/60000 (49%)]\tLoss: 0.032523\tAccuracy: 99.00%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [29696/60000 (49%)]\tLoss: 0.017787\tAccuracy: 99.00%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [29760/60000 (50%)]\tLoss: 0.011973\tAccuracy: 99.00%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [29824/60000 (50%)]\tLoss: 0.009045\tAccuracy: 99.00%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [29888/60000 (50%)]\tLoss: 0.040971\tAccuracy: 99.00%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [29952/60000 (50%)]\tLoss: 0.070432\tAccuracy: 99.00%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [30016/60000 (50%)]\tLoss: 0.004231\tAccuracy: 99.00%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [30080/60000 (50%)]\tLoss: 0.016334\tAccuracy: 99.00%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [30144/60000 (50%)]\tLoss: 0.019362\tAccuracy: 99.00%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [30208/60000 (50%)]\tLoss: 0.017113\tAccuracy: 99.01%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [30272/60000 (50%)]\tLoss: 0.006957\tAccuracy: 99.01%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [30336/60000 (51%)]\tLoss: 0.023853\tAccuracy: 99.01%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [30400/60000 (51%)]\tLoss: 0.042992\tAccuracy: 99.01%\tGradient Norm: 0.000245\n",
      "Epoch: 1 [30464/60000 (51%)]\tLoss: 0.012751\tAccuracy: 99.01%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [30528/60000 (51%)]\tLoss: 0.017351\tAccuracy: 99.01%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [30592/60000 (51%)]\tLoss: 0.017863\tAccuracy: 99.01%\tGradient Norm: 0.000247\n",
      "Epoch: 1 [30656/60000 (51%)]\tLoss: 0.041655\tAccuracy: 99.01%\tGradient Norm: 0.000242\n",
      "Epoch: 1 [30720/60000 (51%)]\tLoss: 0.004064\tAccuracy: 99.01%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [30784/60000 (51%)]\tLoss: 0.007196\tAccuracy: 99.01%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [30848/60000 (51%)]\tLoss: 0.004436\tAccuracy: 99.02%\tGradient Norm: 0.000243\n",
      "Epoch: 1 [30912/60000 (51%)]\tLoss: 0.070479\tAccuracy: 99.01%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [30976/60000 (52%)]\tLoss: 0.078640\tAccuracy: 99.01%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [31040/60000 (52%)]\tLoss: 0.006646\tAccuracy: 99.01%\tGradient Norm: 0.000244\n",
      "Epoch: 1 [31104/60000 (52%)]\tLoss: 0.008778\tAccuracy: 99.01%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [31168/60000 (52%)]\tLoss: 0.024815\tAccuracy: 99.01%\tGradient Norm: 0.000241\n",
      "Epoch: 1 [31232/60000 (52%)]\tLoss: 0.022078\tAccuracy: 99.02%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [31296/60000 (52%)]\tLoss: 0.006339\tAccuracy: 99.02%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [31360/60000 (52%)]\tLoss: 0.100497\tAccuracy: 99.02%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [31424/60000 (52%)]\tLoss: 0.036013\tAccuracy: 99.02%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [31488/60000 (52%)]\tLoss: 0.013517\tAccuracy: 99.02%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [31552/60000 (53%)]\tLoss: 0.008151\tAccuracy: 99.02%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [31616/60000 (53%)]\tLoss: 0.009336\tAccuracy: 99.02%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [31680/60000 (53%)]\tLoss: 0.049756\tAccuracy: 99.02%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [31744/60000 (53%)]\tLoss: 0.007080\tAccuracy: 99.02%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [31808/60000 (53%)]\tLoss: 0.042811\tAccuracy: 99.02%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [31872/60000 (53%)]\tLoss: 0.064011\tAccuracy: 99.02%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [31936/60000 (53%)]\tLoss: 0.015098\tAccuracy: 99.02%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [32000/60000 (53%)]\tLoss: 0.009402\tAccuracy: 99.02%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [32064/60000 (53%)]\tLoss: 0.018646\tAccuracy: 99.02%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [32128/60000 (54%)]\tLoss: 0.017183\tAccuracy: 99.02%\tGradient Norm: 0.000255\n",
      "Epoch: 1 [32192/60000 (54%)]\tLoss: 0.064060\tAccuracy: 99.02%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [32256/60000 (54%)]\tLoss: 0.014962\tAccuracy: 99.02%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [32320/60000 (54%)]\tLoss: 0.021808\tAccuracy: 99.02%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [32384/60000 (54%)]\tLoss: 0.036864\tAccuracy: 99.02%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [32448/60000 (54%)]\tLoss: 0.021506\tAccuracy: 99.02%\tGradient Norm: 0.000219\n",
      "Epoch: 1 [32512/60000 (54%)]\tLoss: 0.007082\tAccuracy: 99.03%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [32576/60000 (54%)]\tLoss: 0.013621\tAccuracy: 99.03%\tGradient Norm: 0.000174\n",
      "Epoch: 1 [32640/60000 (54%)]\tLoss: 0.032022\tAccuracy: 99.03%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [32704/60000 (54%)]\tLoss: 0.106038\tAccuracy: 99.02%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [32768/60000 (55%)]\tLoss: 0.045914\tAccuracy: 99.02%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [32832/60000 (55%)]\tLoss: 0.090406\tAccuracy: 99.02%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [32896/60000 (55%)]\tLoss: 0.091683\tAccuracy: 99.01%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [32960/60000 (55%)]\tLoss: 0.047886\tAccuracy: 99.01%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [33024/60000 (55%)]\tLoss: 0.082538\tAccuracy: 99.00%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [33088/60000 (55%)]\tLoss: 0.038144\tAccuracy: 99.00%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [33152/60000 (55%)]\tLoss: 0.003489\tAccuracy: 99.00%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [33216/60000 (55%)]\tLoss: 0.053540\tAccuracy: 99.00%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [33280/60000 (55%)]\tLoss: 0.111397\tAccuracy: 99.00%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [33344/60000 (56%)]\tLoss: 0.009862\tAccuracy: 99.00%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [33408/60000 (56%)]\tLoss: 0.005435\tAccuracy: 99.00%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [33472/60000 (56%)]\tLoss: 0.033399\tAccuracy: 99.00%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [33536/60000 (56%)]\tLoss: 0.024163\tAccuracy: 99.00%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [33600/60000 (56%)]\tLoss: 0.007906\tAccuracy: 99.00%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [33664/60000 (56%)]\tLoss: 0.019572\tAccuracy: 99.01%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [33728/60000 (56%)]\tLoss: 0.033433\tAccuracy: 99.01%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [33792/60000 (56%)]\tLoss: 0.032822\tAccuracy: 99.01%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [33856/60000 (56%)]\tLoss: 0.084837\tAccuracy: 99.01%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [33920/60000 (57%)]\tLoss: 0.006979\tAccuracy: 99.01%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [33984/60000 (57%)]\tLoss: 0.021361\tAccuracy: 99.01%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [34048/60000 (57%)]\tLoss: 0.007482\tAccuracy: 99.01%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [34112/60000 (57%)]\tLoss: 0.009274\tAccuracy: 99.01%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [34176/60000 (57%)]\tLoss: 0.144121\tAccuracy: 99.01%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [34240/60000 (57%)]\tLoss: 0.101505\tAccuracy: 99.00%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [34304/60000 (57%)]\tLoss: 0.031276\tAccuracy: 99.00%\tGradient Norm: 0.000168\n",
      "Epoch: 1 [34368/60000 (57%)]\tLoss: 0.087613\tAccuracy: 99.00%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [34432/60000 (57%)]\tLoss: 0.007703\tAccuracy: 99.00%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [34496/60000 (57%)]\tLoss: 0.018665\tAccuracy: 99.00%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [34560/60000 (58%)]\tLoss: 0.056674\tAccuracy: 99.00%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [34624/60000 (58%)]\tLoss: 0.003307\tAccuracy: 99.00%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [34688/60000 (58%)]\tLoss: 0.004833\tAccuracy: 99.00%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [34752/60000 (58%)]\tLoss: 0.037075\tAccuracy: 99.00%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [34816/60000 (58%)]\tLoss: 0.019073\tAccuracy: 99.01%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [34880/60000 (58%)]\tLoss: 0.037524\tAccuracy: 99.00%\tGradient Norm: 0.000165\n",
      "Epoch: 1 [34944/60000 (58%)]\tLoss: 0.038351\tAccuracy: 99.00%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [35008/60000 (58%)]\tLoss: 0.016396\tAccuracy: 99.00%\tGradient Norm: 0.000172\n",
      "Epoch: 1 [35072/60000 (58%)]\tLoss: 0.007457\tAccuracy: 99.01%\tGradient Norm: 0.000165\n",
      "Epoch: 1 [35136/60000 (59%)]\tLoss: 0.010500\tAccuracy: 99.01%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [35200/60000 (59%)]\tLoss: 0.061902\tAccuracy: 99.01%\tGradient Norm: 0.000175\n",
      "Epoch: 1 [35264/60000 (59%)]\tLoss: 0.032764\tAccuracy: 99.01%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [35328/60000 (59%)]\tLoss: 0.018736\tAccuracy: 99.01%\tGradient Norm: 0.000177\n",
      "Epoch: 1 [35392/60000 (59%)]\tLoss: 0.010082\tAccuracy: 99.01%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [35456/60000 (59%)]\tLoss: 0.031268\tAccuracy: 99.01%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [35520/60000 (59%)]\tLoss: 0.017634\tAccuracy: 99.01%\tGradient Norm: 0.000172\n",
      "Epoch: 1 [35584/60000 (59%)]\tLoss: 0.041823\tAccuracy: 99.01%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [35648/60000 (59%)]\tLoss: 0.014590\tAccuracy: 99.01%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [35712/60000 (59%)]\tLoss: 0.011851\tAccuracy: 99.02%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [35776/60000 (60%)]\tLoss: 0.013407\tAccuracy: 99.02%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [35840/60000 (60%)]\tLoss: 0.099931\tAccuracy: 99.02%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [35904/60000 (60%)]\tLoss: 0.008355\tAccuracy: 99.02%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [35968/60000 (60%)]\tLoss: 0.010103\tAccuracy: 99.02%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [36032/60000 (60%)]\tLoss: 0.038343\tAccuracy: 99.02%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [36096/60000 (60%)]\tLoss: 0.004459\tAccuracy: 99.02%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [36160/60000 (60%)]\tLoss: 0.133189\tAccuracy: 99.02%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [36224/60000 (60%)]\tLoss: 0.038102\tAccuracy: 99.02%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [36288/60000 (60%)]\tLoss: 0.028563\tAccuracy: 99.02%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [36352/60000 (61%)]\tLoss: 0.136235\tAccuracy: 99.01%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [36416/60000 (61%)]\tLoss: 0.038076\tAccuracy: 99.01%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [36480/60000 (61%)]\tLoss: 0.056748\tAccuracy: 99.01%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [36544/60000 (61%)]\tLoss: 0.016225\tAccuracy: 99.01%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [36608/60000 (61%)]\tLoss: 0.042122\tAccuracy: 99.01%\tGradient Norm: 0.000219\n",
      "Epoch: 1 [36672/60000 (61%)]\tLoss: 0.013419\tAccuracy: 99.01%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [36736/60000 (61%)]\tLoss: 0.037700\tAccuracy: 99.01%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [36800/60000 (61%)]\tLoss: 0.008016\tAccuracy: 99.02%\tGradient Norm: 0.000219\n",
      "Epoch: 1 [36864/60000 (61%)]\tLoss: 0.004258\tAccuracy: 99.02%\tGradient Norm: 0.000179\n",
      "Epoch: 1 [36928/60000 (62%)]\tLoss: 0.018124\tAccuracy: 99.02%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [36992/60000 (62%)]\tLoss: 0.008897\tAccuracy: 99.02%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [37056/60000 (62%)]\tLoss: 0.006008\tAccuracy: 99.02%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [37120/60000 (62%)]\tLoss: 0.003597\tAccuracy: 99.02%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [37184/60000 (62%)]\tLoss: 0.017418\tAccuracy: 99.02%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [37248/60000 (62%)]\tLoss: 0.092682\tAccuracy: 99.02%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [37312/60000 (62%)]\tLoss: 0.003705\tAccuracy: 99.02%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [37376/60000 (62%)]\tLoss: 0.038735\tAccuracy: 99.03%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [37440/60000 (62%)]\tLoss: 0.012079\tAccuracy: 99.03%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [37504/60000 (62%)]\tLoss: 0.009983\tAccuracy: 99.03%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [37568/60000 (63%)]\tLoss: 0.031301\tAccuracy: 99.03%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [37632/60000 (63%)]\tLoss: 0.012418\tAccuracy: 99.03%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [37696/60000 (63%)]\tLoss: 0.055246\tAccuracy: 99.03%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [37760/60000 (63%)]\tLoss: 0.021629\tAccuracy: 99.03%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [37824/60000 (63%)]\tLoss: 0.086640\tAccuracy: 99.03%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [37888/60000 (63%)]\tLoss: 0.006821\tAccuracy: 99.03%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [37952/60000 (63%)]\tLoss: 0.049533\tAccuracy: 99.03%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [38016/60000 (63%)]\tLoss: 0.014609\tAccuracy: 99.03%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [38080/60000 (63%)]\tLoss: 0.006888\tAccuracy: 99.03%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [38144/60000 (64%)]\tLoss: 0.021891\tAccuracy: 99.03%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [38208/60000 (64%)]\tLoss: 0.032047\tAccuracy: 99.04%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [38272/60000 (64%)]\tLoss: 0.026395\tAccuracy: 99.03%\tGradient Norm: 0.000158\n",
      "Epoch: 1 [38336/60000 (64%)]\tLoss: 0.045718\tAccuracy: 99.03%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [38400/60000 (64%)]\tLoss: 0.035969\tAccuracy: 99.03%\tGradient Norm: 0.000178\n",
      "Epoch: 1 [38464/60000 (64%)]\tLoss: 0.031909\tAccuracy: 99.03%\tGradient Norm: 0.000172\n",
      "Epoch: 1 [38528/60000 (64%)]\tLoss: 0.038435\tAccuracy: 99.03%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [38592/60000 (64%)]\tLoss: 0.036056\tAccuracy: 99.03%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [38656/60000 (64%)]\tLoss: 0.025633\tAccuracy: 99.03%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [38720/60000 (64%)]\tLoss: 0.011125\tAccuracy: 99.03%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [38784/60000 (65%)]\tLoss: 0.034166\tAccuracy: 99.03%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [38848/60000 (65%)]\tLoss: 0.068238\tAccuracy: 99.03%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [38912/60000 (65%)]\tLoss: 0.008842\tAccuracy: 99.03%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [38976/60000 (65%)]\tLoss: 0.005271\tAccuracy: 99.03%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [39040/60000 (65%)]\tLoss: 0.049088\tAccuracy: 99.03%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [39104/60000 (65%)]\tLoss: 0.062909\tAccuracy: 99.03%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [39168/60000 (65%)]\tLoss: 0.043052\tAccuracy: 99.03%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [39232/60000 (65%)]\tLoss: 0.008537\tAccuracy: 99.03%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [39296/60000 (65%)]\tLoss: 0.040847\tAccuracy: 99.03%\tGradient Norm: 0.000176\n",
      "Epoch: 1 [39360/60000 (66%)]\tLoss: 0.032452\tAccuracy: 99.03%\tGradient Norm: 0.000162\n",
      "Epoch: 1 [39424/60000 (66%)]\tLoss: 0.007295\tAccuracy: 99.03%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [39488/60000 (66%)]\tLoss: 0.021834\tAccuracy: 99.03%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [39552/60000 (66%)]\tLoss: 0.016262\tAccuracy: 99.03%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [39616/60000 (66%)]\tLoss: 0.012048\tAccuracy: 99.03%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [39680/60000 (66%)]\tLoss: 0.024229\tAccuracy: 99.04%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [39744/60000 (66%)]\tLoss: 0.007343\tAccuracy: 99.04%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [39808/60000 (66%)]\tLoss: 0.103277\tAccuracy: 99.03%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [39872/60000 (66%)]\tLoss: 0.014158\tAccuracy: 99.04%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [39936/60000 (67%)]\tLoss: 0.057353\tAccuracy: 99.03%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [40000/60000 (67%)]\tLoss: 0.040223\tAccuracy: 99.03%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [40064/60000 (67%)]\tLoss: 0.005719\tAccuracy: 99.04%\tGradient Norm: 0.000175\n",
      "Epoch: 1 [40128/60000 (67%)]\tLoss: 0.014497\tAccuracy: 99.04%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [40192/60000 (67%)]\tLoss: 0.013142\tAccuracy: 99.04%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [40256/60000 (67%)]\tLoss: 0.016162\tAccuracy: 99.04%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [40320/60000 (67%)]\tLoss: 0.017657\tAccuracy: 99.04%\tGradient Norm: 0.000183\n",
      "Epoch: 1 [40384/60000 (67%)]\tLoss: 0.036563\tAccuracy: 99.04%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [40448/60000 (67%)]\tLoss: 0.006408\tAccuracy: 99.04%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [40512/60000 (67%)]\tLoss: 0.055657\tAccuracy: 99.04%\tGradient Norm: 0.000176\n",
      "Epoch: 1 [40576/60000 (68%)]\tLoss: 0.021683\tAccuracy: 99.04%\tGradient Norm: 0.000179\n",
      "Epoch: 1 [40640/60000 (68%)]\tLoss: 0.166489\tAccuracy: 99.04%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [40704/60000 (68%)]\tLoss: 0.009983\tAccuracy: 99.04%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [40768/60000 (68%)]\tLoss: 0.022436\tAccuracy: 99.04%\tGradient Norm: 0.000166\n",
      "Epoch: 1 [40832/60000 (68%)]\tLoss: 0.013449\tAccuracy: 99.04%\tGradient Norm: 0.000175\n",
      "Epoch: 1 [40896/60000 (68%)]\tLoss: 0.034791\tAccuracy: 99.04%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [40960/60000 (68%)]\tLoss: 0.004085\tAccuracy: 99.04%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [41024/60000 (68%)]\tLoss: 0.036507\tAccuracy: 99.04%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [41088/60000 (68%)]\tLoss: 0.027468\tAccuracy: 99.04%\tGradient Norm: 0.000163\n",
      "Epoch: 1 [41152/60000 (69%)]\tLoss: 0.006507\tAccuracy: 99.04%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [41216/60000 (69%)]\tLoss: 0.038097\tAccuracy: 99.04%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [41280/60000 (69%)]\tLoss: 0.010904\tAccuracy: 99.04%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [41344/60000 (69%)]\tLoss: 0.043584\tAccuracy: 99.04%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [41408/60000 (69%)]\tLoss: 0.118395\tAccuracy: 99.04%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [41472/60000 (69%)]\tLoss: 0.006746\tAccuracy: 99.04%\tGradient Norm: 0.000173\n",
      "Epoch: 1 [41536/60000 (69%)]\tLoss: 0.018617\tAccuracy: 99.04%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [41600/60000 (69%)]\tLoss: 0.057775\tAccuracy: 99.04%\tGradient Norm: 0.000176\n",
      "Epoch: 1 [41664/60000 (69%)]\tLoss: 0.039454\tAccuracy: 99.04%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [41728/60000 (70%)]\tLoss: 0.049725\tAccuracy: 99.04%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [41792/60000 (70%)]\tLoss: 0.027914\tAccuracy: 99.04%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [41856/60000 (70%)]\tLoss: 0.029445\tAccuracy: 99.04%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [41920/60000 (70%)]\tLoss: 0.085993\tAccuracy: 99.04%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [41984/60000 (70%)]\tLoss: 0.047762\tAccuracy: 99.03%\tGradient Norm: 0.000179\n",
      "Epoch: 1 [42048/60000 (70%)]\tLoss: 0.025499\tAccuracy: 99.03%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [42112/60000 (70%)]\tLoss: 0.057120\tAccuracy: 99.03%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [42176/60000 (70%)]\tLoss: 0.066976\tAccuracy: 99.03%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [42240/60000 (70%)]\tLoss: 0.006480\tAccuracy: 99.03%\tGradient Norm: 0.000183\n",
      "Epoch: 1 [42304/60000 (70%)]\tLoss: 0.005902\tAccuracy: 99.03%\tGradient Norm: 0.000169\n",
      "Epoch: 1 [42368/60000 (71%)]\tLoss: 0.025482\tAccuracy: 99.03%\tGradient Norm: 0.000169\n",
      "Epoch: 1 [42432/60000 (71%)]\tLoss: 0.013697\tAccuracy: 99.03%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [42496/60000 (71%)]\tLoss: 0.007822\tAccuracy: 99.03%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [42560/60000 (71%)]\tLoss: 0.005746\tAccuracy: 99.04%\tGradient Norm: 0.000176\n",
      "Epoch: 1 [42624/60000 (71%)]\tLoss: 0.008888\tAccuracy: 99.04%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [42688/60000 (71%)]\tLoss: 0.043516\tAccuracy: 99.04%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [42752/60000 (71%)]\tLoss: 0.085251\tAccuracy: 99.03%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [42816/60000 (71%)]\tLoss: 0.013701\tAccuracy: 99.03%\tGradient Norm: 0.000173\n",
      "Epoch: 1 [42880/60000 (71%)]\tLoss: 0.042322\tAccuracy: 99.03%\tGradient Norm: 0.000183\n",
      "Epoch: 1 [42944/60000 (72%)]\tLoss: 0.038432\tAccuracy: 99.03%\tGradient Norm: 0.000174\n",
      "Epoch: 1 [43008/60000 (72%)]\tLoss: 0.020799\tAccuracy: 99.03%\tGradient Norm: 0.000172\n",
      "Epoch: 1 [43072/60000 (72%)]\tLoss: 0.005503\tAccuracy: 99.03%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [43136/60000 (72%)]\tLoss: 0.036459\tAccuracy: 99.03%\tGradient Norm: 0.000151\n",
      "Epoch: 1 [43200/60000 (72%)]\tLoss: 0.004990\tAccuracy: 99.03%\tGradient Norm: 0.000174\n",
      "Epoch: 1 [43264/60000 (72%)]\tLoss: 0.011930\tAccuracy: 99.04%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [43328/60000 (72%)]\tLoss: 0.088365\tAccuracy: 99.03%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [43392/60000 (72%)]\tLoss: 0.005681\tAccuracy: 99.03%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [43456/60000 (72%)]\tLoss: 0.018070\tAccuracy: 99.03%\tGradient Norm: 0.000178\n",
      "Epoch: 1 [43520/60000 (72%)]\tLoss: 0.005080\tAccuracy: 99.04%\tGradient Norm: 0.000177\n",
      "Epoch: 1 [43584/60000 (73%)]\tLoss: 0.011209\tAccuracy: 99.04%\tGradient Norm: 0.000172\n",
      "Epoch: 1 [43648/60000 (73%)]\tLoss: 0.014143\tAccuracy: 99.04%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [43712/60000 (73%)]\tLoss: 0.017986\tAccuracy: 99.04%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [43776/60000 (73%)]\tLoss: 0.061648\tAccuracy: 99.04%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [43840/60000 (73%)]\tLoss: 0.059659\tAccuracy: 99.03%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [43904/60000 (73%)]\tLoss: 0.003699\tAccuracy: 99.04%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [43968/60000 (73%)]\tLoss: 0.003365\tAccuracy: 99.04%\tGradient Norm: 0.000169\n",
      "Epoch: 1 [44032/60000 (73%)]\tLoss: 0.005859\tAccuracy: 99.04%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [44096/60000 (73%)]\tLoss: 0.017818\tAccuracy: 99.04%\tGradient Norm: 0.000164\n",
      "Epoch: 1 [44160/60000 (74%)]\tLoss: 0.002400\tAccuracy: 99.04%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [44224/60000 (74%)]\tLoss: 0.069613\tAccuracy: 99.04%\tGradient Norm: 0.000169\n",
      "Epoch: 1 [44288/60000 (74%)]\tLoss: 0.007964\tAccuracy: 99.04%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [44352/60000 (74%)]\tLoss: 0.020422\tAccuracy: 99.04%\tGradient Norm: 0.000179\n",
      "Epoch: 1 [44416/60000 (74%)]\tLoss: 0.023908\tAccuracy: 99.04%\tGradient Norm: 0.000172\n",
      "Epoch: 1 [44480/60000 (74%)]\tLoss: 0.005696\tAccuracy: 99.04%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [44544/60000 (74%)]\tLoss: 0.006426\tAccuracy: 99.04%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [44608/60000 (74%)]\tLoss: 0.125856\tAccuracy: 99.04%\tGradient Norm: 0.000168\n",
      "Epoch: 1 [44672/60000 (74%)]\tLoss: 0.047207\tAccuracy: 99.04%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [44736/60000 (75%)]\tLoss: 0.026749\tAccuracy: 99.04%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [44800/60000 (75%)]\tLoss: 0.034625\tAccuracy: 99.03%\tGradient Norm: 0.000163\n",
      "Epoch: 1 [44864/60000 (75%)]\tLoss: 0.109155\tAccuracy: 99.03%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [44928/60000 (75%)]\tLoss: 0.109878\tAccuracy: 99.03%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [44992/60000 (75%)]\tLoss: 0.018237\tAccuracy: 99.03%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [45056/60000 (75%)]\tLoss: 0.006474\tAccuracy: 99.04%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [45120/60000 (75%)]\tLoss: 0.008793\tAccuracy: 99.04%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [45184/60000 (75%)]\tLoss: 0.031962\tAccuracy: 99.04%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [45248/60000 (75%)]\tLoss: 0.026558\tAccuracy: 99.04%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [45312/60000 (75%)]\tLoss: 0.023286\tAccuracy: 99.04%\tGradient Norm: 0.000174\n",
      "Epoch: 1 [45376/60000 (76%)]\tLoss: 0.005577\tAccuracy: 99.04%\tGradient Norm: 0.000175\n",
      "Epoch: 1 [45440/60000 (76%)]\tLoss: 0.088990\tAccuracy: 99.04%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [45504/60000 (76%)]\tLoss: 0.006210\tAccuracy: 99.04%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [45568/60000 (76%)]\tLoss: 0.013083\tAccuracy: 99.04%\tGradient Norm: 0.000178\n",
      "Epoch: 1 [45632/60000 (76%)]\tLoss: 0.068316\tAccuracy: 99.04%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [45696/60000 (76%)]\tLoss: 0.052112\tAccuracy: 99.04%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [45760/60000 (76%)]\tLoss: 0.055003\tAccuracy: 99.04%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [45824/60000 (76%)]\tLoss: 0.004062\tAccuracy: 99.04%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [45888/60000 (76%)]\tLoss: 0.003739\tAccuracy: 99.04%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [45952/60000 (77%)]\tLoss: 0.015719\tAccuracy: 99.04%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [46016/60000 (77%)]\tLoss: 0.040260\tAccuracy: 99.04%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [46080/60000 (77%)]\tLoss: 0.038661\tAccuracy: 99.04%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [46144/60000 (77%)]\tLoss: 0.113878\tAccuracy: 99.03%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [46208/60000 (77%)]\tLoss: 0.009154\tAccuracy: 99.03%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [46272/60000 (77%)]\tLoss: 0.095298\tAccuracy: 99.03%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [46336/60000 (77%)]\tLoss: 0.069538\tAccuracy: 99.03%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [46400/60000 (77%)]\tLoss: 0.003235\tAccuracy: 99.03%\tGradient Norm: 0.000166\n",
      "Epoch: 1 [46464/60000 (77%)]\tLoss: 0.066883\tAccuracy: 99.03%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [46528/60000 (78%)]\tLoss: 0.069195\tAccuracy: 99.03%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [46592/60000 (78%)]\tLoss: 0.032080\tAccuracy: 99.02%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [46656/60000 (78%)]\tLoss: 0.017131\tAccuracy: 99.03%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [46720/60000 (78%)]\tLoss: 0.009616\tAccuracy: 99.03%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [46784/60000 (78%)]\tLoss: 0.002958\tAccuracy: 99.03%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [46848/60000 (78%)]\tLoss: 0.005581\tAccuracy: 99.03%\tGradient Norm: 0.000171\n",
      "Epoch: 1 [46912/60000 (78%)]\tLoss: 0.029960\tAccuracy: 99.03%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [46976/60000 (78%)]\tLoss: 0.070714\tAccuracy: 99.03%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [47040/60000 (78%)]\tLoss: 0.010233\tAccuracy: 99.03%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [47104/60000 (78%)]\tLoss: 0.137192\tAccuracy: 99.03%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [47168/60000 (79%)]\tLoss: 0.037406\tAccuracy: 99.03%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [47232/60000 (79%)]\tLoss: 0.038296\tAccuracy: 99.03%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [47296/60000 (79%)]\tLoss: 0.002578\tAccuracy: 99.03%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [47360/60000 (79%)]\tLoss: 0.047179\tAccuracy: 99.03%\tGradient Norm: 0.000186\n",
      "Epoch: 1 [47424/60000 (79%)]\tLoss: 0.058847\tAccuracy: 99.03%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [47488/60000 (79%)]\tLoss: 0.013387\tAccuracy: 99.03%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [47552/60000 (79%)]\tLoss: 0.044473\tAccuracy: 99.03%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [47616/60000 (79%)]\tLoss: 0.013044\tAccuracy: 99.03%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [47680/60000 (79%)]\tLoss: 0.063933\tAccuracy: 99.02%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [47744/60000 (80%)]\tLoss: 0.004376\tAccuracy: 99.03%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [47808/60000 (80%)]\tLoss: 0.005646\tAccuracy: 99.03%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [47872/60000 (80%)]\tLoss: 0.117523\tAccuracy: 99.03%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [47936/60000 (80%)]\tLoss: 0.055606\tAccuracy: 99.02%\tGradient Norm: 0.000170\n",
      "Epoch: 1 [48000/60000 (80%)]\tLoss: 0.017509\tAccuracy: 99.02%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [48064/60000 (80%)]\tLoss: 0.013986\tAccuracy: 99.03%\tGradient Norm: 0.000243\n",
      "Epoch: 1 [48128/60000 (80%)]\tLoss: 0.019863\tAccuracy: 99.03%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [48192/60000 (80%)]\tLoss: 0.082981\tAccuracy: 99.03%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [48256/60000 (80%)]\tLoss: 0.075490\tAccuracy: 99.02%\tGradient Norm: 0.000175\n",
      "Epoch: 1 [48320/60000 (80%)]\tLoss: 0.101352\tAccuracy: 99.02%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [48384/60000 (81%)]\tLoss: 0.010003\tAccuracy: 99.02%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [48448/60000 (81%)]\tLoss: 0.035585\tAccuracy: 99.02%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [48512/60000 (81%)]\tLoss: 0.027813\tAccuracy: 99.02%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [48576/60000 (81%)]\tLoss: 0.003033\tAccuracy: 99.02%\tGradient Norm: 0.000178\n",
      "Epoch: 1 [48640/60000 (81%)]\tLoss: 0.045452\tAccuracy: 99.02%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [48704/60000 (81%)]\tLoss: 0.023835\tAccuracy: 99.02%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [48768/60000 (81%)]\tLoss: 0.008148\tAccuracy: 99.02%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [48832/60000 (81%)]\tLoss: 0.006955\tAccuracy: 99.02%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [48896/60000 (81%)]\tLoss: 0.012465\tAccuracy: 99.03%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [48960/60000 (82%)]\tLoss: 0.016262\tAccuracy: 99.03%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [49024/60000 (82%)]\tLoss: 0.071330\tAccuracy: 99.03%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [49088/60000 (82%)]\tLoss: 0.006877\tAccuracy: 99.03%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [49152/60000 (82%)]\tLoss: 0.005498\tAccuracy: 99.03%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [49216/60000 (82%)]\tLoss: 0.100341\tAccuracy: 99.03%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [49280/60000 (82%)]\tLoss: 0.018619\tAccuracy: 99.03%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [49344/60000 (82%)]\tLoss: 0.075757\tAccuracy: 99.02%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [49408/60000 (82%)]\tLoss: 0.007923\tAccuracy: 99.03%\tGradient Norm: 0.000169\n",
      "Epoch: 1 [49472/60000 (82%)]\tLoss: 0.003489\tAccuracy: 99.03%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [49536/60000 (83%)]\tLoss: 0.082293\tAccuracy: 99.02%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [49600/60000 (83%)]\tLoss: 0.060655\tAccuracy: 99.02%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [49664/60000 (83%)]\tLoss: 0.053937\tAccuracy: 99.02%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [49728/60000 (83%)]\tLoss: 0.006198\tAccuracy: 99.02%\tGradient Norm: 0.000221\n",
      "Epoch: 1 [49792/60000 (83%)]\tLoss: 0.010977\tAccuracy: 99.02%\tGradient Norm: 0.000229\n",
      "Epoch: 1 [49856/60000 (83%)]\tLoss: 0.015823\tAccuracy: 99.02%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [49920/60000 (83%)]\tLoss: 0.027092\tAccuracy: 99.02%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [49984/60000 (83%)]\tLoss: 0.036325\tAccuracy: 99.02%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [50048/60000 (83%)]\tLoss: 0.013644\tAccuracy: 99.02%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [50112/60000 (83%)]\tLoss: 0.047199\tAccuracy: 99.02%\tGradient Norm: 0.000217\n",
      "Epoch: 1 [50176/60000 (84%)]\tLoss: 0.014491\tAccuracy: 99.02%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [50240/60000 (84%)]\tLoss: 0.124556\tAccuracy: 99.02%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [50304/60000 (84%)]\tLoss: 0.043498\tAccuracy: 99.02%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [50368/60000 (84%)]\tLoss: 0.030246\tAccuracy: 99.02%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [50432/60000 (84%)]\tLoss: 0.026571\tAccuracy: 99.02%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [50496/60000 (84%)]\tLoss: 0.013533\tAccuracy: 99.02%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [50560/60000 (84%)]\tLoss: 0.006738\tAccuracy: 99.02%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [50624/60000 (84%)]\tLoss: 0.013289\tAccuracy: 99.02%\tGradient Norm: 0.000193\n",
      "Epoch: 1 [50688/60000 (84%)]\tLoss: 0.011205\tAccuracy: 99.02%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [50752/60000 (85%)]\tLoss: 0.012436\tAccuracy: 99.02%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [50816/60000 (85%)]\tLoss: 0.008523\tAccuracy: 99.03%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [50880/60000 (85%)]\tLoss: 0.009544\tAccuracy: 99.03%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [50944/60000 (85%)]\tLoss: 0.055017\tAccuracy: 99.02%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [51008/60000 (85%)]\tLoss: 0.016175\tAccuracy: 99.02%\tGradient Norm: 0.000176\n",
      "Epoch: 1 [51072/60000 (85%)]\tLoss: 0.104552\tAccuracy: 99.02%\tGradient Norm: 0.000235\n",
      "Epoch: 1 [51136/60000 (85%)]\tLoss: 0.031551\tAccuracy: 99.02%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [51200/60000 (85%)]\tLoss: 0.013297\tAccuracy: 99.02%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [51264/60000 (85%)]\tLoss: 0.023045\tAccuracy: 99.02%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [51328/60000 (86%)]\tLoss: 0.011622\tAccuracy: 99.03%\tGradient Norm: 0.000234\n",
      "Epoch: 1 [51392/60000 (86%)]\tLoss: 0.036595\tAccuracy: 99.02%\tGradient Norm: 0.000230\n",
      "Epoch: 1 [51456/60000 (86%)]\tLoss: 0.004546\tAccuracy: 99.03%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [51520/60000 (86%)]\tLoss: 0.002754\tAccuracy: 99.03%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [51584/60000 (86%)]\tLoss: 0.031494\tAccuracy: 99.03%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [51648/60000 (86%)]\tLoss: 0.018880\tAccuracy: 99.03%\tGradient Norm: 0.000233\n",
      "Epoch: 1 [51712/60000 (86%)]\tLoss: 0.044668\tAccuracy: 99.02%\tGradient Norm: 0.000252\n",
      "Epoch: 1 [51776/60000 (86%)]\tLoss: 0.035139\tAccuracy: 99.02%\tGradient Norm: 0.000219\n",
      "Epoch: 1 [51840/60000 (86%)]\tLoss: 0.068829\tAccuracy: 99.02%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [51904/60000 (86%)]\tLoss: 0.133052\tAccuracy: 99.02%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [51968/60000 (87%)]\tLoss: 0.058440\tAccuracy: 99.02%\tGradient Norm: 0.000183\n",
      "Epoch: 1 [52032/60000 (87%)]\tLoss: 0.114464\tAccuracy: 99.01%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [52096/60000 (87%)]\tLoss: 0.008169\tAccuracy: 99.01%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [52160/60000 (87%)]\tLoss: 0.011487\tAccuracy: 99.01%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [52224/60000 (87%)]\tLoss: 0.070031\tAccuracy: 99.01%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [52288/60000 (87%)]\tLoss: 0.006838\tAccuracy: 99.01%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [52352/60000 (87%)]\tLoss: 0.026494\tAccuracy: 99.01%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [52416/60000 (87%)]\tLoss: 0.032172\tAccuracy: 99.01%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [52480/60000 (87%)]\tLoss: 0.019837\tAccuracy: 99.01%\tGradient Norm: 0.000237\n",
      "Epoch: 1 [52544/60000 (88%)]\tLoss: 0.051801\tAccuracy: 99.01%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [52608/60000 (88%)]\tLoss: 0.038486\tAccuracy: 99.01%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [52672/60000 (88%)]\tLoss: 0.009635\tAccuracy: 99.01%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [52736/60000 (88%)]\tLoss: 0.023657\tAccuracy: 99.01%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [52800/60000 (88%)]\tLoss: 0.005643\tAccuracy: 99.01%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [52864/60000 (88%)]\tLoss: 0.010812\tAccuracy: 99.02%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [52928/60000 (88%)]\tLoss: 0.029754\tAccuracy: 99.01%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [52992/60000 (88%)]\tLoss: 0.010178\tAccuracy: 99.02%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [53056/60000 (88%)]\tLoss: 0.003663\tAccuracy: 99.02%\tGradient Norm: 0.000185\n",
      "Epoch: 1 [53120/60000 (88%)]\tLoss: 0.006333\tAccuracy: 99.02%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [53184/60000 (89%)]\tLoss: 0.002249\tAccuracy: 99.02%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [53248/60000 (89%)]\tLoss: 0.024869\tAccuracy: 99.02%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [53312/60000 (89%)]\tLoss: 0.011428\tAccuracy: 99.02%\tGradient Norm: 0.000169\n",
      "Epoch: 1 [53376/60000 (89%)]\tLoss: 0.041784\tAccuracy: 99.02%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [53440/60000 (89%)]\tLoss: 0.005434\tAccuracy: 99.02%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [53504/60000 (89%)]\tLoss: 0.005223\tAccuracy: 99.02%\tGradient Norm: 0.000175\n",
      "Epoch: 1 [53568/60000 (89%)]\tLoss: 0.007448\tAccuracy: 99.02%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [53632/60000 (89%)]\tLoss: 0.028659\tAccuracy: 99.02%\tGradient Norm: 0.000223\n",
      "Epoch: 1 [53696/60000 (89%)]\tLoss: 0.044308\tAccuracy: 99.02%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [53760/60000 (90%)]\tLoss: 0.047127\tAccuracy: 99.02%\tGradient Norm: 0.000205\n",
      "Epoch: 1 [53824/60000 (90%)]\tLoss: 0.011535\tAccuracy: 99.02%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [53888/60000 (90%)]\tLoss: 0.024607\tAccuracy: 99.02%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [53952/60000 (90%)]\tLoss: 0.011184\tAccuracy: 99.02%\tGradient Norm: 0.000184\n",
      "Epoch: 1 [54016/60000 (90%)]\tLoss: 0.028143\tAccuracy: 99.02%\tGradient Norm: 0.000177\n",
      "Epoch: 1 [54080/60000 (90%)]\tLoss: 0.074016\tAccuracy: 99.02%\tGradient Norm: 0.000195\n",
      "Epoch: 1 [54144/60000 (90%)]\tLoss: 0.039539\tAccuracy: 99.02%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [54208/60000 (90%)]\tLoss: 0.006951\tAccuracy: 99.02%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [54272/60000 (90%)]\tLoss: 0.010045\tAccuracy: 99.02%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [54336/60000 (91%)]\tLoss: 0.004140\tAccuracy: 99.02%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [54400/60000 (91%)]\tLoss: 0.008011\tAccuracy: 99.03%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [54464/60000 (91%)]\tLoss: 0.007896\tAccuracy: 99.03%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [54528/60000 (91%)]\tLoss: 0.025491\tAccuracy: 99.03%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [54592/60000 (91%)]\tLoss: 0.011056\tAccuracy: 99.03%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [54656/60000 (91%)]\tLoss: 0.009038\tAccuracy: 99.03%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [54720/60000 (91%)]\tLoss: 0.006647\tAccuracy: 99.03%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [54784/60000 (91%)]\tLoss: 0.046930\tAccuracy: 99.03%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [54848/60000 (91%)]\tLoss: 0.027314\tAccuracy: 99.03%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [54912/60000 (91%)]\tLoss: 0.015485\tAccuracy: 99.03%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [54976/60000 (92%)]\tLoss: 0.010399\tAccuracy: 99.03%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [55040/60000 (92%)]\tLoss: 0.029378\tAccuracy: 99.03%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [55104/60000 (92%)]\tLoss: 0.027758\tAccuracy: 99.03%\tGradient Norm: 0.000188\n",
      "Epoch: 1 [55168/60000 (92%)]\tLoss: 0.030164\tAccuracy: 99.03%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [55232/60000 (92%)]\tLoss: 0.002874\tAccuracy: 99.03%\tGradient Norm: 0.000251\n",
      "Epoch: 1 [55296/60000 (92%)]\tLoss: 0.016517\tAccuracy: 99.03%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [55360/60000 (92%)]\tLoss: 0.022521\tAccuracy: 99.03%\tGradient Norm: 0.000194\n",
      "Epoch: 1 [55424/60000 (92%)]\tLoss: 0.022251\tAccuracy: 99.03%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [55488/60000 (92%)]\tLoss: 0.016625\tAccuracy: 99.04%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [55552/60000 (93%)]\tLoss: 0.038082\tAccuracy: 99.03%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [55616/60000 (93%)]\tLoss: 0.013410\tAccuracy: 99.04%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [55680/60000 (93%)]\tLoss: 0.029359\tAccuracy: 99.03%\tGradient Norm: 0.000212\n",
      "Epoch: 1 [55744/60000 (93%)]\tLoss: 0.042727\tAccuracy: 99.03%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [55808/60000 (93%)]\tLoss: 0.039033\tAccuracy: 99.03%\tGradient Norm: 0.000211\n",
      "Epoch: 1 [55872/60000 (93%)]\tLoss: 0.003615\tAccuracy: 99.03%\tGradient Norm: 0.000214\n",
      "Epoch: 1 [55936/60000 (93%)]\tLoss: 0.021978\tAccuracy: 99.04%\tGradient Norm: 0.000183\n",
      "Epoch: 1 [56000/60000 (93%)]\tLoss: 0.017805\tAccuracy: 99.04%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [56064/60000 (93%)]\tLoss: 0.042162\tAccuracy: 99.04%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [56128/60000 (93%)]\tLoss: 0.028508\tAccuracy: 99.04%\tGradient Norm: 0.000225\n",
      "Epoch: 1 [56192/60000 (94%)]\tLoss: 0.094956\tAccuracy: 99.03%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [56256/60000 (94%)]\tLoss: 0.009813\tAccuracy: 99.04%\tGradient Norm: 0.000178\n",
      "Epoch: 1 [56320/60000 (94%)]\tLoss: 0.054829\tAccuracy: 99.04%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [56384/60000 (94%)]\tLoss: 0.015685\tAccuracy: 99.04%\tGradient Norm: 0.000266\n",
      "Epoch: 1 [56448/60000 (94%)]\tLoss: 0.009088\tAccuracy: 99.04%\tGradient Norm: 0.000191\n",
      "Epoch: 1 [56512/60000 (94%)]\tLoss: 0.019393\tAccuracy: 99.04%\tGradient Norm: 0.000203\n",
      "Epoch: 1 [56576/60000 (94%)]\tLoss: 0.004692\tAccuracy: 99.04%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [56640/60000 (94%)]\tLoss: 0.017183\tAccuracy: 99.04%\tGradient Norm: 0.000198\n",
      "Epoch: 1 [56704/60000 (94%)]\tLoss: 0.013385\tAccuracy: 99.04%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [56768/60000 (95%)]\tLoss: 0.011530\tAccuracy: 99.04%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [56832/60000 (95%)]\tLoss: 0.008222\tAccuracy: 99.04%\tGradient Norm: 0.000200\n",
      "Epoch: 1 [56896/60000 (95%)]\tLoss: 0.004510\tAccuracy: 99.04%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [56960/60000 (95%)]\tLoss: 0.024003\tAccuracy: 99.05%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [57024/60000 (95%)]\tLoss: 0.007294\tAccuracy: 99.05%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [57088/60000 (95%)]\tLoss: 0.003611\tAccuracy: 99.05%\tGradient Norm: 0.000197\n",
      "Epoch: 1 [57152/60000 (95%)]\tLoss: 0.010634\tAccuracy: 99.05%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [57216/60000 (95%)]\tLoss: 0.033631\tAccuracy: 99.05%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [57280/60000 (95%)]\tLoss: 0.010282\tAccuracy: 99.05%\tGradient Norm: 0.000196\n",
      "Epoch: 1 [57344/60000 (96%)]\tLoss: 0.050995\tAccuracy: 99.05%\tGradient Norm: 0.000204\n",
      "Epoch: 1 [57408/60000 (96%)]\tLoss: 0.042018\tAccuracy: 99.05%\tGradient Norm: 0.000231\n",
      "Epoch: 1 [57472/60000 (96%)]\tLoss: 0.025596\tAccuracy: 99.05%\tGradient Norm: 0.000189\n",
      "Epoch: 1 [57536/60000 (96%)]\tLoss: 0.019491\tAccuracy: 99.05%\tGradient Norm: 0.000190\n",
      "Epoch: 1 [57600/60000 (96%)]\tLoss: 0.003905\tAccuracy: 99.05%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [57664/60000 (96%)]\tLoss: 0.016443\tAccuracy: 99.05%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [57728/60000 (96%)]\tLoss: 0.045108\tAccuracy: 99.05%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [57792/60000 (96%)]\tLoss: 0.054745\tAccuracy: 99.05%\tGradient Norm: 0.000232\n",
      "Epoch: 1 [57856/60000 (96%)]\tLoss: 0.015011\tAccuracy: 99.05%\tGradient Norm: 0.000180\n",
      "Epoch: 1 [57920/60000 (96%)]\tLoss: 0.076972\tAccuracy: 99.05%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [57984/60000 (97%)]\tLoss: 0.004053\tAccuracy: 99.05%\tGradient Norm: 0.000187\n",
      "Epoch: 1 [58048/60000 (97%)]\tLoss: 0.013559\tAccuracy: 99.05%\tGradient Norm: 0.000181\n",
      "Epoch: 1 [58112/60000 (97%)]\tLoss: 0.008872\tAccuracy: 99.05%\tGradient Norm: 0.000206\n",
      "Epoch: 1 [58176/60000 (97%)]\tLoss: 0.034669\tAccuracy: 99.05%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [58240/60000 (97%)]\tLoss: 0.048675\tAccuracy: 99.05%\tGradient Norm: 0.000215\n",
      "Epoch: 1 [58304/60000 (97%)]\tLoss: 0.023324\tAccuracy: 99.05%\tGradient Norm: 0.000236\n",
      "Epoch: 1 [58368/60000 (97%)]\tLoss: 0.002824\tAccuracy: 99.05%\tGradient Norm: 0.000216\n",
      "Epoch: 1 [58432/60000 (97%)]\tLoss: 0.026599\tAccuracy: 99.05%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [58496/60000 (97%)]\tLoss: 0.004285\tAccuracy: 99.05%\tGradient Norm: 0.000208\n",
      "Epoch: 1 [58560/60000 (98%)]\tLoss: 0.006174\tAccuracy: 99.05%\tGradient Norm: 0.000210\n",
      "Epoch: 1 [58624/60000 (98%)]\tLoss: 0.032798\tAccuracy: 99.05%\tGradient Norm: 0.000220\n",
      "Epoch: 1 [58688/60000 (98%)]\tLoss: 0.007194\tAccuracy: 99.05%\tGradient Norm: 0.000226\n",
      "Epoch: 1 [58752/60000 (98%)]\tLoss: 0.015413\tAccuracy: 99.05%\tGradient Norm: 0.000224\n",
      "Epoch: 1 [58816/60000 (98%)]\tLoss: 0.086297\tAccuracy: 99.05%\tGradient Norm: 0.000182\n",
      "Epoch: 1 [58880/60000 (98%)]\tLoss: 0.012286\tAccuracy: 99.05%\tGradient Norm: 0.000192\n",
      "Epoch: 1 [58944/60000 (98%)]\tLoss: 0.012140\tAccuracy: 99.05%\tGradient Norm: 0.000279\n",
      "Epoch: 1 [59008/60000 (98%)]\tLoss: 0.045255\tAccuracy: 99.05%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [59072/60000 (98%)]\tLoss: 0.107806\tAccuracy: 99.05%\tGradient Norm: 0.000264\n",
      "Epoch: 1 [59136/60000 (99%)]\tLoss: 0.025948\tAccuracy: 99.05%\tGradient Norm: 0.000199\n",
      "Epoch: 1 [59200/60000 (99%)]\tLoss: 0.006704\tAccuracy: 99.06%\tGradient Norm: 0.000207\n",
      "Epoch: 1 [59264/60000 (99%)]\tLoss: 0.061656\tAccuracy: 99.05%\tGradient Norm: 0.000227\n",
      "Epoch: 1 [59328/60000 (99%)]\tLoss: 0.030096\tAccuracy: 99.05%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [59392/60000 (99%)]\tLoss: 0.052240\tAccuracy: 99.05%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [59456/60000 (99%)]\tLoss: 0.004290\tAccuracy: 99.05%\tGradient Norm: 0.000222\n",
      "Epoch: 1 [59520/60000 (99%)]\tLoss: 0.024422\tAccuracy: 99.05%\tGradient Norm: 0.000202\n",
      "Epoch: 1 [59584/60000 (99%)]\tLoss: 0.025691\tAccuracy: 99.05%\tGradient Norm: 0.000253\n",
      "Epoch: 1 [59648/60000 (99%)]\tLoss: 0.007977\tAccuracy: 99.05%\tGradient Norm: 0.000209\n",
      "Epoch: 1 [59712/60000 (99%)]\tLoss: 0.159080\tAccuracy: 99.05%\tGradient Norm: 0.000228\n",
      "Epoch: 1 [59776/60000 (100%)]\tLoss: 0.027572\tAccuracy: 99.05%\tGradient Norm: 0.000218\n",
      "Epoch: 1 [59840/60000 (100%)]\tLoss: 0.003707\tAccuracy: 99.05%\tGradient Norm: 0.000213\n",
      "Epoch: 1 [59904/60000 (100%)]\tLoss: 0.011003\tAccuracy: 99.05%\tGradient Norm: 0.000201\n",
      "Epoch: 1 [29984/60000 (100%)]\tLoss: 0.047824\tAccuracy: 99.05%\tGradient Norm: 0.000225\n",
      "Accuracy:  95.26166666666667\n"
     ]
    }
   ],
   "source": [
    "#call the train function\n",
    "model = train(model, train_loader, optimizer, epoch=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will test the model on usps test dataset\n",
    "#now we have the trained model on MNIST, we will use the same model to test on USPS\n",
    "#the usps test data is in dataloader : test_loader\n",
    "\n",
    "#we will define a function to test the model\n",
    "def test(model, test_loader):\n",
    "    #set the model to evaluation mode\n",
    "    model.eval()\n",
    "    #set the total and correct to zero\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    #we will not calculate the gradients\n",
    "    with torch.no_grad():\n",
    "        #loop through the test data\n",
    "        for data, target in test_loader:\n",
    "            #send the data to gpu\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #get the output from the model\n",
    "            output = model(data)\n",
    "            #get the max value from the output\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            #calculate the total number of labels\n",
    "            total += target.size(0)\n",
    "            #calculate the correct predictions\n",
    "            correct += (predicted == target).sum().item()\n",
    "    #calculate the accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    #print the accuracy\n",
    "    print('Accuracy: ', accuracy)\n",
    "    #return the accuracy\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  9.971197366616376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.971197366616376"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call the test function\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "#the name be experiment_id + _final\n",
    "torch.save(model.state_dict(), 'saved_models/'+experiment_id+'_final.pth')\n",
    "\n",
    "#save the optimizer\n",
    "torch.save(optimizer.state_dict(), 'saved_models/'+experiment_id+'_final_optimizer.pth')\n",
    "\n",
    "#close the tensorboard writer\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to print precision, recall, f1 score and support of each class in a classification report\n",
    "#input will be a model and a dataloader\n",
    "def classification_report(model, test_loader):\n",
    "    #number of classes is in the variable num_classes\n",
    "    #we will builld a confusion matrix\n",
    "\n",
    "    #now we build y_true and y_pred \n",
    "    #we will use the test_loader\n",
    "    #we will set the model to evaluation mode\n",
    "    # model.eval()\n",
    "    #we will not calculate the gradients\n",
    "    #build the y_true and y_pred\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        #loop through the test data\n",
    "        for data, target in test_loader:\n",
    "            #send the data to gpu\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #get the output from the model\n",
    "            output = model(data)\n",
    "            #get the max value from the output\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            #get the y_pred\n",
    "            y_pred_new = predicted.cpu().numpy()\n",
    "            #concate the y_pred\n",
    "            y_pred = np.concatenate((y_pred, y_pred_new))\n",
    "            #get the y_true\n",
    "            y_true = target.cpu().numpy()\n",
    "            #concate the y_true\n",
    "            y_true = np.concatenate((y_true, y_true))\n",
    "    #print the y_true and y_pred\n",
    "    #now we will build the confusion matrix\n",
    "    #we will use the sklearn confusion matrix\n",
    "    # cm = confusion_matrix(y_true, y_pred)\n",
    "    #now we will print the classification report\n",
    "    # print(classification_report(y_true, y_pred))\n",
    "    #now we will plot class wise precision, recall, f1 score and support\n",
    "    # we use (classification_report(y_true, y_pred, labels=[list of classes], output_dict=True))\n",
    "    #create a list of classes\n",
    "    classes = [i for i in range(NUM_CLASSES)]\n",
    "    # cr = classification_report(y_true, y_pred, labels=classes, output_dict=True)\n",
    "    #now we will plot the precision, recall, f1 score and support\n",
    "    #print \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    #now we will plot confusion matrix from ConfusionMatrixDisplay.from_estimator\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    #plot graphically \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.float64 object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb Cell 51\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#call classification_report\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m classification_report(model, test_loader)\n",
      "\u001b[1;32m/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb Cell 51\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m classes \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_CLASSES)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# cr = classification_report(y_true, y_pred, labels=classes, output_dict=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#now we will plot the precision, recall, f1 score and support\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#print \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_true, y_pred))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#now we will plot confusion matrix from ConfusionMatrixDisplay.from_estimator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(y_true, y_pred)\n",
      "\u001b[1;32m/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb Cell 51\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m#loop through the test data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data, target \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m#send the data to gpu\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/A2/Domaain_Adaptation/classifier_mnist_usps.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m#get the output from the model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float64 object"
     ]
    }
   ],
   "source": [
    "#call classification_report\n",
    "classification_report(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079402cc50f681fca3bc4b588c8594ae5b0127c6215ec7c89d21fdfb87f97274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
